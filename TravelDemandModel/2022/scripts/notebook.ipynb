{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> previous results and methods at \n",
    "* https://trpa-agency.github.io/travel_demand_model/base_2018.html\n",
    "* TravelDemandModel\\2018\\scripts\n",
    "> Metadata at \n",
    "* TravelDemandModel\\2022\\metadata\\TDM_DataEngineering_Methods.docx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Files \n",
    "* TravelDemandModel\\2022\\scripts\\Lookup_Lists\n",
    "* TravelDemandModel\\2022\\data\\raw_data\n",
    "* TravelDemandModel\\2022\\data\\processed_data\n",
    "* TravelDemandModel\\2022\\scripts\\Workspace.gdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import os\n",
    "import arcpy\n",
    "from utils import *\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# pandas options\n",
    "pd.options.mode.copy_on_write = True\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = 999\n",
    "pd.options.display.max_rows    = 999\n",
    "\n",
    "# my workspace \n",
    "workspace = r\"C:\\Users\\mbindl\\Desktop\\Workspace.gdb\"\n",
    "# current working directory\n",
    "local_path = pathlib.Path().absolute()\n",
    "# set data path as a subfolder of the current working directory TravelDemandModel\\2022\\\n",
    "data_dir = local_path.parents[0] / 'data/raw_data'\n",
    "# folder to save processed data\n",
    "out_dir  = local_path.parents[0] / 'data/processed_data'\n",
    "# workspace gdb for stuff that doesnt work in memory\n",
    "# gdb = os.path.join(local_path,'Workspace.gdb')\n",
    "gdb = workspace\n",
    "# set environement workspace to in memory \n",
    "arcpy.env.workspace = 'memory'\n",
    "# # clear memory workspace\n",
    "# arcpy.management.Delete('memory')\n",
    "\n",
    "# overwrite true\n",
    "arcpy.env.overwriteOutput = True\n",
    "# Set spatial reference to NAD 1983 UTM Zone 10N\n",
    "sr = arcpy.SpatialReference(26910)\n",
    "\n",
    "# get parcels from the database\n",
    "# network path to connection files\n",
    "filePath = \"F:/GIS/PARCELUPDATE/Workspace/\"\n",
    "# database file path \n",
    "sdeBase    = os.path.join(filePath, \"Vector.sde\")\n",
    "sdeCollect = os.path.join(filePath, \"Collection.sde\")\n",
    "sdeTabular = os.path.join(filePath, \"Tabular.sde\")\n",
    "\n",
    "# parcel feature class years to use for all cumulative accounting data\n",
    "years = [2012, 2018, 2019, 2020, 2021, 2022, 2023]\n",
    "\n",
    "# schema for the final output\n",
    "final_schema = ['APN', 'Residential_Units', 'TouristAccommodation_Units', 'CommercialFloorArea_SqFt',\n",
    "                'RoomsRented_PerDay', 'VHR_Occupancy_Rate','TAU_Occupancy_Rate', \n",
    "                'PrimaryResidence_Rate', 'SecondaryResidence_Rate',\n",
    "                'HighIncome_Rate',\t'MediumIncome_Rate', 'LowIncome_Rate', 'PersonsPerUnit',\n",
    "                'TAU_TYPE', 'VHR', 'BLOCK_GROUP', 'TAZ', 'OCCUPANCY_ZONE', \n",
    "                'JURISDICTION', 'COUNTY', 'OWNERSHIP_TYPE','EXISTING_LANDUSE', 'WITHIN_TRPA_BNDY', \n",
    "                'PARCEL_ACRES', 'PARCEL_SQFT', 'SHAPE']\n",
    "\n",
    "# Pickle variables\n",
    "# part 1 - spatial join categories, occupancy rates, and parcels\n",
    "parcel_pickle_part1    = data_dir / 'parcel_pickle1.pkl'\n",
    "# part 2 - known occupancy rates applied to the parcel and spatial interpolation of occupancy rates\n",
    "parcel_pickle_part2    = data_dir / 'parcel_pickle2.pkl'\n",
    "# part 3 - fill in missing occupancy rates with spatial interpolation\n",
    "parcel_pickle_part3    = data_dir / 'parcel_pickle3.pkl'\n",
    "# part 4 - attribute join with socioeconmic data\n",
    "parcel_pickle_part4    = data_dir / 'parcel_pickle4.pkl'\n",
    "\n",
    "# pickle for occupancry rates\n",
    "occupancy_rates_pickle = data_dir / 'occupancy_rates.pkl'\n",
    "# campground pickles\n",
    "campground_pickle      = data_dir / 'campground.pkl'\n",
    "# school pickle\n",
    "school_pickle          = data_dir / 'school.pkl'\n",
    "# visitor pickle\n",
    "visitor_pickle         = data_dir / 'visitor.pkl'\n",
    "# socioeconmic pickle\n",
    "socioeconomic_pickle   = data_dir / 'socioeconomic.pkl'\n",
    "# employment pickle\n",
    "employment_pickle      = data_dir / 'employment.pkl'\n",
    "# summary pickle\n",
    "summary_pickle         = data_dir / 'summary.pkl'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Future Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to do a spatial join and\n",
    "# map values from the source to the target\n",
    "def spatial_join_map(target, source, join_field,  map_field, target_field):\n",
    "    # spatial join\n",
    "    arcpy.SpatialJoin_analysis(target, source, 'memory\\\\temp', \n",
    "                               'JOIN_ONE_TO_ONE', 'KEEP_ALL','HAVE_THEIR_CENTER_IN')\n",
    "    # get result as a spatial dataframe\n",
    "    join = pd.DataFrame.spatial.from_featureclass('memory\\\\temp')\n",
    "    join.info()\n",
    "    # map values\n",
    "    target[target_field] = target[join_field].map(dict(zip(join[join_field], join[map_field])))\n",
    "    return target\n",
    "\n",
    "# check for duplicates\n",
    "def check_dupes(df, col):\n",
    "    df['is_duplicate'] = df.duplicated(subset=col, keep=False)\n",
    "    df.is_duplicate.value_counts()\n",
    "    df.loc[df['is_duplicate'] == True]\n",
    "    df = df.drop_duplicates(subset=col, keep='first', inplace=True)\n",
    "    return df[df.duplicated([col], keep=False)]\n",
    "\n",
    "# check if field exists in data frame and final_schema and if not add it\n",
    "def check_field(df, fields):\n",
    "    for field in fields:\n",
    "        if field not in df.columns:\n",
    "            df[field] = np.nan\n",
    "    return df\n",
    "\n",
    "# function to run interpolation and join by APN\n",
    "def interpolate_join(df, sdf):\n",
    "    # interpolate occupancy rate for VHR and TAU parcels where NULL\n",
    "    return df\n",
    "\n",
    "# function to fill missing values\n",
    "def fill_missing_values(df, sdf):\n",
    "    return df\n",
    "\n",
    "# function to run zonal stats and map values\n",
    "def zonal_stats_map(target, source, join_field,  map_field, target_field):\n",
    "    # zonal stats\n",
    "    arcpy.sa.ZonalStatisticsAsTable(target, join_field, source, 'memory\\\\temp', 'DATA', 'MEAN')\n",
    "    # get result as a spatial dataframe\n",
    "    join = pd.DataFrame.spatial.from_featureclass('memory\\\\temp')\n",
    "    join.info()\n",
    "    # map values\n",
    "    target[target_field] = target[join_field].map(dict(zip(join[join_field], join[map_field])))\n",
    "    return target\n",
    "\n",
    "# save to pickle\n",
    "def to_pickle(data, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f'{filename} pickled')\n",
    "\n",
    "# save to pickle and feature class\n",
    "def to_pickle_fc(data, filename):\n",
    "    data.spatial.to_featureclass(filename)\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f'{filename} pickled and saved as feature class')\n",
    "\n",
    "# get a pickled file as a dataframe\n",
    "def from_pickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    print(f'{filename} unpickled')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAZ feature layer polygons\n",
    "taz_url = 'https://maps.trpa.org/server/rest/services/Transportation_Planning/MapServer/6'\n",
    "# get as spatial dataframe\n",
    "sdf_taz = get_fs_data_spatial(taz_url)\n",
    "# set spatial reference to NAD 1983 UTM Zone 10N\n",
    "sdf_taz.spatial.sr = sr\n",
    "\n",
    "# # parcel development layer polygons\n",
    "# units_url = 'https://maps.trpa.org/server/rest/services/Existing_Development/MapServer/2'\n",
    "# # query 2022 rows\n",
    "# sdf_units = get_fs_data_spatial_query(units_url, \"Year = 2022\")\n",
    "# sdf_units.spatial.sr = sr\n",
    "\n",
    "# block group feature layer polygons\n",
    "block_groups_url = 'https://maps.trpa.org/server/rest/services/Demographics/MapServer/27'\n",
    "sdf_block = get_fs_data_spatial(block_groups_url)\n",
    "sdf_block = sdf_block.loc[(sdf_block['YEAR'] == 2020) & (sdf_block['GEOGRAPHY'] == 'Block Group')]\n",
    "sdf_block.spatial.sr = sr\n",
    "\n",
    "# # vhr feature layer polygons \n",
    "# vhr_url = 'https://maps.trpa.org/server/rest/services/VHR/MapServer/0'\n",
    "# sdf_vhr = get_fs_data_spatial(vhr_url)\n",
    "# sdf_vhr.spatial.sr = sr\n",
    "# # filter vhr layer to active status\n",
    "# sdf_vhr = sdf_vhr.loc[sdf_vhr['Status'] == 'Active']\n",
    "\n",
    "# ACS 2022 bolock group table\n",
    "census_url = 'https://maps.trpa.org/server/rest/services/Demographics/MapServer/28'\n",
    "df_census = get_fs_data(census_url)\n",
    "df_census_2022 = df_census.loc[(df_census['year_sample'] == 2022) & (df_census['sample_level'] == 'block group')]\n",
    "\n",
    "# campground points feature layer\n",
    "campground_url = 'https://maps.trpa.org/server/rest/services/Recreation/MapServer/1'\n",
    "sdf_campground =  get_fs_data_spatial_query(campground_url, \"RECREATION_TYPE='Campground'\")\n",
    "\n",
    "# campground visits table\n",
    "campvisits_url = 'https://maps.trpa.org/server/rest/services/Transportation_Planning/MapServer/14'\n",
    "dfCamp = get_fs_data_query(campvisits_url, \"Year = 2022\")\n",
    "\n",
    "# occupancy zone feature layer polygons\n",
    "occupancyzones_url = 'https://maps.trpa.org/server/rest/services/Transportation_Planning/MapServer/15'\n",
    "sdf_occ = get_fs_data_spatial(occupancyzones_url)\n",
    "sdf_occ.spatial.sr = sr\n",
    "\n",
    "# occupancy rate table\n",
    "occupancyrate_url = 'https://maps.trpa.org/server/rest/services/Transportation_Planning/MapServer/13'\n",
    "df_occ = get_fs_data(occupancyrate_url)\n",
    "\n",
    "# school enrollment table - incomplete data - missing meyers elementary, bijou, sierra house, and LTCC and Sierra Nevada College\n",
    "school_url_table     = 'https://maps.trpa.org/server/rest/services/Transportation_Planning/MapServer/17'\n",
    "df_school_enrollment = get_fs_data_query(school_url_table, \"Year = '2021-2022'\")\n",
    "\n",
    "# school feature layer points\n",
    "school_url_spatial    = 'https://maps.trpa.org/server/rest/services/Transportation_Planning/MapServer/16'\n",
    "sdf_school            =  get_fs_data_spatial(school_url_spatial)\n",
    "sdf_school.spatial.sr = sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parcel development layer polygons\n",
    "parcel_db = sdeCollect + \"\\\\SDE.Parcel\\\\SDE.Parcel_History_Attributed\"\n",
    "# query 2022 rows\n",
    "sdf_units = pd.DataFrame.spatial.from_featureclass(parcel_db)\n",
    "sdf_units = sdf_units.loc[sdf_units['YEAR'] == 2022]\n",
    "sdf_units.spatial.sr = sr\n",
    "\n",
    "# vhr feature layer polygons \n",
    "vhr_db = sdeCollect + \"\\\\SDE.Parcel\\\\SDE.Parcel_VHR\"\n",
    "sdf_vhr = pd.DataFrame.spatial.from_featureclass(vhr_db)\n",
    "sdf_vhr.spatial.sr = sr\n",
    "# filter vhr layer to active status\n",
    "sdf_vhr = sdf_vhr.loc[sdf_vhr['Status'] == 'Active']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAZ feature layer polygons\n",
    "taz_db = sdeBase + \"\\\\SDE.Transportation\\\\SDE.Transportation_Analysis_Zone\"\n",
    "# get as spatial dataframe\n",
    "sdf_taz = pd.DataFrame.spatial.from_featureclass(taz_db)\n",
    "# set spatial reference to NAD 1983 UTM Zone 10N\n",
    "sdf_taz.spatial.sr = sr\n",
    "\n",
    "# parcel development layer polygons\n",
    "parcel_db = sdeCollect + \"\\\\SDE.Parcel\\\\SDE.Parcel_History_Attributed\"\n",
    "# query 2022 rows\n",
    "sdf_units = pd.DataFrame.spatial.from_featureclass(parcel_db)\n",
    "sdf_units = sdf_units.loc[sdf_units['YEAR'] == 2022]\n",
    "sdf_units.spatial.sr = sr\n",
    "\n",
    "# block group feature layer polygons\n",
    "block_groups_db = sdeBase + \"\\\\SDE.Census\\\\SDE.Tahoe_Census_Geography\"\n",
    "sdf_block = pd.DataFrame.spatial.from_featureclass(block_groups_db)\n",
    "sdf_block = sdf_block.loc[(sdf_block['YEAR'] == 2020) & (sdf_block['GEOGRAPHY'] == 'Block Group')]\n",
    "sdf_block.spatial.sr = sr\n",
    "\n",
    "# vhr feature layer polygons \n",
    "vhr_db = sdeCollect + \"\\\\SDE.Parcel\\\\SDE.Parcel_VHR\"\n",
    "sdf_vhr = pd.DataFrame.spatial.from_featureclass(vhr_db)\n",
    "sdf_vhr.spatial.sr = sr\n",
    "# filter vhr layer to active status\n",
    "sdf_vhr = sdf_vhr.loc[sdf_vhr['Status'] == 'Active']\n",
    "\n",
    "# ACS 2022 bolock group table\n",
    "census_url = sdeBase + \"\\\\SDE.Census_Demographics\"\n",
    "df_census = pd.DataFrame.spatial.from_table(census_url)\n",
    "df_census_2022 = df_census.loc[(df_census['year_sample'] == 2022) & (df_census['sample_level'] == 'block group')]\n",
    "\n",
    "# campground points feature layer\n",
    "campground_db = sdeBase + \"\\\\SDE.Recreation\\\\SDE.Recreation_Sites\"\n",
    "sdf_campground = pd.DataFrame.spatial.from_featureclass(campground_db) \n",
    "sdf_campground = sdf_campground.loc[sdf_campground['RECREATION_TYPE'] == 'Campground']\n",
    "sdf_campground.spatial.sr = sr\n",
    "\n",
    "# campground visits table\n",
    "campvisits_db = sdeTabular + \"\\\\SDE.Campground_Visitation\"\n",
    "dfCamp = pd.DataFrame.spatial.from_table(campvisits_db)\n",
    "dfCamp = dfCamp.loc[dfCamp['Year'] == 2022]\n",
    "\n",
    "# occupancy zone feature layer polygons\n",
    "occupancyzones_db = sdeBase + \"\\\\SDE.Transportation\\\\SDE.Occupancy_Rate_Zones\"\n",
    "sdf_occ = pd.DataFrame.spatial.from_featureclass(occupancyzones_db)\n",
    "sdf_occ.spatial.sr = sr\n",
    "\n",
    "# occupancy rate table\n",
    "occupancyrate_db = sdeTabular + \"\\\\SDE.Occupancy_Rates\"\n",
    "df_occ = pd.DataFrame.spatial.from_table(occupancyrate_db)\n",
    "\n",
    "# school enrollment table - incomplete data - missing meyers elementary, bijou, sierra house, and LTCC and Sierra Nevada College\n",
    "school_db_table     = sdeTabular + \"\\\\SDE.School_Enrollment\"\n",
    "df_school_enrollment = pd.DataFrame.spatial.from_table(school_db_table)\n",
    "df_school_enrollment = df_school_enrollment.loc[df_school_enrollment['Year'] == '2021-2022']\n",
    "\n",
    "# school feature layer points\n",
    "school_db_spatial    = sdeBase + \"\\\\SDE.Jurisdictions\\\\SDE.Schools\"\n",
    "sdf_school = pd.DataFrame.spatial.from_featureclass(school_db_spatial)\n",
    "sdf_school.spatial.sr = sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total Residential Units for 2022\n",
    "sdf_units_2022 = sdf_units.loc[sdf_units['YEAR'] == 2022]\n",
    "# total Residential Units for 2022\n",
    "sdf_units_2022.Residential_Units.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Occupancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> general spatial joins and categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial join to get TAZ\n",
    "arcpy.SpatialJoin_analysis(sdf_units, sdf_taz, \"Existing_Development_TAZ\", \n",
    "                           \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", \"\", \"HAVE_THEIR_CENTER_IN\")\n",
    "# spatial join to get Block Group\n",
    "arcpy.SpatialJoin_analysis(sdf_units, sdf_block, \"Existing_Development_BlockGroup\", \n",
    "                           \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", \"\", \"HAVE_THEIR_CENTER_IN\")\n",
    "# spatail join to get Occupancy Rate Zone\n",
    "sdf_occ = sdf_occ.loc[sdf_occ['OccupancyRate_ZoneID'] != 'CSLT_ALL']\n",
    "arcpy.SpatialJoin_analysis(sdf_units, sdf_occ, \"Existing_Development_OccupancyZone\", \n",
    "                           \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", \"\", \"HAVE_THEIR_CENTER_IN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Parcels APN with TAU Types\n",
    "tau_lookup = pd.read_csv('Lookup_Lists/lookup_tau_type.csv')\n",
    "#sro_lookup = pd.read_csv('Lookup_Lists/lookup_sro.csv')\n",
    "\n",
    "# check if fields exist in the dataframes\n",
    "sdfParcel   = check_field(sdf_units, final_schema)\n",
    "\n",
    "# merge parcel 2022 with parcel VHR\n",
    "sdfParcel = sdfParcel.merge(sdf_vhr, on='APN', how='left', indicator=True)\n",
    "\n",
    "# calculate VHR = Yes if VHR is in the parcel\n",
    "sdfParcel['VHR'] = 'No'\n",
    "sdfParcel.loc[sdfParcel['_merge'] == 'both', 'VHR'] = 'Yes'\n",
    "\n",
    "# setup TAU_Type\n",
    "sdfParcel['TAU_TYPE'] = 'N/A'\n",
    "\n",
    "# filter parcels so only APNs in the lookup are included\n",
    "sdfTAU = sdfParcel[sdfParcel['APN'].isin(tau_lookup['APN'])]\n",
    "# get TAU_Type from lookup\n",
    "sdfTAU['TAU_TYPE'] = sdfTAU['APN'].map(tau_lookup.set_index('APN')['TAU_Type'])\n",
    "\n",
    "# any row with ToursitAccommodation_Units > 0 and TAU_Type is null, set TAU_Type to 'HotelMotel'\n",
    "sdfParcel.loc[(sdfParcel['TouristAccommodation_Units'] > 0) & (sdfParcel['TAU_TYPE']=='N/A'), 'TAU_TYPE'] = 'HotelMotel'\n",
    "# for the rows in df that match rows by APN in dfTAU set TAU_Type to the value in dfTAU\n",
    "sdfParcel.loc[sdfParcel['APN'].isin(sdfTAU['APN']), 'TAU_TYPE'] = sdfTAU['TAU_TYPE']\n",
    "\n",
    "# remove _x from column names\n",
    "sdfParcel.columns = sdfParcel.columns.str.replace('_x', '')\n",
    "\n",
    "# get results of spatial joins as spatial dataframes\n",
    "sdf_units_taz   = pd.DataFrame.spatial.from_featureclass(\"Existing_Development_TAZ\", sr=sr)  \n",
    "sdf_units_block = pd.DataFrame.spatial.from_featureclass(\"Existing_Development_BlockGroup\", sr=sr)\n",
    "sdf_units_occ   = pd.DataFrame.spatial.from_featureclass(\"Existing_Development_OccupancyZone\", sr=sr)\n",
    "\n",
    "# map dictionary to sdf_units dataframe to fill in TAZ and Block Group fields\n",
    "sdfParcel['TAZ']           = sdfParcel.APN.map(dict(zip(sdf_units_taz.APN,   sdf_units_taz.TAZ_1)))\n",
    "sdfParcel['BLOCK_GROUP']   = sdfParcel.APN.map(dict(zip(sdf_units_block.APN, sdf_units_block.TRPAID)))\n",
    "sdfParcel['OCCUPANCY_ZONE']= sdfParcel.APN.map(dict(zip(sdf_units_occ.APN,   sdf_units_occ.OccupancyRate_ZoneID)))\n",
    "\n",
    "# if df.JURISDICTION == \"CSLT\" and VHR == \"Yes\" then set OCCUPANCY_ZONE to \"CSLT_ALL\"\n",
    "sdfParcel.loc[(sdfParcel['JURISDICTION'] == 'CSLT') & (sdfParcel['VHR'] == 'Yes'), 'OCCUPANCY_ZONE'] = 'CSLT_ALL'\n",
    "\n",
    "# columns to keep\n",
    "sdfParcel = sdfParcel[final_schema]\n",
    "\n",
    "# export to pickle\n",
    "sdfParcel.to_pickle(parcel_pickle_part1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Occupancy Rates Table Fix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Fill in missing data\n",
    "* Washoe Rooms Available, Washoe Reported Occupancy Rates, Washoe Quarterly VHRs added to Monthly Rows\n",
    "* Rest of El Dorado County VHR zones need Rooms Available & Rooms Rented\n",
    "    * Why are we using CSLT rate data instead of IDW interpolated data?\n",
    "* Weight the rates and calculate rooms rented per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of occupancy rates table and parcel layer\n",
    "dfOcc     = df_occ.copy()\n",
    "sdfParcel = pd.read_pickle(parcel_pickle_part1)\n",
    "\n",
    "# filter to columns \n",
    "columns = ['Zone_ID', 'Period', 'RoomType', 'Report_OccRate','TRPA_OccRate']\n",
    "\n",
    "# dictinary to convert the time frames to make things cleaner\n",
    "timeframe_dict = {\n",
    "    '2022-06-01': 'June',\n",
    "    '2022-08-01': 'August',\n",
    "    '2022-09-01': 'September',\n",
    "    'Q4 21-22'  : 'April-June',\n",
    "    'Q1 22-23'  : 'July-September',\n",
    "    'Q2 2022'   : 'April-June',\n",
    "    'Q3 2022'   : 'July-September'\n",
    "}\n",
    "\n",
    "# Period field based on Timeframe and timeframe_dict\n",
    "dfOcc['Period'] = dfOcc['Timeframe'].map(timeframe_dict)\n",
    "\n",
    "## Fill in Missing Data for Washoe County ##\n",
    "\n",
    "# get total WA taus and vhrs from the parcel layer\n",
    "tauWA = sdfParcel.loc[(sdfParcel.COUNTY == 'WA'), 'TouristAccommodation_Units'].sum()\n",
    "vhrWA = sdfParcel.loc[(sdfParcel.COUNTY == 'WA')&(sdfParcel.VHR == 'Yes'), 'APN'].count()\n",
    "\n",
    "# Calculate Rooms available for HotelMotel, Casino, Resort in Washoe County using total TAUs from the parcel layer\n",
    "dfOcc.loc[(dfOcc['Zone_ID'] == 'Washoe County') & (dfOcc['Temporal_Scale'] == 'Monthly') &\n",
    "                (dfOcc.RoomType.isin(['HotelMotel', 'Casino', 'Resort'])) & (dfOcc['Period'].isin(['June', 'September'])), \n",
    "                'Report_RoomsAvailable'] = tauWA * 30\n",
    "dfOcc.loc[(dfOcc['Zone_ID'] == 'Washoe County') & (dfOcc['Temporal_Scale'] == 'Monthly') &\n",
    "              (dfOcc.RoomType.isin(['HotelMotel', 'Casino', 'Resort'])) & (dfOcc['Period'].isin(['July','August'])), \n",
    "              'Report_RoomsAvailable'] = tauWA * 31\n",
    "\n",
    "# caclulate Rooms available for VHRs in Washoe County using total VHRs from the parcel layer\n",
    "dfOcc.loc[(dfOcc['Zone_ID'] == 'Washoe County') & (dfOcc['Temporal_Scale'] == 'Monthly') & \n",
    "              (dfOcc.RoomType == 'VHR') & (dfOcc['Period'].isin(['July', 'August'])), \n",
    "              'Report_RoomsAvailable'] = vhrWA * 31\n",
    "dfOcc.loc[(dfOcc['Zone_ID'] == 'Washoe County') & (dfOcc['Temporal_Scale'] == 'Monthly') & \n",
    "              (dfOcc.RoomType == 'VHR') & (dfOcc['Period'].isin(['June', 'September'])), \n",
    "              'Report_RoomsAvailable'] = vhrWA * 30\n",
    "\n",
    "# if the Zone_ID is Washoe County and VHR and Timeframe is the Q3 or Q2 then /3 to get monthly rooms available for that quarter/row\n",
    "waVHRZoneQ2 = dfOcc.loc[(dfOcc['Zone_ID'] == 'Washoe County') & (dfOcc.RoomType == 'VHR') & (dfOcc.Timeframe == 'Q2 2022')]\n",
    "extraVHRQ2 = int((waVHRZoneQ2['Report_RoomsRented'] / 3).round(0).iloc[0])\n",
    "waVHRZoneQ3 = dfOcc.loc[(dfOcc['Zone_ID'] == 'Washoe County') & (dfOcc.RoomType == 'VHR') & (dfOcc.Timeframe == 'Q3 2022')]\n",
    "extraVHRQ3 = int((waVHRZoneQ3['Report_RoomsRented'] / 3).round(0).iloc[0])\n",
    "\n",
    "# add the extra VHR rooms rented to the monthly rows that fall within that quarter Zone_ID is Washoe County and Temporal_Scale is Monthly\n",
    "dfOcc.loc[(dfOcc['Zone_ID'] == 'Washoe County') & (dfOcc['Temporal_Scale'] == 'Monthly') \n",
    "              & (dfOcc.RoomType =='VHR') & (dfOcc.Timeframe == 'June'), \n",
    "              'Report_RoomsRented'] = dfOcc['Report_RoomsRented'] + extraVHRQ2\n",
    "dfOcc.loc[(dfOcc['Zone_ID'] == 'Washoe County') & (dfOcc['Temporal_Scale'] == 'Monthly')\n",
    "              & (dfOcc.RoomType =='VHR') & (dfOcc.Timeframe.isin(['July', 'August'])),\n",
    "              'Report_RoomsRented'] = dfOcc['Report_RoomsRented'] + extraVHRQ3\n",
    "\n",
    "# if the Zone_ID is Washoe County set Report_OccRate to Report_RoomsRented by Report _RoomsAvailable\n",
    "dfOcc.loc[dfOcc['Zone_ID'] == 'Washoe County', 'Report_OccRate'] = dfOcc['Report_RoomsRented']/dfOcc['Report_RoomsAvailable']\n",
    "\n",
    "## Fill in Missing Data for El Dorado County ##\n",
    "\n",
    "# get total VHRs in El Dorado County from the parcel layer\n",
    "vhrEL = sdfParcel.loc[(sdfParcel.JURISDICTION == 'EL') & (sdfParcel.VHR == 'Yes'), 'APN'].count()\n",
    "\n",
    "# caclulate Rooms available for VHRs in El Dorado County using total VHRs from the parcel layer\n",
    "dfOcc.loc[(dfOcc['Zone_ID'] == 'Rest of El Dorado County') & (dfOcc['Temporal_Scale'] == 'Monthly') & \n",
    "              (dfOcc.RoomType == 'VHR') & (dfOcc['Period'].isin(['July', 'August'])), \n",
    "              'Report_RoomsAvailable'] = vhrEL * 31\n",
    "dfOcc.loc[(dfOcc['Zone_ID'] == 'Rest of El Dorado County') & (dfOcc['Temporal_Scale'] == 'Monthly') &\n",
    "              (dfOcc.RoomType == 'VHR') & (dfOcc['Period'].isin(['June', 'September'])),\n",
    "              'Report_RoomsAvailable'] = vhrEL * 30\n",
    "\n",
    "# calculate Rooms Rented for VHRs in El Dorado County using Report_OccRate and Report_RoomsAvailable\n",
    "dfOcc.loc[(dfOcc['Zone_ID'] == 'Rest of El Dorado County') & (dfOcc['Temporal_Scale'] == 'Monthly') & (dfOcc.RoomType == 'VHR'), \n",
    "              'Report_RoomsRented'] = (dfOcc['Report_OccRate'] * dfOcc['Report_RoomsAvailable']).fillna(0).astype(int)\n",
    "\n",
    "## Calculate Weighted Average Occupancy Rate ##\n",
    "\n",
    "# df copy\n",
    "df = dfOcc.copy()\n",
    "\n",
    "# Define the weights for each month based on the number of days they contribute\n",
    "weights = {\n",
    "    'June'          : 8/20,\n",
    "    'August'        : 3/20,\n",
    "    'September'     : 9/20,\n",
    "    'April-June'    : 8/20,\n",
    "    'July-September': 12/20\n",
    "}\n",
    "\n",
    "# calculate the weighted occupancy rates\n",
    "for key,value in weights.items():\n",
    "    # Apply weights to the occupancy rates\n",
    "    df.loc[df['Period'] == key, 'TRPA_OccRate'] = df['Report_OccRate'] * value\n",
    "\n",
    "# Calculate RoomsRentedPerDay based on the period\n",
    "df['RoomsRentedPerDay'] = df.apply(lambda row: row['Report_RoomsRented'] / 30 if row['Period'] in ['June', 'September'] else\n",
    "                                   (row['Report_RoomsRented'] / 31 if row['Period'] == 'August' else\n",
    "                                    (row['Report_RoomsRented'] / 91 if row['Period'] == 'April-June' else\n",
    "                                     (row['Report_RoomsRented'] / 92 if row['Period'] == 'July-September' else 0))), axis=1).fillna(0).astype(int)\n",
    "\n",
    "# filter by Temporal_Scale\n",
    "df_monthly   = df.loc[df['Temporal_Scale'] == 'Monthly']\n",
    "df_quarterly = df.loc[df['Temporal_Scale'] == 'Quarterly']\n",
    "\n",
    "# group by for montthly and quarterly and mean for Report_OccRate and sum for TRPA_OccRate\n",
    "dfMonthly   = df_monthly.groupby(['Zone_ID', 'RoomType', 'Temporal_Scale']).agg({'RoomsRentedPerDay': 'mean','Report_RoomsAvailable':'sum',\n",
    "                                                                                 'Report_RoomsRented':'sum', 'Report_OccRate': 'mean', \n",
    "                                                                                 'TRPA_OccRate': 'sum'}).reset_index()\n",
    "\n",
    "dfQuarterly = df_quarterly.groupby(['Zone_ID', 'RoomType', 'Temporal_Scale']).agg({'RoomsRentedPerDay': 'mean','Report_RoomsAvailable':'sum',\n",
    "                                                                                   'Report_RoomsRented':'sum', 'Report_OccRate': 'mean', \n",
    "                                                                                   'TRPA_OccRate': 'sum'}).reset_index()\n",
    "\n",
    "# concat the two dataframes into the final occupancy rate dataframe\n",
    "dfOccFinal = pd.concat([dfMonthly, dfQuarterly]).reset_index(drop=True)\n",
    "\n",
    "# cast RoomsRentedPerDay as int \n",
    "dfOccFinal['RoomsRentedPerDay'] = dfOccFinal['RoomsRentedPerDay'].astype(int)\n",
    "# drop rows where Zone_ID is Washoe County and Temporal_Scale is Quarterly\n",
    "df = dfOccFinal.loc[~((dfOccFinal['Zone_ID'] == 'Washoe County') & (dfOccFinal['Temporal_Scale'] == 'Quarterly'))].reset_index(drop=True)\n",
    "df.info()\n",
    "# save to pickle\n",
    "df.to_pickle(occupancy_rates_pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Spatial join to apply Lodging Occupany Rates to parcel layer, \n",
    "* select parcels where Lodging Occupancy Rate is Null, \n",
    "* run interpolation, \n",
    "* apply interpoleted values to parcels where occupancy rate is null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Filter Occupancy Rate table to Timeframe and Room Type, Merge with Occupancy Zone Feature Class, and Export to Feature Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the pickled parcel dataframe\n",
    "sdfParcel = pd.read_pickle(parcel_pickle_part1)\n",
    "# read in the pickled occupancy rates table\n",
    "dfOcc     = pd.read_pickle(occupancy_rates_pickle)\n",
    "\n",
    "# filter occupancy rate table by RoomType\n",
    "dfOccTAU = dfOcc.loc[dfOcc['RoomType'].isin(['HotelMotel', 'Casino', 'Resort'])]    \n",
    "dfOccVHR = dfOcc.loc[dfOcc['RoomType'] == 'VHR']\n",
    "\n",
    "# specify the output feature classes\n",
    "tau_occ_zones = os.path.join(gdb,'OccupancyRate_Zones_TAU')\n",
    "vhr_occ_zones = os.path.join(gdb,'OccupancyRate_Zones_VHR')\n",
    "\n",
    "# merge occupancy rate data to occupancy zones\n",
    "sdfOccTAU = pd.merge(sdf_occ, dfOccTAU, left_on='OccupancyRate_ZoneID', right_on='Zone_ID', how='left')\n",
    "sdfOccVHR = pd.merge(sdf_occ, dfOccVHR, left_on='OccupancyRate_ZoneID', right_on='Zone_ID', how='left')\n",
    "\n",
    "# export sdf to feature class\n",
    "sdfOccTAU.spatial.to_featureclass(location=tau_occ_zones, overwrite=True)\n",
    "sdfOccVHR.spatial.to_featureclass(location=vhr_occ_zones, overwrite=True)\n",
    "\n",
    "# filter rows where VHR = Yes and rows where TouristAccommodation_Units > 0\n",
    "sdfTAU = sdfParcel.loc[sdfParcel['TouristAccommodation_Units'] > 0]\n",
    "sdfVHR = sdfParcel.loc[sdfParcel['VHR'] == 'Yes']\n",
    "# specify the output feature classes\n",
    "tau_occ_zones = os.path.join(gdb,'OccupancyRate_Zones_TAU')\n",
    "vhr_occ_zones = os.path.join(gdb,'OccupancyRate_Zones_VHR')\n",
    "\n",
    "# spatial join TAU to occupancy rate zones with TAU values\n",
    "spjoin_tau = arcpy.analysis.SpatialJoin(sdfTAU, tau_occ_zones, 'OccupancyRate_Zones_TAU_Parcels', \n",
    "                                        \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", None, \"INTERSECT\", None, None)\n",
    "# spatial join VHR to occupancy rate zones with VHR values\n",
    "spjoin_vhr = arcpy.analysis.SpatialJoin(sdfVHR, vhr_occ_zones, 'OccupancyRate_Zones_VHR_Parcels', \n",
    "                                        \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", None, \"INTERSECT\", None, None)\n",
    "\n",
    "# get results of spatial joins as spatial dataframes\n",
    "sdf_parcel_tau_rates = pd.DataFrame.spatial.from_featureclass(\"OccupancyRate_Zones_TAU_Parcels\", sr=sr)  \n",
    "sdf_parcel_vhr_rates = pd.DataFrame.spatial.from_featureclass(\"OccupancyRate_Zones_VHR_Parcels\", sr=sr)\n",
    "\n",
    "# map dictionary for TAU and VHR parcels respectively\n",
    "sdfParcel['VHR_Occupancy_Rate'] = sdfVHR.APN.map(dict(zip(sdf_parcel_vhr_rates.APN, sdf_parcel_vhr_rates.trpa_occ_rate)))\n",
    "sdfParcel['TAU_Occupancy_Rate'] = sdfTAU.APN.map(dict(zip(sdf_parcel_tau_rates.APN, sdf_parcel_tau_rates.trpa_occ_rate)))\n",
    "\n",
    "# cast VHR_Occupancy_Rate and TAU_Occupancy_Rate as float and fill na as 0\n",
    "sdfParcel['VHR_Occupancy_Rate'] = sdfParcel['VHR_Occupancy_Rate'].fillna(0).astype(float)\n",
    "sdfParcel['TAU_Occupancy_Rate'] = sdfParcel['TAU_Occupancy_Rate'].fillna(0).astype(float)\n",
    "\n",
    "# export to pickle\n",
    "sdfParcel.to_pickle(parcel_pickle_part2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Generate Spatial Interpolated Occupancy Rate Surfaces and Fill in parcel level missing occupancy rates with interpolated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pickle\n",
    "sdfParcel = pd.read_pickle(parcel_pickle_part2)\n",
    "\n",
    "# Set the extent environment using a feature class\n",
    "arcpy.env.extent = os.path.join(gdb,\"OccupancyRate_Zones_TAU\")\n",
    "# set the input feature class\n",
    "tau_fc = os.path.join(gdb,'TAU_points')\n",
    "vhr_fc = os.path.join(gdb,'VHR_points')\n",
    "# set the output raster\n",
    "tau_raster = os.path.join(gdb,'tau_occupancy_rate')\n",
    "vhr_raster = os.path.join(gdb,'vhr_occupancy_rate')\n",
    "# set the output cell size\n",
    "cell_size = 30\n",
    "# set the power parameter\n",
    "power = 2\n",
    "# set the search radius\n",
    "search_radius = 10000\n",
    "\n",
    "# select rows where TAU_TYPE is not null but TAU_Occupancy_Rate is null\n",
    "tauParcel_NULL = sdfParcel.loc[(sdfParcel['TAU_TYPE'].isin(['HotelMotel','Casino','Resort'])) & \n",
    "                                  (sdfParcel['TAU_Occupancy_Rate']==0)]\n",
    "vhrParcel_NULL = sdfParcel.loc[(sdfParcel['VHR'] == 'Yes') & \n",
    "                                  (sdfParcel['VHR_Occupancy_Rate']==0)] \n",
    "# get not null parcels for TAU and VHR\n",
    "tauParcel_notNULL = sdfParcel.loc[(sdfParcel['TAU_TYPE'].isin(['HotelMotel','Casino','Resort'])) & \n",
    "                                  (sdfParcel['TAU_Occupancy_Rate']!=0)]\n",
    "vhrParcel_notNULL = sdfParcel.loc[(sdfParcel['VHR'] == 'Yes') & \n",
    "                                  (sdfParcel['VHR_Occupancy_Rate']!=0)]\n",
    "# NULL parcels\n",
    "# to feature class\n",
    "tauParcel_NULL.spatial.to_featureclass(location=os.path.join(gdb,\"TAU_NULL_occ\"), overwrite=True)\n",
    "vhrParcel_NULL.spatial.to_featureclass(location=os.path.join(gdb,\"VHR_NULL_occ\"), overwrite=True)\n",
    "# not NULL parcels\n",
    "# to feature class\n",
    "tauParcel_notNULL.spatial.to_featureclass(location=os.path.join(gdb,\"TAU_occ\"), overwrite=True)\n",
    "vhrParcel_notNULL.spatial.to_featureclass(location=os.path.join(gdb,\"VHR_occ\"), overwrite=True)\n",
    "\n",
    "# feature to point for TAU and VHR\n",
    "arcpy.management.FeatureToPoint(tauParcel_NULL,    os.path.join(gdb,'TAU_NULL_points'),\"INSIDE\")\n",
    "arcpy.management.FeatureToPoint(vhrParcel_NULL,    os.path.join(gdb,'VHR_NULL_points'), \"INSIDE\")\n",
    "arcpy.management.FeatureToPoint(tauParcel_notNULL, os.path.join(gdb,'TAU_points'), \"INSIDE\")\n",
    "arcpy.management.FeatureToPoint(vhrParcel_notNULL, os.path.join(gdb,'VHR_points'), \"INSIDE\")\n",
    "\n",
    "# run the IDW for TAU parcels with rates\n",
    "arcpy.sa.Idw(tau_fc, \n",
    "            z_field='TAU_Occupancy_Rate', \n",
    "            cell_size=cell_size, \n",
    "            power=power, \n",
    "            search_radius=search_radius).save(tau_raster)\n",
    "\n",
    "# and for VHR parcels with rates\n",
    "arcpy.sa.Idw(vhr_fc,\n",
    "            z_field='VHR_Occupancy_Rate',\n",
    "            cell_size=cell_size,\n",
    "            power=power,\n",
    "            search_radius=search_radius).save(vhr_raster)\n",
    "\n",
    "# Set the local variables for ZonalStatisticsAsTable\n",
    "zoneField   = \"APN\"\n",
    "tauZoneData = os.path.join(gdb, 'TAU_NULL_occ')\n",
    "vhrZoneData = os.path.join(gdb, 'VHR_NULL_occ')\n",
    "tauRaster   = os.path.join(gdb, 'tau_occupancy_rate')\n",
    "vhrRaster   = os.path.join(gdb, 'vhr_occupancy_rate')\n",
    "tauTable    = os.path.join(gdb, 'zonalstat_TAU_Occupancy')\n",
    "vhrTable    = os.path.join(gdb, 'zonalstat_VHR_Occupancy')\n",
    "\n",
    "# Execute ZonalStatisticsAsTable\n",
    "tauZSaT = arcpy.sa.ZonalStatisticsAsTable(tauZoneData, zoneField, tauRaster, \n",
    "                                            tauTable, \"DATA\", \"MEAN\")\n",
    "vhrZSaT = arcpy.sa.ZonalStatisticsAsTable(vhrZoneData, zoneField, vhrRaster,\n",
    "                                            vhrTable, \"DATA\", \"MEAN\")\n",
    "\n",
    "# convert zonal stats tables to dataframes\n",
    "tauZonalStats = arcpy.da.TableToNumPyArray(tauZSaT, '*')\n",
    "vhrZonalStats = arcpy.da.TableToNumPyArray(vhrZSaT, '*')\n",
    "dfTAU = pd.DataFrame(tauZonalStats)\n",
    "dfVHR = pd.DataFrame(vhrZonalStats)\n",
    "\n",
    "# Create a temporary column with the new mapped values\n",
    "sdfParcel['New_TAU_Occupancy_Rate'] = sdfParcel['APN'].map(dict(zip(dfTAU['apn'], dfTAU['MEAN'])))\n",
    "sdfParcel['New_VHR_Occupancy_Rate'] = sdfParcel['APN'].map(dict(zip(dfVHR['apn'], dfVHR['MEAN'])))\n",
    "\n",
    "# Combine the new column with the existing column, preserving existing values where the new values are NaN or 0\n",
    "sdfParcel['TAU_Occupancy_Rate'] = sdfParcel['New_TAU_Occupancy_Rate'].combine_first(sdfParcel['TAU_Occupancy_Rate'])\n",
    "sdfParcel['VHR_Occupancy_Rate'] = sdfParcel['New_VHR_Occupancy_Rate'].combine_first(sdfParcel['VHR_Occupancy_Rate'])\n",
    "\n",
    "# Drop the temporary column\n",
    "sdfParcel.drop(columns=['New_TAU_Occupancy_Rate'], inplace=True)\n",
    "sdfParcel.drop(columns=['New_VHR_Occupancy_Rate'], inplace=True)\n",
    "\n",
    "### Why isnt the zonal stats working for these parcels?? ###\n",
    "\n",
    "# those APNs to list\n",
    "tau_apn_list = sdfParcel.loc[(sdfParcel['TAU_Occupancy_Rate'] == 0) & \n",
    "                             (sdfParcel['TouristAccommodation_Units'] > 0)]['APN'].tolist()\n",
    "vhr_apn_list = sdfParcel.loc[(sdfParcel['VHR_Occupancy_Rate'] == 0) & \n",
    "                             (sdfParcel['VHR'] == 'Yes')]['APN'].tolist()\n",
    "\n",
    "# # classify the occupancy rates for those parcels\n",
    "sdfParcel.loc[sdfParcel['APN'].isin(tau_apn_list), 'TAU_Occupancy_Rate'] = 0.592337\n",
    "sdfParcel.loc[sdfParcel['APN'].isin(vhr_apn_list), 'VHR_Occupancy_Rate'] = 0.422337\n",
    "\n",
    "# pickle and save to feature class\n",
    "outfc = 'sdf_units_attributed_occupancy_interpolated'\n",
    "# export to feature class\n",
    "sdfParcel.spatial.to_featureclass(location=os.path.join(gdb, outfc), sanitize_columns=False)\n",
    "# export to pickle\n",
    "sdfParcel.to_pickle(parcel_pickle_part3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create csv with APN field and List fiedl using tau_apn_list and vhr_apn_list\n",
    "# dictionary of APN and \"TUA\" or \"VHR\"\n",
    "apn_dict = {apn: 'TAU' for apn in tau_apn_list}\n",
    "apn_dict.update({apn: 'VHR' for apn in vhr_apn_list})\n",
    "# create a dataframe from the dictionary\n",
    "df_apn = pd.DataFrame(list(apn_dict.items()), columns=['APN', 'List'])\n",
    "# save to csv\n",
    "df_apn.to_csv('missing_interpolation_zstats_apn_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Campgrounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Campground Occupancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out Bayview Campground from dfCamp\n",
    "dfCamp = dfCamp.loc[dfCamp['Campground'] != 'Bayview Campground']\n",
    "\n",
    "# merge campground data with occupancy rate data on campground name\n",
    "dfCampOcc = sdf_campground.merge(dfCamp, left_on='RECREATION_NAME', right_on='Campground', \n",
    "                                      how='left', indicator=True)\n",
    "\n",
    "# spatial join TAZ data to campground data\n",
    "arcpy.SpatialJoin_analysis(dfCampOcc, sdf_taz, 'taz_campground', \n",
    "                           'JOIN_ONE_TO_ONE', 'KEEP_ALL', \n",
    "                           match_option='HAVE_THEIR_CENTER_IN')\n",
    "\n",
    "# read in output of spatial join as sdf\n",
    "sdf_campground_taz = pd.DataFrame.spatial.from_featureclass('taz_campground')\n",
    "\n",
    "# get sites sold by multiplying the number of sites by the occupancy rate\n",
    "sdf_campground_taz['SitesSold'] = sdf_campground_taz['Total_Sites'] * sdf_campground_taz['Occupancy_Rate']\n",
    "\n",
    "# group by TAZ and sum of sites sold within TAZ\n",
    "sdf_campground_taz_grouped = sdf_campground_taz.groupby('TAZ').agg(\n",
    "                                                {'Total_Sites': 'sum',\n",
    "                                                'SitesSold': 'sum',\n",
    "                                                'Occupancy_Rate': 'mean'\n",
    "                                                 }).reset_index()\n",
    "\n",
    "# sdf_campground to pickle\n",
    "sdf_campground_taz_grouped.to_pickle(campground_pickle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Interpolation to fill nan if neccesary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overnight Visitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdfParcel    = pd.read_pickle(parcel_pickle_part4)\n",
    "sdfParcel\n",
    "dfCampground = pd.read_pickle(campground_pickle)\n",
    "sdfTAZ       = sdf_taz.copy()\n",
    "overnight_fields = ['taz', \n",
    "                    'hotelmotel', # Hotel/Motel total rooms available by TAU_TYPE = hotelmotel\n",
    "                    'resort',     # Resort total rooms is coming from ### We dont have occupany rates for these ###\n",
    "                    'casino',     # Casino total rooms available by TAU_TYPE = casino\n",
    "                    'campground', # Campground total sites avaialble\n",
    "                    'percentHouseSeasonal', # Percent of houses that are seasonal ### total units- (probability unit is seasonal) / total units ###\n",
    "                    # 'beach'      # NA\n",
    "                    ]\n",
    "\n",
    "df_overnight = pd.DataFrame(columns=overnight_fields)\n",
    "df_overnight['taz'] = sdfTAZ['TAZ'].astype(int)\n",
    "\n",
    "df_overnight.campground = df_overnight.taz.map(dict(zip(dfCampground['TAZ'], dfCampground['SitesSold'])))\n",
    "df_overnight.hotelmotel = sdfParcel.loc[sdfParcel['TAU_TYPE'] == 'HotelMotel'].groupby('TAZ')['TouristAccommodation_Units'].sum()\n",
    "df_overnight.casino     = sdfParcel.loc[sdfParcel['TAU_TYPE'] == 'Casino'].groupby('TAZ')['TouristAccommodation_Units'].sum()\n",
    "df_overnight.resort     = sdfParcel.loc[sdfParcel['TAU_TYPE'] == 'Resort'].groupby('TAZ')['TouristAccommodation_Units'].sum()\n",
    "# df_overnight.percentHouseSeasonal = sdfParcel.groupby('TAZ')['Seasonal'].mean()\n",
    "df_overnight.fillna(0, inplace=True)\n",
    "df_overnight.astype(int)\n",
    "df_overnight.to_pickle(visitor_pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## School Enrollment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## df_school_enrollment is missing meyers elementary, bijou, sierra house, and LTCC and Sierra Nevada College\n",
    "df_school_enrollment_22 = df_school_enrollment[df_school_enrollment['Year'] == '2022-2023']\n",
    "# Add a row for LTCC - Lake Tahoe Community College\n",
    "ltcc = {'School_Name': 'Lake Tahoe Community College', 'Level_': 'College', 'Enrollment': 2909}\n",
    "df_school_enrollment_22 = pd.concat([df_school_enrollment_22, pd.DataFrame([ltcc])], ignore_index=True)\n",
    "# join school spatial to school table\n",
    "sdf_school_enroll = pd.merge(sdf_school, df_school_enrollment, left_on='SchoolID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a copy of the school data\n",
    "sdf_school = sdf_school.copy()\n",
    "# set Type to Null\n",
    "sdf_school['TYPE'] = None\n",
    "# set SchoolType to 'elementary' if it contains 'elementary' or 'magnet' or 'academy'\n",
    "sdf_school.loc[sdf_school['NAME'].str.contains('elementary', case=False), \n",
    "               'TYPE'] = 'Elementary School'\n",
    "# set SchoolType to 'middle' if it contains 'middle'\n",
    "sdf_school.loc[sdf_school['NAME'].str.contains('middle', case=False), \n",
    "               'TYPE'] = 'Middle School'\n",
    "# set SchoolType to 'high' if it contains 'high'\n",
    "sdf_school.loc[sdf_school['NAME'].str.contains('high', case=False), \n",
    "               'TYPE'] = 'High School'\n",
    "# set SchoolType to 'college' if it contains 'college'\n",
    "sdf_school.loc[sdf_school['NAME'].str.contains('college', case=False), \n",
    "               'TYPE'] = 'College'\n",
    "# set School Type to 'other' if it it does not contain any of the above\n",
    "sdf_school.loc[sdf_school['TYPE'].isnull(), \n",
    "               'TYPE'] = 'Elementary School'\n",
    "\n",
    "# spatial join TAZs to School points, gettting the TAZ for each school\n",
    "sdf_school_taz = sdf_school.spatial.join(sdf_taz, how='inner')\n",
    "\n",
    "# group by TYPE and sum of Enrollment within TAZ \n",
    "sdf_school_taz_grouped = sdf_school_taz.groupby(['TYPE', 'TAZ']).agg(\n",
    "                                                {'ENROLLMENT': 'sum'}).reset_index()\n",
    "\n",
    "# unstack by TYPE as columns and TAZ as a column\n",
    "sdf_school_taz_grouped_pivot = sdf_school_taz_grouped.pivot(index='TAZ', \n",
    "                                                            columns='TYPE', \n",
    "                                                            values='ENROLLMENT').reset_index()\n",
    "\n",
    "# merge to sdf_taz to get all tazs - the join only gets us the tazs with schools\n",
    "df_school = pd.merge(sdf_taz, sdf_school_taz_grouped_pivot, how='left', on='TAZ')\n",
    "\n",
    "# rename columns\n",
    "df_school.rename(columns={'Elementary School':'elementary_school_enrollment',\n",
    "                          'Middle School':'middle_school_enrollment',\n",
    "                          'High School':'high_school_enrollment',\n",
    "                          'College':'college_enrollment'}, inplace=True)\n",
    "\n",
    "# group by TAZ, sum of enrollment by school type\n",
    "schools_final = df_school.groupby('TAZ').agg({'elementary_school_enrollment':'sum',\n",
    "                                              'middle_school_enrollment':'sum',\n",
    "                                              'high_school_enrollment':'sum',\n",
    "                                              'college_enrollment':'sum'}).reset_index()\n",
    "\n",
    "# fields to integer and fill na with 0\n",
    "schools_final = schools_final.fillna(0).astype(int)\n",
    "# to pickle\n",
    "schools_final.to_pickle(school_pickle)\n",
    "# to csv\n",
    "schools_final.to_csv(os.path.join(out_dir,'SchoolEnrollment.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Socio Econ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get relevant census variables and calculate rates at block group level\n",
    "# Get Occupancy Data - B25002_003E = Vacant, B25002_002E = Occupied , B25004_006E = Vacant Seasonal\n",
    "occupancy_codes = ['B25002_003E','B25002_002E', 'B25004_006E']\n",
    "df_census_occupancy = df_census_2022[df_census_2022['variable_code'].isin(occupancy_codes)]\n",
    "df_census_occupancy = df_census_occupancy[['TRPAID', 'variable_code', 'value']]\n",
    "# pivot to wide format so we can calculate percentages and totals\n",
    "df_census_occupancy = df_census_occupancy.pivot(index='TRPAID', columns='variable_code', values='value').reset_index()\n",
    "# vacant units + occupied units = total units\n",
    "df_census_occupancy['total_units'] = df_census_occupancy['B25002_003E'] + df_census_occupancy['B25002_002E']\n",
    "# occupancy rate = occupied units / total units\n",
    "df_census_occupancy['occupancy_rate'] = df_census_occupancy['B25002_002E'] / df_census_occupancy['total_units']\n",
    "# seasonal rate = seasonal units / total units\n",
    "df_census_occupancy['seasonal_rate'] = df_census_occupancy['B25004_006E'] / df_census_occupancy['total_units']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Household Size Data - B25010_001E = Total Households\n",
    "df_census_household_size = df_census_2022[df_census_2022['variable_code'] == 'B25010_001E']\n",
    "df_census_household_size = df_census_household_size[['TRPAID', 'variable_code', 'value']]\n",
    "df_census_household_size = df_census_household_size.pivot(index='TRPAID', columns='variable_code', values='value').reset_index()\n",
    "df_census_household_size['household_size'] = df_census_household_size['B25010_001E']\n",
    "#adjust household size by a constant factor so that total residents = total population from acs\n",
    "ACS_total_population = 53842\n",
    "# This is from a final calculated input summary - will eventually make an explicit calculation\n",
    "total_calculated_population = 52570.83\n",
    "household_size_adjustment = ACS_total_population / total_calculated_population\n",
    "df_census_household_size['household_size'] = df_census_household_size['household_size'] * household_size_adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Codes by the category they fall into - Census categroy to broader category\n",
    "code_lookup = pd.read_csv('Lookup_Lists/income_census_codes.csv')\n",
    "#Filter census so only variable codes in the code lookup are included\n",
    "df_census_income = df_census_2022[df_census_2022['variable_code'].isin(code_lookup['variable_code'])]\n",
    "#Create a new column that has a value from code lookup based on the variable code\n",
    "df_census_income['income_category'] = df_census_income['variable_code'].map(code_lookup.set_index('variable_code')['category'])\n",
    "#group by block group and income category and sum the values\n",
    "df_census_income = df_census_income.groupby(['TRPAID','income_category'])['value'].sum().reset_index()\n",
    "df_census_income = df_census_income.pivot(index='TRPAID', columns='income_category', values='value').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRPAID is a 16 digit ID, but it is imported as a float. Convert to string and to retain leading zeros\n",
    "df_census_household_size['TRPAID']= df_census_household_size['TRPAID'].astype(str).str.zfill(16)\n",
    "df_census_income['TRPAID']= df_census_income['TRPAID'].astype(str).str.zfill(16)\n",
    "# merge all the census data together\n",
    "df_census_occupancy_all = pd.merge(df_census_occupancy, df_census_household_size, on='TRPAID', how='left')\n",
    "df_census_all = pd.merge(df_census_occupancy_all, df_census_income, on='TRPAID', how='left')\n",
    "# rename columns of df_census_all\n",
    "column_rename = {\n",
    "    'B25002_003E': 'vacant_units',\n",
    "    'B25002_002E': 'occupied_units',\n",
    "    'B25004_006E': 'seasonal_units',\n",
    "    'High Income': 'high_income',\n",
    "    'Low Income': 'low_income',\n",
    "    'Medium Income': 'middle_income',\n",
    "}\n",
    "df_census_all.rename(columns=column_rename, inplace=True)\n",
    "\n",
    "df_census_all.drop(columns=['B25010_001E'], inplace=True)\n",
    "# calculate proportions of income categories\n",
    "df_census_all['high_income_proportion'] = df_census_all['high_income'] / df_census_all['occupied_units']\n",
    "df_census_all['middle_income_proportion'] = df_census_all['middle_income'] / df_census_all['occupied_units']\n",
    "df_census_all['low_income_proportion'] = df_census_all['low_income'] / df_census_all['occupied_units']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pickle part 3\n",
    "sdfParcel = pd.read_pickle(parcel_pickle_part3)\n",
    "\n",
    "# map values from Census data to parcel data via left_on BLOCK_GROUP and right_on TRPAID\n",
    "sdfParcel.PrimaryResidence_Rate   = sdfParcel.BLOCK_GROUP.map(df_census_all.set_index('TRPAID')['occupancy_rate'])\n",
    "sdfParcel.SecondaryResidence_Rate = sdfParcel.BLOCK_GROUP.map(df_census_all.set_index('TRPAID')['seasonal_rate'])\n",
    "sdfParcel.HighIncome_Rate         = sdfParcel.BLOCK_GROUP.map(df_census_all.set_index('TRPAID')['high_income_proportion'])\n",
    "sdfParcel.MediumIncome_Rate       = sdfParcel.BLOCK_GROUP.map(df_census_all.set_index('TRPAID')['middle_income_proportion'])\n",
    "sdfParcel.LowIncome_Rate          = sdfParcel.BLOCK_GROUP.map(df_census_all.set_index('TRPAID')['low_income_proportion'])\n",
    "sdfParcel.PersonsPerUnit          = sdfParcel.BLOCK_GROUP.map(df_census_all.set_index('TRPAID')['household_size'])\n",
    "\n",
    "# seasonal rate calculation\n",
    "# group by BLOCK_GROUP\n",
    "# filter sdfParcel where VHR   = 'Yes'\n",
    "vhrs = sdfParcel.loc[sdfParcel['VHR']=='Yes']\n",
    "totalRes = sdfParcel.groupby('BLOCK_GROUP').agg({'Residential_Units':'sum', 'PrimaryResidence_Rate':'mean', 'SecondaryResidence_Rate':'mean'}).reset_index()\n",
    "totalVHR = vhrs.groupby('BLOCK_GROUP').agg({'Residential_Units':'sum'}).reset_index()\n",
    "totalVHR.rename(columns={'Residential_Units':'VHR_Units'}, inplace=True)\n",
    "\n",
    "# merge totalRes and totalVHR\n",
    "totalResVHR = pd.merge(totalRes, totalVHR, on='BLOCK_GROUP', how='left')\n",
    "# fill NA with 0\n",
    "totalResVHR.VHR_Units = totalResVHR.VHR_Units.fillna(0)\n",
    "# calculate seasonal rate\n",
    "totalResVHR['non_vhr_units'] = totalResVHR['Residential_Units'] - totalResVHR['VHR_Units']\n",
    "# calculate the non-adjusted number of seasonal units and then subtract the number of VHRs\n",
    "totalResVHR['non_adjusted_seasonal_units'] = totalResVHR['SecondaryResidence_Rate'] * totalResVHR['Residential_Units']\n",
    "totalResVHR['non_primary_residence_units'] = totalResVHR['Residential_Units']-(totalResVHR['PrimaryResidence_Rate'] * totalResVHR['Residential_Units'])\n",
    "totalResVHR['adjusted_seasonal_units']     = totalResVHR['non_adjusted_seasonal_units'] - totalResVHR['VHR_Units']\n",
    "# Manually adjust the seasonal units for block group 3200500170022020 because of a lag in the data\n",
    "# The census reports 100% occupancy but I think it has to do with the beach club development\n",
    "totalResVHR.loc[totalResVHR['BLOCK_GROUP'] == '3200500170022020', 'adjusted_seasonal_units'] = 0\n",
    "# calculate the adjusted seasonal rate\n",
    "totalResVHR['adjusted_seasonal_rate'] = totalResVHR['adjusted_seasonal_units'] / totalResVHR['non_primary_residence_units']\n",
    "\n",
    "# map the adjusted seasonal rate to the parcel data\n",
    "sdfParcel['SecondaryResidence_Rate'] = sdfParcel['BLOCK_GROUP'].map(totalResVHR.set_index('BLOCK_GROUP')['adjusted_seasonal_rate'])\n",
    "\n",
    "# export to pickle part 4\n",
    "sdfParcel.to_pickle(parcel_pickle_part4)\n",
    "sdfParcel.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Employment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top line employment data for NV from 2018 lives here: ????\n",
    "# we got employment data from NV at the Tahoe Basin level by NAICS code....\n",
    "\n",
    "# get the employment data\n",
    "nv_employ = pd.read_csv('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WE HAVE GOOD DATA FOR CASINO EMPLOYMENT on the South Shore ## \n",
    "# for employment data we have multiple years of CA EDD data\n",
    "# california employment development department data for 2018 and 2022 was transformed to a feature class and spatial joined to TAZs and Block Group\n",
    "# exported to a csv\n",
    "# stacekd data by temporal scale\n",
    "# grouped by TAZ and NAICS code, and summed employment\n",
    "# F:\\GIS\\PROJECTS\\ResearchAnalysis\\Employment\\Data\\EDD_Grouped\n",
    "# F:\\GIS\\PROJECTS\\ResearchAnalysis\\Employment\\\n",
    "# then looking at difference of total and trends over time (month-month) and year over year\n",
    "#\n",
    "# LODES data https://maps.trpa.org/server/rest/services/LTinfo_Climate_Resilience_Dashboard/MapServer/142\n",
    "\n",
    "# compare 2018 to 2022 by block group \n",
    "\n",
    "# checking trends of each. \n",
    "#  - what is the trend of employment by NAICS code\n",
    "#  - what is the trend of employment by TAZ\n",
    "#  - what is the trend of employment by block group\n",
    "#  - what is the trend of employment by zip code\n",
    "# \n",
    "# CBP data for 2018 and 2022\n",
    "# data is mostly in the service. or in Vector.sde>Census>Jobs\n",
    "# look at comparisons of trends by same geography and temporal scale\n",
    "\n",
    "# workflow is to get the data, clean it, join it to the spatial data, then group by the spatial data and sum the employment\n",
    "# \n",
    "# establish trends for CA for the three datasources...compare the trends and see if they are similar\n",
    "# \n",
    "\n",
    "### NAICS codes are one order higher in LODES data, CA EDD and CBP data have the same granularity of NAICS codes\n",
    "### LODES is by year so the trend might be different if there is a sesaonal component to the data\n",
    "\n",
    "# we'll have two of the three datasets analyzed for Nevada and all three in California.\n",
    "    # where we have all three datasets we'll compare the trends and see if they are similar\n",
    "    # we'll look at the trends for each dataset and see if they are similar\n",
    "    # we'll look at the trends for each geography and see if they are similar\n",
    "    # we'll look at the trends for each temporal scale and see if they are similar\n",
    "\n",
    "# For Nevada we have block level data for 2018 so if consistent with 2022 we can use that as a proxy for 2022\n",
    "\n",
    "# we subtract out any known employment from the 2018 data (e.g. Lakeside Inn) and compare the trends\n",
    "# generate adjustment factors by sector and apply those adjustments to the 2018 data that was aggregated to the TAZ level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAZ Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data Again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataframe copies\n",
    "sdfTAZ = sdf_taz.copy()\n",
    "dfCamp = dfCamp.copy()\n",
    "# filter out Bayview Campground from dfCamp\n",
    "dfCamp = dfCamp.loc[dfCamp['Campground'] != 'Bayview Campground']\n",
    "# read in the pickles \n",
    "sdfParcel    = pd.read_pickle(parcel_pickle_part4)\n",
    "dfVisitor    = pd.read_pickle(visitor_pickle)\n",
    "dfCampground = pd.read_pickle(campground_pickle)\n",
    "dfSchool     = pd.read_pickle(school_pickle)\n",
    "# sdfSocio  = pd.read_pickle(socioeconomic_pickle)\n",
    "# sdfEmploy = pd.read_pickle(employment_pickle)\n",
    "dfOccupancy = pd.read_pickle(occupancy_rates_pickle)\n",
    "\n",
    "# remove duplicate APNs from sdfParcel\n",
    "sdfParcel = sdfParcel.drop_duplicates(subset=['APN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore which fields we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all csv files in the data directory\n",
    "csv_files = list(out_dir.glob('*.csv'))\n",
    "csv_files\n",
    "# read in summary files as dataframes and get columns to lists\n",
    "dfOverNight  = pd.read_csv(out_dir/'OvernightVisitorZonalData_Summer.csv')\n",
    "dfEmployment = pd.read_csv(out_dir/'Employment.csv')\n",
    "dfSchoolEnrl = pd.read_csv(out_dir/'SchoolEnrollment.csv')\n",
    "dfSocioEcon  = pd.read_csv(out_dir/'SocioEcon_Summer.csv')\n",
    "dfVisitor    = pd.read_csv(out_dir/'VisitorOccupancyRates_Summer.csv')\n",
    "dfInputs     = pd.read_csv(out_dir/'inputs_summarized.csv')\n",
    "\n",
    "# get lists of columns\n",
    "overnight_fields  = dfOverNight.columns.tolist()\n",
    "employment_fields = dfEmployment.columns.tolist()\n",
    "school_fields     = dfSchoolEnrl.columns.tolist()\n",
    "socio_fields      = dfSocioEcon.columns.tolist()\n",
    "visitor_fields    = dfVisitor.columns.tolist()\n",
    "inputs_fields     = dfInputs.columns.tolist()\n",
    "# print the lists\n",
    "overnight_fields, employment_fields, school_fields, socio_fields, visitor_fields, inputs_fields\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overnight_fields = ['taz', \n",
    "                    'hotelmotel', # Hotel/Motel total rooms available by TAU_TYPE = hotelmotel\n",
    "                    'resort',     # Resort total rooms is coming from ### We dont have occupany rates for these ###\n",
    "                    'casino',     # Casino total rooms available by TAU_TYPE = casino\n",
    "                    'campground', # Campground total sites avaialble\n",
    "                    'percentHouseSeasonal', # Percent of houses that are seasonal ### total units- (probability unit is seasonal) / total units ###\n",
    "                    'beach'      # NA\n",
    "                    ]\n",
    "\n",
    "employment_fields = ['TAZ', \n",
    "                    'emp_other', # Other employment equals total employees in NAICS codes\n",
    "                    'emp_rec',   # Recreation employment\n",
    "                    'emp_retail',# Retail employment\n",
    "                    'emp_srvc',  # Service employment\n",
    "                    'emp_gaming' # Gaming employment\n",
    "                    ]\n",
    "\n",
    "school_fields     = ['taz',\n",
    "                    'elementary_school_enrollment', # Elementary school enrollment\n",
    "                    'middle_school_enrollment',     # Middle school enrollment\n",
    "                    'high_school_enrollment',       # High school enrollment\n",
    "                    'college_enrollment'            # College enrollment\n",
    "                    ]\n",
    "\n",
    "socio_fields      = ['taz',\n",
    "                    'total_residential_units', # Total residential units in parcels\n",
    "                    'census_occ_rate',         # Census occupancy rate \n",
    "                    'total_occ_units',         # Total occupied units \n",
    "                    'occ_units_low_inc',       # Low income occupied units\n",
    "                    'occ_units_med_inc',       # Medium income occupied units\n",
    "                    'occ_units_high_inc',      # High income occupied units\n",
    "                    'persons_per_occ_unit',    # Persons per occupied unit\n",
    "                    'total_persons',           # Total persons\n",
    "                    'emp_retail',              # Retail employment\n",
    "                    'emp_srvc',                # Service employment\n",
    "                    'emp_rec',                 # Recreation employment\n",
    "                    'emp_game',                # Gaming employment\n",
    "                    'emp_other'                # Other employment\n",
    "                    ]\n",
    "\n",
    "visitor_fields    = ['taz', \n",
    "                    'hotelmotel',    # Hotel/Motel rooms\n",
    "                    'resort',        # Resort rooms\n",
    "                    'casino',        # Casino rooms\n",
    "                    'campground',    # Campground sites sold?\n",
    "                    'house',         # House units\n",
    "                    'seasonal'       # Seasonal units\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input File Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the totals at the parcel level\n",
    "sdfParcel['OccupiedUnits']   = sdfParcel.Residential_Units * sdfParcel.PrimaryResidence_Rate\n",
    "sdfParcel['UnoccupiedUnits'] = sdfParcel.Residential_Units - sdfParcel.OccupiedUnits\n",
    "sdfParcel['SeasonalUnits']   = sdfParcel.UnoccupiedUnits * sdfParcel.SecondaryResidence_Rate\n",
    "sdfParcel['HighUnits']       = sdfParcel.OccupiedUnits * sdfParcel.HighIncome_Rate\n",
    "sdfParcel['MediumUnits']     = sdfParcel.OccupiedUnits * sdfParcel.MediumIncome_Rate\n",
    "sdfParcel['LowUnits']        = sdfParcel.OccupiedUnits * sdfParcel.LowIncome_Rate\n",
    "sdfParcel['People']          = sdfParcel.OccupiedUnits * sdfParcel.PersonsPerUnit    \n",
    "\n",
    "# group by TAZ and sum of units\n",
    "sdfParcel_taz = sdfParcel.groupby('TAZ').agg(\n",
    "                                                {'Residential_Units':'sum', \n",
    "                                                 'OccupiedUnits':'sum', \n",
    "                                                 'SeasonalUnits':'sum', \n",
    "                                                 'HighUnits':'sum', \n",
    "                                                 'MediumUnits':'sum', \n",
    "                                                 'LowUnits':'sum',\n",
    "                                                 'PersonsPerUnit':'mean',\n",
    "                                                 'People':'sum',\n",
    "                                                 'TAU_Occupancy_Rate': 'mean',\n",
    "                                                 'SecondaryResidence_Rate':'mean',\n",
    "                                                 'VHR_Occupancy_Rate':'mean',\n",
    "                                                 'PrimaryResidence_Rate':'mean'}).reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Generate 'OvernightVisitorZonalData_Summer.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcel_TAU_grouped = sdfParcel.groupby(['TAZ','TAU_TYPE'])['TouristAccommodation_Units'].sum().reset_index()\n",
    "parcel_TAU_grouped = parcel_TAU_grouped.pivot(index='TAZ', columns='TAU_TYPE', values='TouristAccommodation_Units').reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the overnight visitor dataframe, pickle and csv\n",
    "overnight_fields = ['taz', \n",
    "                    'hotelmotel', # Hotel/Motel total rooms available by TAU_TYPE = hotelmotel\n",
    "                    'resort',     # Resort total rooms is coming from ### We dont have occupany rates for these ###\n",
    "                    'casino',     # Casino total rooms available by TAU_TYPE = casino\n",
    "                    'campground', # Campground total sites avaialble\n",
    "                    'percentHouseSeasonal', # Percent of houses that are seasonal ### total units- (probability unit is seasonal) / total units ###\n",
    "                    # 'beach'      # NA\n",
    "                    ]\n",
    "\n",
    "df_overnight = pd.DataFrame(columns=overnight_fields)\n",
    "df_overnight['taz'] = sdfTAZ['TAZ'].astype(int)\n",
    "\n",
    "df_overnight.campground           = df_overnight.taz.map(dict(zip(dfCampground['TAZ'], dfCampground['Total_Sites'])))\n",
    "df_overnight.percentHouseSeasonal = df_overnight.taz.map(dict(zip(sdfParcel_taz['TAZ'], sdfParcel_taz['SecondaryResidence_Rate'])))\n",
    "df_overnight.hotelmotel           = df_overnight.taz.map(dict(zip(parcel_TAU_grouped['TAZ'], parcel_TAU_grouped['HotelMotel'])))\n",
    "df_overnight.casino               = df_overnight.taz.map(dict(zip(parcel_TAU_grouped['TAZ'], parcel_TAU_grouped['Casino'])))\n",
    "df_overnight.resort               = df_overnight.taz.map(dict(zip(parcel_TAU_grouped['TAZ'], parcel_TAU_grouped['Resort'])))\n",
    "\n",
    "# fill NA with 0\n",
    "df_overnight.fillna(0, inplace=True)\n",
    "df_overnight.astype(int)\n",
    "df_overnight.to_pickle(visitor_pickle)\n",
    "df_overnight.to_csv(os.path.join(out_dir,'OvernightVisitorZonalData_Summer.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Generate 'SocioEcon_Summer.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the socio-economic dataframe, pickle and csv\n",
    "socio_fields      = ['taz',\n",
    "                    'total_residential_units', # Total residential units in parcels\n",
    "                    'census_occ_rate',         # Census occupancy rate \n",
    "                    'total_occ_units',         # Total occupied units \n",
    "                    'occ_units_low_inc',       # Low income occupied units\n",
    "                    'occ_units_med_inc',       # Medium income occupied units\n",
    "                    'occ_units_high_inc',      # High income occupied units\n",
    "                    'persons_per_occ_unit',    # Persons per occupied unit\n",
    "                    'total_persons',           # Total persons\n",
    "                    'emp_retail',              # Retail employment\n",
    "                    'emp_srvc',                # Service employment\n",
    "                    'emp_rec',                 # Recreation employment\n",
    "                    'emp_game',                # Gaming employment\n",
    "                    'emp_other'                # Other employment\n",
    "                    ]\n",
    "\n",
    "df_socio = pd.DataFrame(columns=socio_fields)\n",
    "df_socio['taz'] = sdfTAZ['TAZ'].astype(int)\n",
    "df_socio.total_residential_units = df_socio.taz.map(dict(zip(sdfParcel_taz['TAZ'], sdfParcel_taz['Residential_Units'])))\n",
    "df_socio.census_occ_rate         = df_socio.taz.map(dict(zip(sdfParcel_taz['TAZ'], sdfParcel_taz['PrimaryResidence_Rate'])))\n",
    "df_socio.total_occ_units         = df_socio.taz.map(dict(zip(sdfParcel_taz['TAZ'], sdfParcel_taz['OccupiedUnits'])))\n",
    "df_socio.occ_units_low_inc       = df_socio.taz.map(dict(zip(sdfParcel_taz['TAZ'], sdfParcel_taz['LowUnits'])))\n",
    "df_socio.occ_units_med_inc       = df_socio.taz.map(dict(zip(sdfParcel_taz['TAZ'], sdfParcel_taz['MediumUnits'])))\n",
    "df_socio.occ_units_high_inc      = df_socio.taz.map(dict(zip(sdfParcel_taz['TAZ'], sdfParcel_taz['HighUnits'])))\n",
    "df_socio.persons_per_occ_unit    = df_socio.taz.map(dict(zip(sdfParcel_taz['TAZ'], sdfParcel_taz['PersonsPerUnit'])))\n",
    "df_socio.total_persons           = df_socio.taz.map(dict(zip(sdfParcel_taz['TAZ'], sdfParcel_taz['People'])))\n",
    "\n",
    "# df_socio.emp_retail              = df_socio.taz.map(dict(zip(dfEmployment['TAZ'], dfEmployment['emp_retail'])))\n",
    "# df_socio.emp_srvc                = df_socio.taz.map(dict(zip(dfEmployment['TAZ'], dfEmployment['emp_srvc'])))\n",
    "# df_socio.emp_rec                 = df_socio.taz.map(dict(zip(dfEmployment['TAZ'], dfEmployment['emp_rec'])))\n",
    "# df_socio.emp_game                = df_socio.taz.map(dict(zip(dfEmployment['TAZ'], dfEmployment['emp_game'])))\n",
    "# df_socio.emp_other               = df_socio.taz.map(dict(zip(dfEmployment['TAZ'], dfEmployment['emp_other'])))\n",
    "\n",
    "df_socio.fillna(0, inplace=True)\n",
    "df_socio.to_pickle(visitor_pickle)\n",
    "df_socio.to_csv(os.path.join(out_dir,'SocioEcon_Summer.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Generate 'Employment.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the employment dataframe, pickle and csv\n",
    "employment_fields = ['TAZ', \n",
    "                    'emp_other', # Other employment equals total employees in NAICS codes\n",
    "                    'emp_rec',   # Recreation employment\n",
    "                    'emp_retail',# Retail employment\n",
    "                    'emp_srvc',  # Service employment\n",
    "                    'emp_gaming' # Gaming employment\n",
    "                    ]\n",
    "# create the employment dataframe\n",
    "df_employ = pd.DataFrame(columns=employment_fields)\n",
    "# set the field values\n",
    "df_employ['taz'] = sdfTAZ['TAZ'].astype(int)\n",
    "df_employ.emp_other = df_employ.taz.map(dict(zip(dfEmployment['TAZ'], dfEmployment['emp_other'])))\n",
    "df_employ.emp_rec   = df_employ.taz.map(dict(zip(dfEmployment['TAZ'], dfEmployment['emp_rec'])))\n",
    "df_employ.emp_retail= df_employ.taz.map(dict(zip(dfEmployment['TAZ'], dfEmployment['emp_retail'])))\n",
    "df_employ.emp_srvc  = df_employ.taz.map(dict(zip(dfEmployment['TAZ'], dfEmployment['emp_srvc'])))\n",
    "df_employ.emp_gaming= df_employ.taz.map(dict(zip(dfEmployment['TAZ'], dfEmployment['emp_gaming'])))\n",
    "# fill NA with 0\n",
    "df_employ.fillna(0, inplace=True)\n",
    "# save to pickle and csv\n",
    "df_employ.to_pickle(employment_pickle)\n",
    "df_employ.to_csv(os.path.join(out_dir,'Employment.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Generate 'VisitorOccupancyRates_Summer.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcel_TAU_rates_grouped = sdfParcel.groupby(['TAZ','TAU_TYPE'])['TAU_Occupancy_Rate'].mean().reset_index()\n",
    "parcel_TAU_rates_grouped = parcel_TAU_rates_grouped.pivot(index='TAZ', columns='TAU_TYPE', values='TAU_Occupancy_Rate').reset_index()\n",
    "parcel_vhr_rates_grouped = sdfParcel.loc[sdfParcel['VHR']=='Yes'].groupby('TAZ')['VHR_Occupancy_Rate'].mean().reset_index() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the visitor dataframe, pickle and csv\n",
    "visitor_occupany_fields  = ['taz', \n",
    "                            'hotelmotel',    # Hotel/Motel rooms\n",
    "                            'resort',        # Resort rooms\n",
    "                            'casino',        # Casino rooms\n",
    "                            'campground',    # Campground sites sold?\n",
    "                            'house',         # House units\n",
    "                            'seasonal'       # Seasonal units\n",
    "                            ]\n",
    "# create the visitor dataframe\n",
    "df_visitor_occ = pd.DataFrame(columns=visitor_occupany_fields)\n",
    "# set the field values\n",
    "df_visitor_occ['taz']     = sdfTAZ['TAZ'].astype(int)\n",
    "df_visitor_occ.hotelmotel = df_visitor_occ.taz.map(dict(zip(parcel_TAU_rates_grouped['TAZ'], parcel_TAU_rates_grouped['HotelMotel'])))\n",
    "df_visitor_occ.resort     = df_visitor_occ.taz.map(dict(zip(parcel_TAU_rates_grouped['TAZ'], parcel_TAU_rates_grouped['Resort'])))\n",
    "df_visitor_occ.casino     = df_visitor_occ.taz.map(dict(zip(parcel_TAU_rates_grouped['TAZ'], parcel_TAU_rates_grouped['Casino'])))\n",
    "df_visitor_occ.house      = df_visitor_occ.taz.map(dict(zip(parcel_vhr_rates_grouped['TAZ'], parcel_vhr_rates_grouped['VHR_Occupancy_Rate'])))\n",
    "df_visitor_occ.seasonal   = df_visitor_occ.taz.map(dict(zip(parcel_vhr_rates_grouped['TAZ'], parcel_vhr_rates_grouped['VHR_Occupancy_Rate'])))\n",
    "df_visitor_occ.campground = df_visitor_occ.taz.map(dict(zip(dfCampground['TAZ'], dfCampground['Occupancy_Rate'])))  \n",
    " \n",
    "# fill NA with 0\n",
    "df_visitor_occ.fillna(0, inplace=True)\n",
    "# save to pickle and csv\n",
    "df_visitor_occ.to_pickle(visitor_pickle)\n",
    "df_visitor_occ.to_csv(os.path.join(out_dir,'VisitorOccupancyRates_Summer.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basin Summary\n",
    "> update and compare to 'RTP_20_base_year_2018', 'RTP_17_base_year_2014' in TravelDemandModel\\2022\\data\\processed_data\\inputs_summarized.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdfParcel.Residential_Units.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the field values\n",
    "value_dict = {\n",
    "'lodging occupancy rate'   :(sdfParcel.loc[sdfParcel['TAU_TYPE'].isin(['HotelMotel','Resort','Casino'])]['TAU_Occupancy_Rate'].mean()),\n",
    "'campground occupancy rate':dfCamp['Occupancy_Rate'].mean(),\n",
    "'house(VHR) rate'          :(sdfParcel.loc[sdfParcel['VHR']=='Yes']['VHR_Occupancy_Rate'].mean()),\n",
    "'seasonal rate'            :(sdfParcel.loc[sdfParcel['VHR']=='Yes']['VHR_Occupancy_Rate'].mean()),\n",
    "'lodging unit'             :sdfParcel.TouristAccommodation_Units.sum(),\n",
    "'campground'               :dfCampground.Total_Sites.sum(),\n",
    "'percentHouseSeasonal'     :(sdfParcel_taz['SeasonalUnits'].sum() / (sdfParcel_taz['Residential_Units'].sum()-sdfParcel_taz['OccupiedUnits'].sum())).mean(),\n",
    "'school enrollment'        :(dfSchool['elementary_school_enrollment'] + dfSchool['middle_school_enrollment'] + dfSchool['high_school_enrollment'] + dfSchool['college_enrollment']).sum(),\n",
    "'employment'               :0, # need to get this data\n",
    "'residential unit'         :sdfParcel.Residential_Units.sum(),\n",
    "'total persons'            :sdfParcel_taz['People'].sum(),\n",
    "'census occupancy rate'    :(sdfParcel_taz['OccupiedUnits'].sum() / sdfParcel_taz['Residential_Units'].sum()).mean(),\n",
    "'low income res unit'      :sdfParcel_taz['LowUnits'].sum(),\n",
    "'medium income res unit'   :sdfParcel_taz['MediumUnits'].sum(),\n",
    "'high income res unit'     :sdfParcel_taz['HighUnits'].sum(),\n",
    "'total occupied unit'      :sdfParcel_taz['OccupiedUnits'].sum(),\n",
    "'persons per occupied unit':(sdfParcel_taz['People'].sum() / sdfParcel_taz['OccupiedUnits'].sum()).mean(),\n",
    "}\n",
    "\n",
    "\n",
    "# get inputs_summarized.csv as a dataframe\n",
    "dfInputs = pd.read_csv(os.path.join(out_dir,'inputs_summarized copy.csv'))\n",
    "# add column to the dataframe 'RTP_24_base_year_2022'\n",
    "dfInputs['RTP_24_base_year_2022'] = 0\n",
    "# use dictionary to map values to dfInputs['RTP_24_base_year_2022']\n",
    "dfInputs['RTP_24_base_year_2022'] = dfInputs['category'].map(value_dict)\n",
    "# round all values to 2 decimal places\n",
    "dfInputs = dfInputs.round(2)\n",
    "# drop column 'Unnamed: 0'\n",
    "dfInputs.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "# save to csv\n",
    "dfInputs.to_csv(os.path.join(out_dir,'inputs_summarized.csv'), index=False)\n",
    "# save to pickle\n",
    "dfInputs.to_pickle(summary_pickle)\n",
    "dfInputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdfParcel1 = pd.read_pickle(parcel_pickle_part1)\n",
    "sdfParcel2 = pd.read_pickle(parcel_pickle_part2)\n",
    "sdfParcel3 = pd.read_pickle(parcel_pickle_part3)\n",
    "sdfParcel4 = pd.read_pickle(parcel_pickle_part4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_units.Residential_Units.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through dataframes and get total TouristAccommodation_Units and Residential_Units\n",
    "for sdfParcel in [sdfParcel1, sdfParcel2, sdfParcel3, sdfParcel4]:\n",
    "    print(sdfParcel.TouristAccommodation_Units.sum())\n",
    "    print(sdfParcel.Residential_Units.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "sdfParcel1.duplicated(subset=['APN']).sum()\n",
    "\n",
    "# print rows with duplicates\n",
    "sdfParcel1[sdfParcel1.duplicated(subset=['APN'], keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "def check_dupes(df, col):\n",
    "    df['is_duplicate'] = df.duplicated(subset=col, keep=False)\n",
    "    df.is_duplicate.value_counts()\n",
    "    df.loc[df['is_duplicate'] == True]\n",
    "    df = df.drop_duplicates(subset=col, keep='first', inplace=True)\n",
    "    return df[df.duplicated([col], keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_residential_units - base_2018 \n",
    "# forecast will be for 2040 and 2050 \n",
    "# rate of development will be based on the current rate of development from the last 12 years (back to 2012)\n",
    "    # current rate will not get us to full build out and will be adjusted to get to full build out by 2050\n",
    "# total_residential_units = base_2018 + (rate_of_development * (2040 - 2018))\n",
    "\n",
    "# forecast max build out will be 2050\n",
    "#  still going to build out all the residential units and then revisit how conversions of TAUs and CFA will be handled\n",
    "\n",
    "# GIS exercise of where the new residential units will be built\n",
    "# 1. get the land use data and see if we can get the residential units on vacant and underbuilt parcels\n",
    "\n",
    "# For TAUs and CFA we only built out what was in the pipeline\n",
    "\n",
    "# Total Occupied Units = Total Residential Units - Vacant Units\n",
    "    # based on block group rate and TAZ crosswalk assigned to Parcel level units\n",
    "# Occupied Units by Income Level = Total Occupied Units * % of Income Level in Block Group\n",
    "    # based on block group rate and TAZ crosswalk assigned to Parcel level units\n",
    "\n",
    "# Lodging Occupany Rates by Tax Rate Zone\n",
    "    # Air DNA? for VHR occupancy rates\n",
    "    # Seasonal Units will be based on the % of seasonal units in the block group?\n",
    "\n",
    "# Adjusted occupancy rates for Residential units to be based on population change to decennial census\n",
    "    # double check total persons in the model against the decennial census population and then apply the rate?\n",
    "\n",
    "# use adjusted ACS numbers to make all the input factors match the same source\n",
    "    # use the 2022 ACS data at the Basin level for all the input factors\n",
    "        # Block Group level data will be to noisy and not as accurate as the Basin level data\n",
    "\n",
    "# forecast growth at the Basin level and show some population growth...\n",
    "    # out year will be 2050 and show the growth in the model at 0.5% per year\n",
    "    # show the growth in the model at 1.0% per year? or use the decennial census growth rate?\n",
    "        # which was 0.04% per year from 2010 to 2020 annualized\n",
    "\n",
    "# show the growth in the model at 0.004% per year? or use the decennial census growth rate?\n",
    "    # adding 3000 units of affordable housing in the model and get 6,000 person increase in population\n",
    "\n",
    "# \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d6c85d1",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e30210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "# local path\n",
    "out_dir = Path(\"Data\")\n",
    "# api url\n",
    "derq_api_url = 'https://api-external.cloud.derq.com'\n",
    "# Set the API key as an environment variable\n",
    "headers = {\n",
    "    \"x-api-key\": os.getenv('derq-api-key')\n",
    "}\n",
    "# global variables\n",
    "all_event_types = 'IC, WWD, STPV, TV, LCV, RLV, NM-VV, NM-VRU, CRSH'\n",
    "default_speed_buckets = '5,10,15,20,25'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49cf392",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DERQ API FUNCTIONS ###\n",
    "\n",
    "# Get DERQ locations for Tahoe\n",
    "def get_derq_locations():\n",
    "    url = derq_api_url + '/locations'\n",
    "    return requests.get(url, headers=headers).json()\n",
    "# parse as dataframe\n",
    "def parse_locations_response(response: dict) -> pd.DataFrame:\n",
    "    \"\"\"Parse the DERQ API response to extract location data.\"\"\"\n",
    "    locations = response.get(\"body\", [])\n",
    "    return pd.DataFrame(locations)\n",
    "# get veh counts for a location\n",
    "def get_derq_veh_counts(location_id: str, start_date: str, end_date: str) -> dict:\n",
    "    \"\"\"\n",
    "    Fetch vehicle count data for a given location and time range.\n",
    "\n",
    "    Parameters:\n",
    "        location_id (str): Location ID to query\n",
    "        start_date (str): Start date in 'YYYY-MM-DD' format\n",
    "        end_date (str): End date in 'YYYY-MM-DD' format\n",
    "\n",
    "    Returns:\n",
    "        dict: JSON response from the API containing vehicle counts\n",
    "    \"\"\"\n",
    "    url = f\"{derq_api_url}/counts/vehicle?locationId={location_id}&startDate={start_date}&endDate={end_date}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "# Get vehicle counts for multiple locations\n",
    "def parse_vehicle_counts_response(response: dict) -> pd.DataFrame:\n",
    "    all_data = []\n",
    "    for location_name, payload in response.items():\n",
    "        if payload.get(\"statusCode\") == \"200\":\n",
    "            for entry in payload.get(\"body\", []):\n",
    "                entry[\"LocationName\"] = location_name\n",
    "                all_data.append(entry)\n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "def get_derq_safety_insights(location_id: str, start_date: str, end_date: str) -> dict:\n",
    "    \"\"\"\n",
    "    Fetch safety insights data for a given location and time range.\n",
    "\n",
    "    Parameters:\n",
    "        location_id (str): Location ID to query\n",
    "        start_date (str): Start date in 'YYYY-MM-DD' format\n",
    "        end_date (str): End date in 'YYYY-MM-DD' format\n",
    "\n",
    "    Returns:\n",
    "        dict: JSON response from the API containing safety insights\n",
    "    \"\"\"\n",
    "    url = f\"{derq_api_url}/safety-insights?locationId={location_id}&startDate={start_date}&endDate={end_date}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "def parse_safety_insights_response(response: dict) -> pd.DataFrame:\n",
    "    if response.get(\"statusCode\") == \"200\":\n",
    "        return pd.DataFrame(response.get(\"body\", []))\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "# get derq vru counts function\n",
    "def get_derq_vru_counts(location_id: str, start_date: str, end_date: str) -> dict:\n",
    "    \"\"\"\n",
    "    Fetch VRU count data for a given location and time range.\n",
    "\n",
    "    Parameters:\n",
    "        location_id (str): Location ID to query\n",
    "        start_date (str): Start date in 'YYYY-MM-DD' format\n",
    "        end_date (str): End date in 'YYYY-MM-DD' format\n",
    "\n",
    "    Returns:\n",
    "        dict: JSON response from the API containing VRU counts\n",
    "    \"\"\"\n",
    "    url = f\"{derq_api_url}/counts/vru?locationId={location_id}&startDate={start_date}&endDate={end_date}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "# parse vru counts response\n",
    "def parse_vru_counts_response(response: dict) -> pd.DataFrame:\n",
    "    if response.get(\"statusCode\") == \"200\":\n",
    "        return pd.DataFrame(response.get(\"body\", []))\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# get derq single location vehicle counts function\n",
    "def parse_single_location_vehicle_counts(response: dict) -> pd.DataFrame:\n",
    "    if response.get(\"statusCode\") == \"200\":\n",
    "        return pd.DataFrame(response.get(\"body\", []))\n",
    "    else:\n",
    "        return pd.DataFrame()  # Return empty DataFrame if error\n",
    "\n",
    "# get derq speed counts function\n",
    "def get_derq_speeds(location_id: str, start_date: str, end_date: str, buckets=default_speed_buckets, unit='mph') -> dict:\n",
    "    \"\"\"\n",
    "    Fetch speed distribution data for a given location and time range.\n",
    "\n",
    "    Parameters:\n",
    "        location_id (str): Location ID to query\n",
    "        start_date (str): Start date in 'YYYY-MM-DD' format\n",
    "        end_date (str): End date in 'YYYY-MM-DD' format\n",
    "        buckets (str): Speed buckets for the distribution\n",
    "        unit (str): Speed unit (e.g., 'mph', 'km/h')\n",
    "\n",
    "    Returns:\n",
    "        dict: JSON response from the API containing speed distribution data\n",
    "    \"\"\"\n",
    "    url = f\"{derq_api_url}/speed-distribution?locationId={location_id}&startDate={start_date}&endDate={end_date}&speedBuckets={buckets}&speedUnit={unit}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "# parse speed response\n",
    "def parse_speed_response(response: dict) -> pd.DataFrame:\n",
    "    if response.get(\"statusCode\") == \"200\":\n",
    "        return pd.DataFrame(response.get(\"body\", []))\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "## Reed's work ###\n",
    "def get_derq_events(location, start, end, event_types=all_event_types):\n",
    "    url = derq_api_url + f'/safety-insights?locationId={location}&startDate={start}&endDate={end}&eventTypes={event_types}'\n",
    "    return requests.get(url, headers=headers).json()\n",
    "\n",
    "def get_derq_veh_counts(location, start, end):\n",
    "    url = derq_api_url + f'/counts/vehicle?locationId={location}&startDate={start}&endDate={end}'\n",
    "    return requests.get(url, headers=headers).json()\n",
    "\n",
    "def get_derq_vru_counts(location, start, end):\n",
    "    url = derq_api_url + f'/counts/vru?locationId={location}&startDate={start}&endDate={end}'\n",
    "    return requests.get(url, headers=headers).json()\n",
    "\n",
    "def get_derq_speeds(location, start, end, buckets=default_speed_buckets, unit='mph'):\n",
    "    url = derq_api_url + f'/speed-distribution?locationId={location}&startDate={start}&endDate={end}&speedBuckets={buckets}&speedUnit={unit}'\n",
    "    return requests.get(url, headers=headers).json()\n",
    "\n",
    "def process_response_derq(response, intersection_id, df_list):\n",
    "    if response:\n",
    "        data = response.get('body', [])\n",
    "        if data:\n",
    "            df = pd.DataFrame(data)\n",
    "            df['intersection_id'] = intersection_id\n",
    "            df_list.append(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0410020e",
   "metadata": {},
   "source": [
    "### Update All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38399221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch location data\n",
    "df_locations = parse_locations_response(get_derq_locations())\n",
    "df_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b194fa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch location data\n",
    "df_locations = parse_locations_response(get_derq_locations())\n",
    "\n",
    "### API will only return 30 days of data at a time, so set a date range accordingly ###\n",
    "\n",
    "# date range for DERQ API calls\n",
    "start_date = \"06/28/2025\"\n",
    "end_date   = \"07/14/2025\"\n",
    "\n",
    "### Fetch DERQ Vehicle Counts ###\n",
    "\n",
    "# Retrieve vehicle counts for all locations in the last 30 days\n",
    "veh_data = []\n",
    "for _, row in df_locations.iterrows():\n",
    "    location_id = row['LocationId']\n",
    "    location_name = row['LocationName']\n",
    "    print(f\"Fetching traffic count data for: {location_name}\")\n",
    "    veh_resp = get_derq_veh_counts(location_id, start_date, end_date)\n",
    "    df_counts = parse_single_location_vehicle_counts(veh_resp)\n",
    "    df_counts['LocationId'] = location_id\n",
    "    df_counts['LocationName'] = location_name\n",
    "    veh_data.append(df_counts)\n",
    "# Combine all results into a single DataFrame\n",
    "df_vehicle_counts = pd.concat(veh_data, ignore_index=True)\n",
    "# Optional: Convert timestamp column to datetime if it exists\n",
    "if 'timestamp' in df_vehicle_counts.columns:\n",
    "    df_vehicle_counts['timestamp'] = pd.to_datetime(df_vehicle_counts['timestamp'])\n",
    "# Save to CSV\n",
    "format = \"%m_%d_%Y\"\n",
    "start_date_f = datetime.strptime(start_date, \"%m/%d/%Y\").strftime(format)\n",
    "end_date_f = datetime.strptime(end_date, \"%m/%d/%Y\").strftime(format)\n",
    "csv_name = f'vehicle_counts_{start_date_f}_to_{end_date_f}.csv'\n",
    "df_vehicle_counts.to_csv(out_dir / csv_name, index=False)\n",
    "\n",
    "\n",
    "### Fetch DERQ Safety Insights ###\n",
    "\n",
    "# Retrieve safety insights for all locations\n",
    "safety_data = []\n",
    "for _, row in df_locations.iterrows():\n",
    "    location_id = row['LocationId']\n",
    "    location_name = row['LocationName']\n",
    "    print(f\"Fetching safety insights for: {location_name}\")\n",
    "    safety_resp = get_derq_safety_insights(location_id, start_date, end_date)\n",
    "    df_safety = parse_safety_insights_response(safety_resp)\n",
    "    df_safety['LocationId'] = location_id\n",
    "    df_safety['LocationName'] = location_name\n",
    "    safety_data.append(df_safety)\n",
    "# Combine all results into a single DataFrame\n",
    "df_safety_insights = pd.concat(safety_data, ignore_index=True)\n",
    "# Optional: Convert datetime columns if present\n",
    "for col in ['timestamp', 'event_time']:\n",
    "    if col in df_safety_insights.columns:\n",
    "        df_safety_insights[col] = pd.to_datetime(df_safety_insights[col])\n",
    "# Save to CSV\n",
    "format = \"%m_%d_%Y\"\n",
    "start_date_f = datetime.strptime(start_date, \"%m/%d/%Y\").strftime(format)\n",
    "end_date_f = datetime.strptime(end_date, \"%m/%d/%Y\").strftime(format)\n",
    "csv_name = f'safety_insights_{start_date_f}_to_{end_date_f}.csv'\n",
    "df_safety_insights.to_csv(out_dir / csv_name, index=False)\n",
    "\n",
    "\n",
    "### Fetch DERQ VRU Counts ###\n",
    "\n",
    "# Get VRU counts for all locations in the last 30 days\n",
    "vru_data = []\n",
    "for _, row in df_locations.iterrows():\n",
    "    location_id = row['LocationId']\n",
    "    location_name = row['LocationName']\n",
    "    print(f\"Fetching VRU data for: {location_name}\")\n",
    "    vru_resp = get_derq_vru_counts(location_id, start_date, end_date)\n",
    "    df_vru = parse_single_location_vehicle_counts(vru_resp)\n",
    "    df_vru['LocationId'] = location_id\n",
    "    df_vru['LocationName'] = location_name\n",
    "    vru_data.append(df_vru)\n",
    "# Combine all results into a single DataFrame\n",
    "df_vru_counts = pd.concat(vru_data, ignore_index=True)\n",
    "# Optional: Convert timestamp column to datetime if it exists\n",
    "if 'timestamp' in df_vru_counts.columns:\n",
    "    df_vru_counts['timestamp'] = pd.to_datetime(df_vru_counts['timestamp'])\n",
    "# Save to CSV\n",
    "format = \"%m_%d_%Y\"\n",
    "start_date_f = datetime.strptime(start_date, \"%m/%d/%Y\").strftime(format)\n",
    "end_date_f = datetime.strptime(end_date, \"%m/%d/%Y\").strftime(format)\n",
    "csv_name = f'vru_counts_{start_date_f}_to_{end_date_f}.csv'\n",
    "df_vru_counts.to_csv(out_dir / csv_name, index=False)\n",
    "\n",
    "\n",
    "### Fetch DERQ Speed Counts ###\n",
    "# get derq speed counts url\n",
    "buckets = '5,10,15,20,25'\n",
    "\n",
    "# get speed data for all locations in the last 30 days\n",
    "speed_data = []\n",
    "for _, row in df_locations.iterrows():\n",
    "    location_id = row['LocationId']\n",
    "    location_name = row['LocationName']\n",
    "    print(f\"Fetching speed data for: {location_name}\")\n",
    "    speed_resp = get_derq_speeds(location_id, start_date, end_date)\n",
    "    df_speed = parse_speed_response(speed_resp)\n",
    "    df_speed['LocationId'] = location_id\n",
    "    df_speed['LocationName'] = location_name\n",
    "    speed_data.append(df_speed)\n",
    "# Combine all results into a single DataFrame\n",
    "df_speed_distribution = pd.concat(speed_data, ignore_index=True)\n",
    "# Optional: Convert datetime columns if present\n",
    "for col in ['timestamp']:\n",
    "    if col in df_speed_distribution.columns:\n",
    "        df_speed_distribution[col] = pd.to_datetime(df_speed_distribution[col])\n",
    "# Save to CSV\n",
    "format = \"%m_%d_%Y\"\n",
    "start_date_f = datetime.strptime(start_date, \"%m/%d/%Y\").strftime(format)\n",
    "end_date_f   = datetime.strptime(end_date, \"%m/%d/%Y\").strftime(format)\n",
    "csv_name = f'speed_distribution_{start_date_f}_to_{end_date_f}.csv'\n",
    "df_speed_distribution.to_csv(out_dir / csv_name, index=False)\n",
    "\n",
    "print(\"DERQ data retrieval and processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3765216",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146a7fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build daily average table by location\n",
    "import pandas as pd\n",
    "from arcgis.features import FeatureLayer\n",
    "\n",
    "# get traffic volume data\n",
    "traffic_service = \"https://maps.trpa.org/server/rest/services/Transportation_SMART/FeatureServer/3\"\n",
    "\n",
    "# Gets data from the TRPA server\n",
    "def get_fs_data(service_url):\n",
    "    feature_layer = FeatureLayer(service_url)\n",
    "    query_result = feature_layer.query()\n",
    "    # Convert the query result to a list of dictionaries\n",
    "    feature_list = query_result.features\n",
    "    # Create a pandas DataFrame from the list of dictionaries\n",
    "    all_data = pd.DataFrame([feature.attributes for feature in feature_list])\n",
    "    # return data frame\n",
    "    return all_data\n",
    "\n",
    "# get fs data as a df\n",
    "df = get_fs_data(traffic_service)\n",
    "\n",
    "# Group by Date and Location, then sum the counts to get daily totals\n",
    "daily_totals = df.groupby(['Date', 'LocationName'])['counts'].sum().reset_index()\n",
    "\n",
    "# Now get the average daily count per location\n",
    "daily_avg = daily_totals.groupby('LocationName')['counts'].mean().reset_index()\n",
    "daily_avg.rename(columns={'counts': 'Avg_Daily_Count'}, inplace=True)\n",
    "\n",
    "print(daily_avg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db188bd4",
   "metadata": {},
   "source": [
    "### Dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac289e0",
   "metadata": {},
   "source": [
    "#### Location Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676cedb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data frame from response: LocationID, LocationName, Latitude, Longitude\n",
    "locations = get_derq_locations()\n",
    "df = parse_locations_response(locations)\n",
    "df.to_csv(out_dir/'locations.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4937868",
   "metadata": {},
   "source": [
    "#### Vehicle Count Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e350f8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current UTC date and date 30 days ago\n",
    "end_date = datetime.utcnow().date().isoformat()\n",
    "start_date = (datetime.utcnow() - timedelta(days=30)).date().isoformat()\n",
    "location_id = '6786b290ddb02855b751179d'  # Example location ID\n",
    "veh_data = get_derq_veh_counts(location_id, start_date, end_date)\n",
    "df = parse_single_location_vehicle_counts(veh_data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc20998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch location data\n",
    "df_locations = parse_locations_response(get_derq_locations())\n",
    "\n",
    "# Retrieve vehicle counts for all locations in the last 30 days\n",
    "data = []\n",
    "for _, row in df_locations.iterrows():\n",
    "    location_id = row['LocationId']\n",
    "    location_name = row['LocationName']\n",
    "    print(f\"Fetching data for: {location_name}\")\n",
    "    veh_data = get_derq_veh_counts(location_id, start_date, end_date)\n",
    "    df_counts = parse_single_location_vehicle_counts(veh_data)\n",
    "    df_counts['LocationId'] = location_id\n",
    "    df_counts['LocationName'] = location_name\n",
    "    data.append(df_counts)\n",
    "\n",
    "# Combine all results into a single DataFrame\n",
    "df_vehicle_counts = pd.concat(data, ignore_index=True)\n",
    "\n",
    "# Optional: Convert timestamp column to datetime if it exists\n",
    "if 'timestamp' in df_vehicle_counts.columns:\n",
    "    df_vehicle_counts['timestamp'] = pd.to_datetime(df_vehicle_counts['timestamp'])\n",
    "\n",
    "# Save to CSV\n",
    "format = \"%m_%d_%Y\"\n",
    "start_date_f = datetime.strptime(start_date, \"%m/%d/%Y\").strftime(format)\n",
    "end_date_f = datetime.strptime(end_date, \"%m/%d/%Y\").strftime(format)\n",
    "csv_name = f'vehicle_counts_{start_date_f}_to_{end_date_f}.csv'\n",
    "df_vehicle_counts.to_csv(out_dir / csv_name, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f217239e",
   "metadata": {},
   "source": [
    "####  Safety Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a76914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get locations from csv\n",
    "# df_locations = pd.read_csv(out_dir / 'locations.csv')\n",
    "# # last 30 days\n",
    "# start_date = (datetime.utcnow() - timedelta(days=30)).date().isoformat()\n",
    "# end_date = datetime.utcnow().date().isoformat()\n",
    "\n",
    "# Retrieve safety insights for all locations\n",
    "safety_data = []\n",
    "for _, row in df_locations.iterrows():\n",
    "    location_id = row['LocationId']\n",
    "    location_name = row['LocationName']\n",
    "    print(f\"Fetching safety insights for: {location_name}\")\n",
    "    safety_resp = get_derq_safety_insights(location_id, start_date, end_date)\n",
    "    df_safety = parse_safety_insights_response(safety_resp)\n",
    "    df_safety['LocationId'] = location_id\n",
    "    df_safety['LocationName'] = location_name\n",
    "    safety_data.append(df_safety)\n",
    "\n",
    "# Combine all results into a single DataFrame\n",
    "df_safety_insights = pd.concat(safety_data, ignore_index=True)\n",
    "\n",
    "# Optional: Convert datetime columns if present\n",
    "for col in ['timestamp', 'event_time']:\n",
    "    if col in df_safety_insights.columns:\n",
    "        df_safety_insights[col] = pd.to_datetime(df_safety_insights[col])\n",
    "\n",
    "# Save to CSV\n",
    "format = \"%m_%d_%Y\"\n",
    "start_date_f = datetime.strptime(start_date, \"%m/%d/%Y\").strftime(format)\n",
    "end_date_f = datetime.strptime(end_date, \"%m/%d/%Y\").strftime(format)\n",
    "csv_name = f'safety_insights_{start_date_f}_to_{end_date_f}.csv'\n",
    "df_safety_insights.to_csv(out_dir / csv_name, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1e299a",
   "metadata": {},
   "source": [
    "#### VRU Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd4eb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "vru_url = derq_api_url + f'/counts/vru?locationId={location_id}&startDate={start_date}&endDate={end_date}'\n",
    "vru_data = get_derq_vru_counts(location_id, start_date, end_date)\n",
    "vru_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be815747",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get VRU counts for all locations in the last 30 days\n",
    "data = []\n",
    "for _, row in df_locations.iterrows():\n",
    "    location_id = row['LocationId']\n",
    "    location_name = row['LocationName']\n",
    "    print(f\"Fetching VRU data for: {location_name}\")\n",
    "    vru_data = get_derq_vru_counts(location_id, start_date, end_date)\n",
    "    df_vru = parse_single_location_vehicle_counts(vru_data)\n",
    "    df_vru['LocationId'] = location_id\n",
    "    df_vru['LocationName'] = location_name\n",
    "    data.append(df_vru)\n",
    "\n",
    "# Combine all results into a single DataFrame\n",
    "df_vru_counts = pd.concat(data, ignore_index=True)\n",
    "# Optional: Convert timestamp column to datetime if it exists\n",
    "if 'timestamp' in df_vru_counts.columns:\n",
    "    df_vru_counts['timestamp'] = pd.to_datetime(df_vru_counts['timestamp'])\n",
    "\n",
    "# Save to CSV\n",
    "format = \"%m_%d_%Y\"\n",
    "start_date_f = datetime.strptime(start_date, \"%m/%d/%Y\").strftime(format)\n",
    "end_date_f = datetime.strptime(end_date, \"%m/%d/%Y\").strftime(format)\n",
    "csv_name = f'vru_counts_{start_date_f}_to_{end_date_f}.csv'\n",
    "df_vru_counts.to_csv(out_dir / csv_name, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c10e9d2",
   "metadata": {},
   "source": [
    "#### Speed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6874e56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_url = derq_api_url + f'/speed-distribution?locationId={location_id}&startDate={start_date}&endDate={end_date}'\n",
    "speed_data = get_derq_speeds(location_id, start_date, end_date)\n",
    "speed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c6a540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get derq speed counts url\n",
    "buckets = '5,10,15,20,25'\n",
    "\n",
    "# get speed data for all locations in the last 30 days\n",
    "data = []\n",
    "for _, row in df_locations.iterrows():\n",
    "    location_id = row['LocationId']\n",
    "    location_name = row['LocationName']\n",
    "    print(f\"Fetching speed data for: {location_name}\")\n",
    "    speed_data = get_derq_speeds(location_id, start_date, end_date)\n",
    "    df_speed = parse_speed_response(speed_data)\n",
    "    df_speed['LocationId'] = location_id\n",
    "    df_speed['LocationName'] = location_name\n",
    "    data.append(df_speed)\n",
    "# Combine all results into a single DataFrame\n",
    "df_speed_distribution = pd.concat(data, ignore_index=True)\n",
    "# Optional: Convert datetime columns if present\n",
    "for col in ['timestamp']:\n",
    "    if col in df_speed_distribution.columns:\n",
    "        df_speed_distribution[col] = pd.to_datetime(df_speed_distribution[col])\n",
    "\n",
    "# Save to CSV\n",
    "format = \"%m_%d_%Y\"\n",
    "start_date_f = datetime.strptime(start_date, \"%m/%d/%Y\").strftime(format)\n",
    "end_date_f   = datetime.strptime(end_date, \"%m/%d/%Y\").strftime(format)\n",
    "csv_name = f'speed_distribution_{start_date_f}_to_{end_date_f}.csv'\n",
    "df_speed_distribution.to_csv(out_dir / csv_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {}
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

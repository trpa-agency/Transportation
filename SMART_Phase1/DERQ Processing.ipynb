{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35a6fd6-dc56-4089-b337-19a7b49bd7b8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491e762d-2d08-4a30-9a53-f51ebd654044",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def spark_table_gen(df, table_name, mode='append'):\n",
    "    \n",
    "    spark_df = spark.createDataFrame(df)\n",
    "\n",
    "    spark_df.write.format('delta').mode(mode).save(lakehouse_path + '/Tables/' + table_name)\n",
    "\n",
    "    # Create a table in the lakehouse that references the existing data\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name}\n",
    "        USING DELTA\n",
    "        LOCATION '{lakehouse_path}'\n",
    "    \"\"\")\n",
    "\n",
    "def extract_movement(full_movement):\n",
    "    movement_parts = full_movement.split(' ')\n",
    "    lane = movement_parts[1]\n",
    "    approach = movement_parts[3].replace(',', '')\n",
    "    movement = movement_parts[4]\n",
    "    return lane, approach, movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f6da59-4230-4811-9439-f7f25765f74b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Microsoft Fabric lakehouse settings\n",
    "app_name = \"tahoe\"\n",
    "lakehouse_path = os.getenv('tahoe_lakehouse_path')\n",
    "\n",
    "spark = SparkSession.builder.appName(app_name).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6c36d0-cac2-4d65-961b-43bca587ffd9",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "unprocessed_dir = '/lakehouse/default/Files/Unprocessed/'\n",
    "processed_dir = '/lakehouse/default/Files/Processed/'\n",
    "\n",
    "vru_approach_dict = {\n",
    "    'NB': 'S',\n",
    "    'SB': 'N',\n",
    "    'EB': 'W',\n",
    "    'WB': 'E'\n",
    "}\n",
    "\n",
    "class_dict = {\n",
    "    'Mobility Aid': 'Pedestrian',\n",
    "    'Motorcycle': 'Passenger Vehicle',\n",
    "    'Articulated Truck': 'Semi Truck',\n",
    "    'Single Unit Truck': 'Box Truck',\n",
    "    'Person Mobility Device': 'Pedestrian'\n",
    "}\n",
    "\n",
    "severity_dict = {\n",
    "    'High': 'Severe',\n",
    "    'Low': 'Moderate'\n",
    "}\n",
    "\n",
    "default_date = '1900-01-01'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43120ad",
   "metadata": {},
   "source": [
    "### Volume Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5787b0-c396-4b01-b8b2-8d4fbaadbda5",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "source_dir = os.path.join(unprocessed_dir, 'DERQ/Volumes')\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for file in os.listdir(source_dir):\n",
    "    if 'csv' in file:\n",
    "        # get intersection id from file name\n",
    "        intersection_id = file\n",
    "        # read csv file to df\n",
    "        file_path = os.path.join(source_dir, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['intersection_id'] = intersection_id\n",
    "        dfs.append(df)\n",
    "\n",
    "combined_volume = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9f0bf6-2fa3-49e7-8a2c-1614293e0481",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# get times\n",
    "combined_volume[['time', 'end_time']] = combined_volume['timeInterval'].str.split(' - ', expand=True)\n",
    "combined_volume['time'] = pd.to_datetime(default_date + ' ' + combined_volume['time'])\n",
    "\n",
    "# get dates as datetime\n",
    "combined_volume['date'] = pd.to_datetime(combined_volume['date'])\n",
    "combined_volume['date'] = combined_volume['date'].dt.date\n",
    "\n",
    "# drop and rename excess columns\n",
    "combined_volume.drop(columns=['timeInterval', 'movement', 'dayOfTheWeek', 'end_time'], inplace=True)\n",
    "combined_volume.rename(columns={'movementType': 'movement', 'count': 'volume'}, inplace=True)\n",
    "\n",
    "combined_volume['class'] = combined_volume['class'].str.replace('_', ' ').str.title()\n",
    "combined_volume['class'] = combined_volume['class'].replace(class_dict)\n",
    "\n",
    "mask = combined_volume['movement'] == 'CROSSING'\n",
    "combined_volume.loc[mask, 'approach'] = combined_volume.loc[mask, 'approach'].map(vru_approach_dict)\n",
    "\n",
    "combined_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23b7c9a-b5a7-4b13-b360-565b0da931e4",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "table_name = 'derq_volume_fact_table'\n",
    "spark_table_gen(combined_volume, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4095aa04",
   "metadata": {},
   "source": [
    "### Event processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4182e96e-e84d-40a9-b440-09092e76ae05",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "source_dir = os.path.join(unprocessed_dir, 'DERQ/Events')\n",
    "\n",
    "event_dfs = []\n",
    "\n",
    "for file_name in os.listdir(source_dir):\n",
    "    file_path = os.path.join(source_dir, file_name)\n",
    "\n",
    "    # read csv file to df\n",
    "    df = pd.read_csv(file_path)\n",
    "    event_dfs.append(df)\n",
    "\n",
    "combined_events = pd.concat(event_dfs, ignore_index=True)\n",
    "combined_events.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc39ad7-f88d-46ae-b8fe-39b4e7dc380f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# rename id to event_id  \n",
    "combined_events.rename(columns={'id': 'event_id'}, inplace=True)\n",
    "\n",
    "# split date into date and time  \n",
    "combined_events['datetime'] = pd.to_datetime(combined_events['datetime'])\n",
    "combined_events['date'] = combined_events['datetime'].dt.date\n",
    "\n",
    "# strip seconds from time \n",
    "combined_events['time_to_second'] = combined_events['datetime'].dt.strftime('%H:%M:%S')\n",
    "combined_events['time'] = combined_events['datetime'].dt.strftime('%H:%M')\n",
    "combined_events['time'] = combined_events['time'] + ':00'\n",
    "\n",
    "# re-set time cols as datetimes\n",
    "combined_events['time'] = pd.to_datetime(default_date + ' ' + combined_events['time'])\n",
    "combined_events['time_to_second'] = pd.to_datetime(default_date + ' ' + combined_events['time_to_second'])\n",
    "\n",
    "# drop excess columns\n",
    "combined_events.drop(columns=['datetime'], inplace=True)\n",
    "combined_events.rename(columns={'speed': 'speed_mph'}, inplace=True)\n",
    "\n",
    "combined_events['speed_mph'] = combined_events['speed_mph'].str.extract(r'(\\d+)').astype(float)\n",
    "\n",
    "combined_events.rename(columns={'isSevere': 'severity'}, inplace=True)\n",
    "combined_events['severity'] = combined_events['severity'].map(severity_dict)\n",
    "combined_events.loc[combined_events['event_type'] == 'Illegal Crossing', 'severity'] = 'Low'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c705b5-6512-4841-98c2-8d2911b40248",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "table_name = 'derq_event_fact_table'\n",
    "spark_table_gen(combined_events, table_name)"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "58a7543d-9eec-45df-881f-5a72eff7b285",
    "default_lakehouse_name": "RTCSN_Smart",
    "default_lakehouse_workspace_id": "29274fa5-1058-4a3d-9c31-14a55c35f0f2"
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

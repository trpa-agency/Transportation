{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up initial data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup \n",
    "# import modules\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import arcpy\n",
    "import numpy as np\n",
    "import urllib\n",
    "import Custom_Functions as cf\n",
    "ca_crash_input_file = 'RawData/CA_Raw_Data.csv'\n",
    "nv_crash_input_file = 'RawData/NV_Raw_Data.csv'\n",
    "# setup workspace folder\n",
    "workspace = \"F:/gis/PROJECTS/ResearchAnalysis/Monitoring/Data/Crash/\"\n",
    "\n",
    "# setup environment variables\n",
    "arcpy.env.overwriteOutput = True\n",
    "arcpy.env.workspace = \"F:/GIS/PROJECTS/ResearchAnalysis/Monitoring/Data/Crash/CrashData/CrashData.gdb\"\n",
    "\n",
    "# create a spatial reference object for the output coordinate system \n",
    "# output projection for data going into SDE should be UTM Zone 10N (EPSG: 26910)\n",
    "out_coordinate_system = arcpy.SpatialReference(26910)\n",
    "\n",
    "# network path to connection files\n",
    "filePath = \"F:/GIS/DB_CONNECT\"\n",
    "\n",
    "# database file path \n",
    "sdeBase  = os.path.join(filePath, \"Vector.sde\")\n",
    "\n",
    "# SDE feature classes needed for spatial joins\n",
    "corridor = os.path.join(sdeBase, 'sde.SDE.Transportation\\sde.SDE.Corridor')\n",
    "trpa     = os.path.join(sdeBase, 'sde.SDE.Jurisdictions\\sde.SDE.TRPA_bdy')\n",
    "\n",
    "# define csv lat/long field names for xy table to point\n",
    "x_coords = 'POINT_X'\n",
    "y_coords = 'POINT_Y'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import raw data and sde highway collisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDE feature class to update\n",
    "crashSDE  = os.path.join(sdeBase, 'sde.SDE.Transportation\\sde.SDE.Highway_Collisions')\n",
    "\n",
    "# Get Crash Data\n",
    "caCrashes = os.path.join(workspace, ca_crash_input_file)\n",
    "dfCACrash_raw = pd.read_csv(caCrashes)\n",
    "\n",
    "nvCrashes = os.path.join(workspace, nv_crash_input_file)\n",
    "dfNVCrash_raw = pd.read_csv(nvCrashes)\n",
    "    \n",
    "# # in memory files\n",
    "memory = \"memory\" + \"\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CA data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import California lookup dictionaries\n",
    "#All lookup values are in a single csv that gets imported and then filtered on state and fieldname\n",
    "#There are more efficient ways to do this but this makes for the most flexibility\n",
    "value_lookup = 'LookupLists/FieldLookups.csv'\n",
    "ca_crash_severity_lookup = cf.import_lookup_dictionary(value_lookup,'key','value','State','CA','FieldName','Crash_Severity')\n",
    "ca_crash_type_lookup = cf.import_lookup_dictionary(value_lookup,'key','value','State','CA','FieldName','Crash_Type')\n",
    "ca_lighting_lookup = cf.import_lookup_dictionary(value_lookup,'key','value','State','CA','FieldName','Lighting')\n",
    "ca_weather_lookup = cf.import_lookup_dictionary(value_lookup,'key','value','State','CA','FieldName','Weather')\n",
    "ca_violation_lookup = cf.import_lookup_dictionary(value_lookup,'key','value','State','CA','FieldName','Violation')\n",
    "ca_road_surface_lookup = cf.import_lookup_dictionary(value_lookup,'key','value','State','CA','FieldName','Road_Surface')\n",
    "ca_road_con_lookup = cf.import_lookup_dictionary(value_lookup,'key','value','State','CA','FieldName','Road_Condition')\n",
    "ca_ped_action_lookup = cf.import_lookup_dictionary(value_lookup,'key','value','State','CA','FieldName','Pedestrian_Action')\n",
    "ca_hit_run_lookup = cf.import_lookup_dictionary(value_lookup,'key','value','State','CA','FieldName','Hit_and_Run')\n",
    "ca_mviw_lookup = cf.import_lookup_dictionary(value_lookup,'key','value','State','CA','FieldName','Motor_Vehicle_Interacted_With')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## CA data translation\n",
    "#This list will be used to rename the columns from the raw data\n",
    "#Future improvement - turn this into a csv?\n",
    "ca_column_mapping = {'COLLISION_SEVERITY' : 'Crash_Severity',\n",
    "'TYPE_OF_COLLISION' : 'Crash_Type',\n",
    "'LIGHTING' : 'Lighting',\n",
    "'WEATHER_1' : 'Weather_1',\n",
    "'WEATHER_2' : 'Weather_2',\n",
    "'PCF_VIOL_CATEGORY' : 'Violation',\n",
    "'ROAD_SURFACE' : 'Road_Surface',\n",
    "'ROAD_COND_1' : 'Road_Condition_1',\n",
    "'ROAD_COND_2' : 'Road_Condition_2',\n",
    "'PED_ACTION' : 'Pedestrian_Action',\n",
    "'HIT_AND_RUN' : 'Hit_and_Run',\n",
    "'MVIW' : 'Motor_Vehicle_Interacted_With',\n",
    "'ACCIDENT_YEAR' : 'Year',\n",
    "'ALCOHOL_INVOLVED' : 'Alcohol_Involved',\n",
    "'BICYCLE_ACCIDENT' : 'Bicycle_Involved',\n",
    "'CASE_ID' : 'CA_Case_ID',\n",
    "'CITY' : 'City',\n",
    "'COLLISION_DATE' : 'Date',\n",
    "'COLLISION_TIME' : '4DigTime',\n",
    "'COUNT_BICYCLIST_INJURED' : 'Num_Bicyclist_Injured',\n",
    "'COUNT_BICYCLIST_KILLED' : 'Num_Bicyclist_Killed',\n",
    "'COUNT_MC_INJURED' : 'Num_Motorcyclist_Injured',\n",
    "'COUNT_MC_KILLED' : 'Num_Motorcyclist_Killed',\n",
    "'COUNT_PED_INJURED' : 'Num_Ped_Injured',\n",
    "'COUNT_PED_KILLED' : 'Num_Ped_Killed',\n",
    "'COUNTY' : 'County',\n",
    "'Hour' : 'Time',\n",
    "'MOTORCYCLE_ACCIDENT' : 'Motorcycle_Involved',\n",
    "'NUMBER_INJURED' : 'Num_Injured',\n",
    "'NUMBER_KILLED' : 'Num_Killed',\n",
    "'PARTY_COUNT' : 'Num_Parties',\n",
    "'PEDESTRIAN_ACCIDENT' : 'Pedestrian_Involved',\n",
    "'COLLISION_TIME':'COLLISION_TIME',\n",
    "'POINT_X':'POINT_X',\n",
    "'POINT_Y':'POINT_Y'\n",
    "}\n",
    "\n",
    "#Any columns not in the column mapping will be deleted\n",
    "dfCACrash=cf.renamecolumns(dfCACrash_raw,ca_column_mapping,True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Add new columns with hard coded values\n",
    "dfCACrash['State']       = \"CA\"\n",
    "dfCACrash['4DigTime']    = dfCACrash['COLLISION_TIME'].astype(str).str.zfill(4)\n",
    "dfCACrash['Hour']        = dfCACrash['4DigTime'].str[:2]\n",
    "dfCACrash['Min']         = dfCACrash['4DigTime'].str[2:]\n",
    "dfCACrash['Time']        = dfCACrash['Hour']+\":\"+dfCACrash['Min']\n",
    "dfCACrash['Data_Source'] = \"CHP/SWITRS\"\n",
    "#convert case ID\n",
    "dfCACrash['NV_Accident_Num']       = np.nan\n",
    "dfCACrash['NV_Accident_Rec_Num']   = np.nan\n",
    "dfCACrash['Num_Vehicles']          = np.nan\n",
    "dfCACrash['Corridor_ID']           = np.nan\n",
    "\n",
    "#Update fields from lookup dictionaries\n",
    "#Imporvement - lookup dictionary that loops through everything? Might hurt readablity\n",
    "dfCACrash['Crash_Severity']=dfCACrash['Crash_Severity'].astype(str)\n",
    "dfCACrash['Crash_Severity'] = dfCACrash['Crash_Severity'].map(ca_crash_severity_lookup)\n",
    "dfCACrash['Crash_Type'] = dfCACrash['Crash_Type'].map(ca_crash_type_lookup)\n",
    "dfCACrash['Lighting'] = dfCACrash['Lighting'].map(ca_lighting_lookup)\n",
    "dfCACrash['Weather_1'] = dfCACrash['Weather_1'].map(ca_weather_lookup)\n",
    "dfCACrash['Weather_2']=dfCACrash['Weather_2'].map(ca_weather_lookup)\n",
    "dfCACrash['Violation'] = dfCACrash['Violation'].map(ca_violation_lookup)\n",
    "dfCACrash['Road_Surface'] = dfCACrash['Road_Surface'].map(ca_road_surface_lookup)\n",
    "dfCACrash['Road_Condition_1']=dfCACrash['Road_Condition_1'].map(ca_road_con_lookup)\n",
    "dfCACrash['Road_Condition_2']=dfCACrash['Road_Condition_2'].map(ca_road_con_lookup)\n",
    "dfCACrash['Pedestrian_Action']=dfCACrash['Pedestrian_Action'].map(ca_ped_action_lookup)\n",
    "dfCACrash['Hit_and_Run'] = dfCACrash['Hit_and_Run'].map(ca_hit_run_lookup)\n",
    "dfCACrash['Motor_Vehicle_Interacted_With'] = dfCACrash['Motor_Vehicle_Interacted_With'].map(ca_mviw_lookup)\n",
    "def populate_all_involved(row):\n",
    "    field_value = ''\n",
    "    if row['Pedestrian_Involved']=='Y':\n",
    "        field_value+='Pedestrian, '\n",
    "    if row['Bicycle_Involved']=='Y':\n",
    "        field_value+='Bicycle, '\n",
    "    if row['Motorcycle_Involved']=='Y':\n",
    "        field_value+='Motorcycle, '\n",
    "    return field_value[:-2] if field_value else ''\n",
    "dfCACrash['All_Involved'] = dfCACrash.apply(populate_all_involved,axis=1)\n",
    "#Specify which fields to keep\n",
    "dfCACrash = dfCACrash[['State',\n",
    "           'CA_Case_ID',\n",
    "           'NV_Accident_Num',\n",
    "           'NV_Accident_Rec_Num',\n",
    "           'Corridor_ID',\n",
    "           'County',\n",
    "           'City',\n",
    "           'Year',\n",
    "           'Date',\n",
    "           'Time',\n",
    "           'Weather_1',\n",
    "           'Weather_2',\n",
    "           'Crash_Severity',\n",
    "           'Num_Killed',\n",
    "           'Num_Injured',\n",
    "           'Num_Ped_Killed',\n",
    "           'Num_Ped_Injured',\n",
    "           'Num_Bicyclist_Killed',\n",
    "           'Num_Bicyclist_Injured',\n",
    "           'Num_Motorcyclist_Killed',\n",
    "           'Num_Motorcyclist_Injured',\n",
    "           'Crash_Type',\n",
    "           'Num_Vehicles',\n",
    "           'Num_Parties',\n",
    "           'Violation',\n",
    "           'Hit_and_Run',\n",
    "           'Motor_Vehicle_Interacted_With',\n",
    "           'Pedestrian_Action', \n",
    "           'Road_Condition_1',\n",
    "           'Road_Condition_2',\n",
    "           'Road_Surface',\n",
    "           'Lighting',\n",
    "           'Pedestrian_Involved',\n",
    "           'Bicycle_Involved',\n",
    "           'Motorcycle_Involved',\n",
    "           'Alcohol_Involved',\n",
    "           'Data_Source',\n",
    "           'All_Involved',\n",
    "           'POINT_X',\n",
    "           'POINT_Y']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NV data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nv_road_surface_lookup = cf.import_lookup_dictionary(value_lookup,'key','value','State','NV','FieldName','Road_Surface')\n",
    "nv_crash_type_lookup = cf.import_lookup_dictionary(value_lookup,'key','value','State','NV','FieldName','Crash_Type')\n",
    "nv_lighting_lookup = cf.import_lookup_dictionary(value_lookup,'key','value','State','NV','FieldName','Lighting')\n",
    "nv_road_condition_lookup = cf.import_lookup_dictionary(value_lookup,'key','value','State','NV','FieldName','Road_Condition_1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NV Data Transformation\n",
    "# set fields for time and case info\n",
    "nv_column_mapping = {\n",
    "    'NV Accident Num' : 'NV_Accident_Num',\n",
    "'NV Accident Rec Num' : 'NV_Accident_Rec_Num',\n",
    "'Collision_Year' : 'Year',\n",
    "'Collision_Date' : 'Date',\n",
    "'Collision_Time' : 'Time',\n",
    "'Total Vehicles' : 'Num_Vehicles',\n",
    "'Fatalities' : 'Num_Killed',\n",
    "'Injured' : 'Num_Injured',\n",
    "'COLLISION_SEVERITY' : 'Crash_Severity',\n",
    "'Crash Type' : 'Crash_Type',\n",
    "'Factors Roadway' : 'Road_Surface',\n",
    "'HWY Factors' : 'Road_Condition_1',\n",
    "'X' : 'POINT_X',\n",
    "'Y': 'POINT_Y',\n",
    "'Weather': 'Weather_1',\n",
    "'LIGHTING':'Lighting' \n",
    "}\n",
    "\n",
    "dfNVCrash = cf.renamecolumns(dfNVCrash_raw,nv_column_mapping,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "nv_motorcycle_list = ['MOTORCYCLE',\"MOTORBIKE\",\"MOPED\"]\n",
    "\n",
    "dfNVCrash['Num_Killed']   = dfNVCrash['Num_Killed'].fillna(0)\n",
    "dfNVCrash['Num_Injured'] = dfNVCrash['Num_Injured'].fillna(0)\n",
    "\n",
    "dfNVCrash['Num_Parties']          = np.nan\n",
    "dfNVCrash['Data_Source']          = \"NDOT\"\n",
    "dfNVCrash['City']                 = np.nan\n",
    "dfNVCrash['CA_Case_ID']           = np.nan\n",
    "dfNVCrash['Num_Ped_Killed'] = np.nan\n",
    "dfNVCrash['Num_Ped_Injured'] = np.nan\n",
    "dfNVCrash['Num_Bicyclist_Killed'] = np.nan\n",
    "dfNVCrash['Num_Bicyclist_Injured'] = np.nan\n",
    "dfNVCrash['Num_Motorcyclist_Killed'] = np.nan\n",
    "dfNVCrash['Num_Motorcyclist_Injured'] = np.nan\n",
    "dfNVCrash['Violation']  = \"N/A\"\n",
    "dfNVCrash['Hit_and_Run'] = \"N/A\"\n",
    "dfNVCrash['Motor_Vehicle_Interacted_With'] = \"N/A\"\n",
    "dfNVCrash['Pedestrian_Action'] = \"N/A\"\n",
    "dfNVCrash['Weather_2']   = np.nan\n",
    "dfNVCrash['Corridor_ID'] = np.nan\n",
    "dfNVCrash['Road_Condition_2'] = np.nan\n",
    "\n",
    "\n",
    "# Convert NV crash type and severity\n",
    "dfNVCrash['Crash_Type'] = dfNVCrash['Crash_Type'].map(nv_crash_type_lookup)\n",
    "dfNVCrash['Lighting'] = dfNVCrash['Lighting'].map(nv_lighting_lookup)\n",
    "\n",
    "#Process alcohol involvement (check outputs)\n",
    "dfNVCrash['V1 Driver Factors'] = dfNVCrash['V1 Driver Factors'].fillna(\"Not stated\")\n",
    "dfNVCrash['V2 Driver Factors'] = dfNVCrash['V2 Driver Factors'].fillna(\"Not stated\")\n",
    "dfNVCrash.loc[dfNVCrash['V1 Driver Factors'].str.contains(\"DRINKING\"), 'Alcohol_Involved'] = \"Y\"\n",
    "dfNVCrash.loc[dfNVCrash['V2 Driver Factors'].str.contains(\"DRINKING\"), 'Alcohol_Involved'] = \"Y\"\n",
    "#Process bike/ped involvement (check outputs)\n",
    "dfNVCrash['V1 All Events'] = dfNVCrash['V1 All Events'].fillna(\"Not stated\")\n",
    "dfNVCrash['V2 All Events'] = dfNVCrash['V2 All Events'].fillna(\"Not stated\")\n",
    "dfNVCrash.loc[dfNVCrash['V1 All Events'].str.contains(\"PEDESTRIAN\"), 'Pedestrian_Involved'] = \"Y\"\n",
    "dfNVCrash.loc[dfNVCrash['V2 All Events'].str.contains(\"PEDESTRIAN\"), 'Pedestrian_Involved'] = \"Y\"\n",
    "dfNVCrash.loc[dfNVCrash['V1 All Events'].str.contains(\"PEDAL CYCLE\"), 'Bicycle_Involved'] = \"Y\"\n",
    "dfNVCrash.loc[dfNVCrash['V2 All Events'].str.contains(\"PEDAL CYCLE\"), 'Bicycle_Involved'] = \"Y\"\n",
    "#Process motorcycle involvement (check outputs)\n",
    "dfNVCrash['V1 Type'] = dfNVCrash['V1 Type'].fillna(\"Not stated\")\n",
    "dfNVCrash['V2 Type'] = dfNVCrash['V1 Type'].fillna(\"Not stated\")\n",
    "dfNVCrash.loc[dfNVCrash['V1 Type'].str.contains('|'.join(nv_motorcycle_list), case=False, na=False), 'Motorcycle_Involved'] = \"Y\"\n",
    "dfNVCrash.loc[dfNVCrash['V2 Type'].str.contains('|'.join(nv_motorcycle_list), case=False, na=False), 'Motorcycle_Involved'] = \"Y\"\n",
    "\n",
    "# #Convert road surface\n",
    "dfNVCrash['Road_Surface']=dfNVCrash['Road_Surface'].fillna('Not stated')\n",
    "dfNVCrash['Road_Condition_1']=dfNVCrash['Road_Condition_1'].fillna('Not stated')\n",
    "dfNVCrash=cf.update_if_contains(dfNVCrash,'Road_Surface',nv_road_surface_lookup)\n",
    "dfNVCrash=cf.update_if_contains(dfNVCrash,'Road_Condition_1',nv_road_condition_lookup)\n",
    "\n",
    "\n",
    "dfNVCrash['All_Involved'] = dfNVCrash.apply(populate_all_involved,axis=1)\n",
    "#Rename to match CA data\n",
    "\n",
    "\n",
    "# final list of fields\n",
    "dfNVCrash = dfNVCrash[['State',\n",
    "           'CA_Case_ID',\n",
    "           'NV_Accident_Num',\n",
    "           'NV_Accident_Rec_Num',\n",
    "           'Corridor_ID',\n",
    "           'County',\n",
    "           'City',\n",
    "           'Year',\n",
    "           'Date',\n",
    "           'Time',\n",
    "           'Weather_1',\n",
    "           'Weather_2',\n",
    "           'Crash_Severity',\n",
    "           'Num_Killed',\n",
    "           'Num_Injured',\n",
    "           'Num_Ped_Killed',\n",
    "           'Num_Ped_Injured',\n",
    "           'Num_Bicyclist_Killed',\n",
    "           'Num_Bicyclist_Injured',\n",
    "           'Crash_Type',\n",
    "           'Num_Vehicles',\n",
    "           'Num_Parties',\n",
    "           'Violation',\n",
    "           'Hit_and_Run',\n",
    "           'Motor_Vehicle_Interacted_With',\n",
    "           'Pedestrian_Action', \n",
    "           'Road_Condition_1',\n",
    "           'Road_Condition_2',\n",
    "           'Road_Surface',\n",
    "           'Lighting',\n",
    "           'Pedestrian_Involved',\n",
    "           'Bicycle_Involved',\n",
    "           'Motorcycle_Involved',\n",
    "           'Alcohol_Involved',\n",
    "           'Data_Source',\n",
    "           'All_Involved',\n",
    "           'POINT_X',\n",
    "           'POINT_Y']].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.remove(os.path.join(workspace, \"NV_Crash_New.csv\" ))\n",
    "\n",
    "# export dataframe to csv \n",
    "dfNVCrash.to_csv(os.path.join(workspace, \"NV_Crash_New.csv\" ))\n",
    "# get NV CSV for XY Table TO Point\n",
    "nvCSV = os.path.join(workspace, \"NV_Crash_New.csv\" )\n",
    "\n",
    "# name the output feature class\n",
    "nvFC  = 'NV_Crash_New_1'\n",
    "\n",
    "def fieldJoinCalc(updateFC, updateFieldsList, sourceFC, sourceFieldsList):\n",
    "    from time import strftime  \n",
    "    print (\"Started data transfer: \" + strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "#     log.info(\"Started data transfer: \" + strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    # Use list comprehension to build a dictionary from arcpy SearchCursor  \n",
    "    valueDict = {r[0]:(r[1:]) for r in arcpy.da.SearchCursor(sourceFC, sourceFieldsList)}  \n",
    "   \n",
    "    with arcpy.da.UpdateCursor(updateFC, updateFieldsList) as updateRows:  \n",
    "        for updateRow in updateRows:  \n",
    "            # store the Join value of the row being updated in a keyValue variable  \n",
    "            keyValue = updateRow[0]  \n",
    "            # verify that the keyValue is in the Dictionary  \n",
    "            if keyValue in valueDict:  \n",
    "                # transfer the value stored under the keyValue from the dictionary to the updated field.  \n",
    "                updateRow[1] = valueDict[keyValue][0]  \n",
    "                updateRows.updateRow(updateRow)    \n",
    "    del valueDict  \n",
    "    print (\"Finished data transfer: \" + strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "# Nevada data frame to feature class \n",
    "# input data is in NAD 1983 UTM Zone 11N coordinate system\n",
    "arcpy.management.XYTableToPoint(nvCSV, nvFC, \n",
    "                                x_coords, y_coords, \"\",\n",
    "                                # set prjoection transform to from\n",
    "                                arcpy.SpatialReference(26911))\n",
    "\n",
    "# output data for project tool\n",
    "output_NV_Crash_Project = \"NV_Crash_Project_1\"\n",
    "\n",
    "# project from UTM to WGS\n",
    "arcpy.Project_management(nvFC, output_NV_Crash_Project, out_coordinate_system)\n",
    "\n",
    "# os.remove(os.path.join(workspace, \"CA_Crash_New.csv\"))\n",
    "## CA Export\n",
    "# export dataframe to csv \n",
    "dfCACrash.to_csv(os.path.join(workspace, \"CA_Crash_New.csv\" ))\n",
    "\n",
    "# get NV CSV for XY Table TO Point\n",
    "caCSV = os.path.join(workspace, \"CA_Crash_New.csv\" )\n",
    "\n",
    "# name the output feature class\n",
    "caFC     = 'CA_Crash_New_1'\n",
    "\n",
    "# CA data frame to feature class\n",
    "arcpy.management.XYTableToPoint(caCSV, caFC, \n",
    "                                x_coords, y_coords, \"\",\n",
    "                                # set prjoection transform to from\n",
    "                                arcpy.SpatialReference(4326))\n",
    "\n",
    "# output data for project tool\n",
    "output_CA_Crash_Project = \"CA_Crash_Project_1\" \n",
    "\n",
    "# project from UTM to WGS\n",
    "arcpy.Project_management(caFC, output_CA_Crash_Project, out_coordinate_system)\n",
    "\n",
    "## Merge CA and NV\n",
    "# out merge fc\n",
    "tahoeCrash = \"Tahoe_Crash_1\"\n",
    "\n",
    "# input feature classes\n",
    "caCrash = \"CA_Crash_Project_1\"\n",
    "nvCrash = \"NV_Crash_Project_1\"\n",
    "\n",
    "# Create FieldMappings object to manage merge output fields\n",
    "fieldMappings = arcpy.FieldMappings()\n",
    "# Add all fields from all parcel staging layers\n",
    "fieldMappings.addTable(caCrash)\n",
    "fieldMappings.addTable(nvCrash)\n",
    "\n",
    "# Remove all output fields from the field mappings, except fields in field_master list\n",
    "for field in fieldMappings.fields:\n",
    "    if field.name not in [  'OBJECTID',\n",
    "                            'State',\n",
    "                            'CA_Case_ID',\n",
    "                            'NV_Accident_Num',\n",
    "                            'NV_Accident_Rec_Num',\n",
    "                            'Corridor_ID',\n",
    "                            'County',\n",
    "                            'City',\n",
    "                            'Year',\n",
    "                            'Date',\n",
    "                            'Time',\n",
    "                            'Weather_1',\n",
    "                            'Weather_2',\n",
    "                            'Crash_Severity',\n",
    "                            'Num_Killed',\n",
    "                            'Num_Injured',\n",
    "                            'Num_Ped_Killed',\n",
    "                            'Num_Ped_Injured',\n",
    "                            'Num_Bicyclist_Killed',\n",
    "                            'Num_Bicyclist_Injured',\n",
    "                            'Num_Motorcyclist_Killed',\n",
    "                            'Num_Motorcyclist_Injured',\n",
    "                            'Crash_Type',\n",
    "                            'Num_Vehicles',\n",
    "                            'Num_Parties',\n",
    "                            'Violation',\n",
    "                            'Hit_and_Run',\n",
    "                            'Motor_Vehicle_Interacted_With',\n",
    "                            'Pedestrian_Action', \n",
    "                            'Road_Condition_1',\n",
    "                            'Road_Condition_2',\n",
    "                            'Road_Surface',\n",
    "                            'Lighting',\n",
    "                            'Pedestrian_Involved',\n",
    "                            'Bicycle_Involved',\n",
    "                            'Alcohol_Involved',\n",
    "                            'Motorcycle_Involved',\n",
    "                            'Data_Source',\n",
    "                            'POINT_X',\n",
    "                            'POINT_Y',\n",
    "                            'SHAPE@']:\n",
    "        # remove everything else\n",
    "        fieldMappings.removeFieldMap(fieldMappings.findFieldMapIndex(field.name)) \n",
    "    \n",
    "# Use Merge tool to move features into single dataset\n",
    "arcpy.management.Merge([caCrash, nvCrash], tahoeCrash, fieldMappings)\n",
    "print(\"Crash Feature Classes Merged\")\n",
    "\n",
    "# ## Spatial Join of Corridor IDs\n",
    "# in memory points to be used for spatial join results\n",
    "corridorPoints = memory + 'CrashPoint_Corridor'\n",
    "# Spatial Join\n",
    "arcpy.SpatialJoin_analysis(tahoeCrash, corridor, corridorPoints, \n",
    "                           \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", \"\", \"HAVE_THEIR_CENTER_IN\", \"\", \"\")\n",
    "\n",
    "# use function to transfer spatial join results to crash stagin layer\n",
    "fieldJoinCalc(tahoeCrash, ['OBJECTID', 'Corridor_ID'], corridorPoints, ['OBJECTID','CORRIDOR_NAME'])\n",
    "print(\"Finished updating staging layer\")\n",
    "\n",
    "\n",
    "tempLayer = \"deleteLayers\"\n",
    "\n",
    "# Run MakeFeatureLayer\n",
    "arcpy.management.MakeFeatureLayer(tahoeCrash, tempLayer)\n",
    " \n",
    "arcpy.management.SelectLayerByLocation(tempLayer, \"have_their_center_in\", \n",
    "                                       trpa,\n",
    "                                       search_distance=\"\", \n",
    "                                       selection_type=\"NEW_SELECTION\", \n",
    "                                       invert_spatial_relationship=\"INVERT\")\n",
    " \n",
    "# Run GetCount and if some features have been selected, then \n",
    "#  run DeleteFeatures to remove the selected features.\n",
    "if int(arcpy.management.GetCount(tempLayer)[0]) > 0:\n",
    "    arcpy.management.DeleteFeatures(tempLayer)\n",
    "print(\"features deleted\")\n",
    "\n",
    "# outfc = \n",
    "# Update SDE - Truncate Append\n",
    "# updateSDE(tahoeCrash, outfc, fieldnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NV 2021 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from arcgis import GeoAccessor\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "# arcpy.env.workspace = \"F:/GIS/PROJECTS/ResearchAnalysis/Monitoring/Data/Crash/2021 Crash Data/NV/Statewide Data 2021.gdb\"\n",
    "\n",
    "# raw_crash_featureclass = \"Crash_Data_2021\"\n",
    "# person_table = \"Person_Table_2021\"\n",
    "# vehicle_table = \"Vehicle_Table_2021\"\n",
    "# updated_crash_featureclass = 'Crash_Data_2021_Updated'\n",
    "\n",
    "arcpy.env.workspace = \"F:/GIS/PROJECTS/ResearchAnalysis/Monitoring/Data/Crash/2022_Crash_Data/5yr 2019-2023 geodatabase/5YR 2019-2023.gdb\"\n",
    "\n",
    "raw_crash_featureclass = \"Export_Output_5yrstatewide\"\n",
    "person_table = \"person_table\"\n",
    "vehicle_table = \"vehicletable\"\n",
    "updated_crash_featureclass = 'Crash_Data_2022_Updated'\n",
    "\n",
    "\n",
    "# Create a new feature class based on the existing one\n",
    "arcpy.management.CopyFeatures(raw_crash_featureclass, updated_crash_featureclass)\n",
    "\n",
    "# Add x and y fields to the new feature class\n",
    "arcpy.management.AddField(updated_crash_featureclass, 'POINT_X', 'DOUBLE')\n",
    "arcpy.management.AddField(updated_crash_featureclass, 'POINT_Y', 'DOUBLE')\n",
    "\n",
    "# Use an UpdateCursor to calculate x and y coordinates\n",
    "with arcpy.da.UpdateCursor(updated_crash_featureclass, ['SHAPE@X', 'SHAPE@Y', 'POINT_X', 'POINT_Y']) as cursor:\n",
    "    for row in cursor:\n",
    "        x, y = row[0], row[1]\n",
    "        row[2] = x\n",
    "        row[3] = y\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "def import_table_from_fgb(tablename):\n",
    "    data = []\n",
    "\n",
    "    # Use SearchCursor to iterate through the feature class\n",
    "    fields = [field.name for field in arcpy.ListFields(tablename)]\n",
    "    with arcpy.da.SearchCursor(tablename, fields) as cursor:\n",
    "        for row in cursor:\n",
    "            data.append(row)\n",
    "    # Convert the list of tuples to a Pandas DataFrame\n",
    "    df = pd.DataFrame(data, columns=fields)\n",
    "    return df\n",
    "\n",
    "person_df = import_table_from_fgb(person_table)\n",
    "vehicle_df = import_table_from_fgb(vehicle_table)\n",
    "\n",
    "crash_sdf = GeoAccessor.from_featureclass(updated_crash_featureclass)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map and rename columns for dfs from a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "crash_column_mapping = {\n",
    "   'CrashDate' : 'Date',\n",
    "'CrashTime' : 'Time',\n",
    "'Weather' : 'Weather_1',\n",
    "'NumFatalities' : 'Num_Killed',\n",
    "'NumInjured' : 'Num_Injured',\n",
    "'VehCrashType' : 'Crash_Type',\n",
    "'NumVehicles' : 'Num_Vehicles',\n",
    "'RoadSurface' : 'Road_Surface',\n",
    "'LightCondition' : 'Lighting',\n",
    "'PedestrianInvolved' : 'Pedestrian_Involved',\n",
    "'CrashNum' : 'NV_Accident_Num',\n",
    "'CrashSeverity': 'Crash_Severity',\n",
    "'aNo' : 'NV_Accident_Rec_Num',\n",
    "'County' : 'County',\n",
    "'RoadEnvironmentalFactors':'Road_Condition_1',\n",
    "'SHAPE':'SHAPE',\n",
    "'POINT_X':'POINT_X',\n",
    "'POINT_Y':'POINT_Y'\n",
    "}\n",
    "\n",
    "crash_sdf_clean = cf.renamecolumns(crash_sdf,crash_column_mapping,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Lookup Lists\n",
    "crash_type_lookup_nv = cf.import_lookup_dictionary('LookupLists/CrashType_Lookup.csv','key','value','State','NV','FieldName','Crash_Type')\n",
    "lighting_lookup_nv = cf.import_lookup_dictionary('LookupLists/Lighting_Lookup.csv','key','value','State','NV','FieldName','Lighting')\n",
    "road_conditions_lookup_nv  = cf.import_lookup_dictionary('LookupLists/RoadConditions_Lookup.csv','key','value','State','NV','FieldName','Road_Conditions')\n",
    "road_surface_lookup_nv = cf.import_lookup_dictionary('LookupLists/RoadSurface_Lookup.csv', 'key','value','State','NV','FieldName','Road_Surface')\n",
    "weather_lookup_nv = cf.import_lookup_dictionary('LookupLists/Weather_Lookup.csv','key','value','State','NV','FieldName','Weather')\n",
    "crash_lookup_nv = cf.import_lookup_dictionary('LookupLists/CrashSeverity_Lookup.csv','key','value','State','NV','FieldName','Crash_Severity')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crash_sdf_clean['Lighting'] = crash_sdf_clean['Lighting'].replace(lighting_lookup_nv, value=\"Not stated\")\n",
    "#crash_sdf_clean['Road_Condition_2']= crash_sdf_clean['Road_Condition_1']\n",
    "crash_sdf_clean = cf.update_if_contains(crash_sdf_clean,'Road_Condition_1',road_conditions_lookup_nv)\n",
    "#Not really sure what's going on with this field - road surface in Raw data is Asphalt etc but road surface in crash data is dry, wet etc\n",
    "crash_sdf_clean['Road_Surface'] = crash_sdf_clean['Road_Condition_1']\n",
    "crash_sdf_clean =cf.update_if_contains(crash_sdf_clean,'Weather_1', weather_lookup_nv)\n",
    "crash_sdf_clean['Crash_Severity']=crash_sdf_clean['Crash_Severity'].map(crash_lookup_nv)\n",
    "crash_sdf_clean['Lighting']=crash_sdf_clean['Lighting'].map(lighting_lookup_nv)\n",
    "crash_sdf_clean['Time']=crash_sdf_clean['Time'].str.slice(0,2) + ':' + crash_sdf_clean['Time'].str.slice(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use person dataframe to get number of bike involved, pedestrian, alcohol status\n",
    "def type_involved(df_slice, types, type_column):\n",
    "    if (df_slice[type_column].isin(types)).any():\n",
    "        return 'Y'\n",
    "    else:\n",
    "        return 'N'\n",
    "    \n",
    "def alcohol_involved(df_slice, substring):\n",
    "    # Check if the substring is contained anywhere in 'Value2'\n",
    "    if ((df_slice['AlcDrugInvolved'].str.contains(substring)) & (df_slice['PersonType']=='Driver')).any():\n",
    "        return 'Y'  \n",
    "    else: \n",
    "        return 'N'\n",
    "\n",
    "def conditional_count(df_slice, condition_column1, types1, condition_column2, types2):\n",
    "    # Count the number of records meeting a specific condition\n",
    "    return ((df_slice[condition_column1].isin(types1)) & (df_slice[condition_column2].isin(types2))).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_columns_to_keep = ['vcNo', 'VehType']\n",
    "vehicle_df_simple = vehicle_df[vehicle_columns_to_keep]\n",
    "person_df_wt_vehicle = pd.merge(person_df,vehicle_df_simple,how='left',left_on='vcNoRelated',right_on='vcNo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ped_types= ['Skater','Pedestrian','Other Non-Motorist','Wheelchair']\n",
    "mortality_codes = ['K']\n",
    "injury_codes = ['C', 'B', 'A']\n",
    "bike_types = ['Pedal Cyclist', 'E-Bike', 'Other Cyclist']\n",
    "motorcycle_types = ['MC - MOTORCYCLE',\t'MD - MOPED',\t'MOPED',\t'MOTORBIKE',\t'MOTORCYCLE',\t'MOTORSCOOTER',\t'MT - MOTORCYCLE',\t'OTHER']\n",
    "\n",
    "person_grouped = person_df_wt_vehicle.groupby('CrashNum').apply(lambda group_df: pd.Series({\n",
    "    'Num_Ped_Killed': conditional_count(group_df, condition_column1='PersonType',types1=ped_types,condition_column2='InjuryCode',types2=mortality_codes),\n",
    "    'Num_Ped_Injured': conditional_count(group_df, condition_column1='PersonType',types1=ped_types,condition_column2='InjuryCode',types2=injury_codes),\n",
    "    'Num_Bicyclist_Killed': conditional_count(group_df, condition_column1='PersonType',types1=bike_types,condition_column2='InjuryCode',types2=mortality_codes),\n",
    "    'Num_Bicyclist_Injured': conditional_count(group_df, condition_column1='PersonType',types1=bike_types,condition_column2='InjuryCode',types2=injury_codes),\n",
    "    'Num_Motorcyclist_Killed': conditional_count(group_df, condition_column1='VehType',types1=motorcycle_types,condition_column2='InjuryCode',types2=mortality_codes),\n",
    "    'Num_Motorcyclist_Injured': conditional_count(group_df, condition_column1='VehType',types1=motorcycle_types,condition_column2='InjuryCode',types2=injury_codes),\n",
    "    'Bicycle_Involved': type_involved(group_df,bike_types,'PersonType'),\n",
    "    'Motorcycle_Involved': type_involved(group_df,motorcycle_types,'VehType'),\n",
    "    'Alcohol_Involved': alcohol_involved(group_df, 'ALCO'),\n",
    "})).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create final dataframe for NV 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_all_involved(row):\n",
    "    field_value = ''\n",
    "    if row['Pedestrian_Involved']=='Y':\n",
    "        field_value+='Pedestrian, '\n",
    "    if row['Bicycle_Involved']=='Y':\n",
    "        field_value+='Bicycle, '\n",
    "    if row['Motorcycle_Involved']=='Y':\n",
    "        field_value+='Motorcycle, '\n",
    "    return field_value[:-2] if field_value else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amcclary\\AppData\\Local\\Temp\\ipykernel_62152\\1276969577.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  nevada_crashes_2021.loc[~nevada_crashes_2021['Pedestrian_Involved'].isna()]['All_Involved'] = nevada_crashes_2021.loc[~nevada_crashes_2021['Pedestrian_Involved'].isna()].apply(populate_all_involved,axis=1)\n",
      "C:\\Users\\amcclary\\AppData\\Local\\Temp\\ipykernel_62152\\1276969577.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  nevada_crashes_2021.loc[nevada_crashes_2021['Pedestrian_Involved'].isna()]['All_Involved'] = 'Unknown'\n"
     ]
    }
   ],
   "source": [
    "nevada_crashes_2021=pd.merge(crash_sdf_clean, person_grouped, how='left',left_on='NV_Accident_Num',right_on='CrashNum')\n",
    "#Fill in hard coded fields\n",
    "nevada_crashes_2021['Violation']  = \"N/A\"\n",
    "nevada_crashes_2021['Hit_and_Run'] = \"N/A\"\n",
    "nevada_crashes_2021['Motor_Vehicle_Interacted_With'] = \"N/A\"\n",
    "nevada_crashes_2021['Pedestrian_Action'] = \"N/A\"\n",
    "#nevada_crashes_2021['Year']  = \"2021\"\n",
    "nevada_crashes_2021['Data_Source'] = \"NDOT\"\n",
    "nevada_crashes_2021['CA_Case_ID'] = np.nan\n",
    "nevada_crashes_2021['City'] = np.nan\n",
    "nevada_crashes_2021['Num_Parties'] = np.nan\n",
    "nevada_crashes_2021['Corridor_ID'] = np.nan\n",
    "nevada_crashes_2021.loc[~nevada_crashes_2021['Pedestrian_Involved'].isna()]['All_Involved'] = nevada_crashes_2021.loc[~nevada_crashes_2021['Pedestrian_Involved'].isna()].apply(populate_all_involved,axis=1)\n",
    "nevada_crashes_2021.loc[nevada_crashes_2021['Pedestrian_Involved'].isna()]['All_Involved'] = 'Unknown'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert nevada crashes to a year\n",
    "nevada_crashes_2021['Year'] = nevada_crashes_2021['Date'].dt.year.astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nevada_crashes_2022 = nevada_crashes_2021[nevada_crashes_2021['Year'].isin(['2022','2023'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nevada_crashes_2021.to_excel('datatest.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = \"F:/gis/PROJECTS/ResearchAnalysis/Monitoring/Data/Crash/\"\n",
    "\n",
    "# setup environment variables\n",
    "arcpy.env.overwriteOutput = True\n",
    "#arcpy.env.workspace = \"//Trpa-fs01/GIS/PROJECTS/ResearchAnalysis/Monitoring/Data/Crash/CrashData/CrashData.gdb\"\n",
    "arcpy.env.workspace = \"F:/GIS/PROJECTS/ResearchAnalysis/Monitoring/Data/Crash/CrashData/CrashData.gdb\"\n",
    "\n",
    "# create a spatial reference object for the output coordinate system \n",
    "# output projection for data going into SDE should be UTM Zone 10N (EPSG: 26910)\n",
    "out_coordinate_system = arcpy.SpatialReference(26910)\n",
    "\n",
    "# network path to connection files\n",
    "#filePath = \"//Trpa-fs01/GIS/DB_CONNECT\"\n",
    "filePath = \"F:/GIS/DB_CONNECT\"\n",
    "\n",
    "# database file path \n",
    "sdeBase  = os.path.join(filePath, \"Vector.sde\")\n",
    "\n",
    "# SDE feature classes needed for spatial joins\n",
    "corridor = os.path.join(sdeBase, 'sde.SDE.Transportation\\sde.SDE.Corridor')\n",
    "trpa     = os.path.join(sdeBase, 'sde.SDE.Jurisdictions\\sde.SDE.TRPA_bdy')\n",
    "# # in memory files\n",
    "memory = \"memory\" + \"\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='gpresult'><h2 class='msg-title'>Messages</h2><div id='messages'>Start Time: Wednesday, January 29, 2025 2:12:45 PM<br>Succeeded at Wednesday, January 29, 2025 2:13:39 PM (Elapsed Time: 53.11 seconds)<br></div><div id = 'default' /></div></div>"
      ],
      "text/plain": [
       "<Result 'F:/GIS/PROJECTS/ResearchAnalysis/Monitoring/Data/Crash/CrashData/CrashData.gdb/NV_Crash_2022_Project'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# export dataframe to csv \n",
    "nevada_crashes_2022.to_csv(os.path.join(workspace, \"NV_Crash_2022.csv\" ))\n",
    "# get NV CSV for XY Table TO Point\n",
    "nvCSV = os.path.join(workspace, \"NV_Crash_2022.csv\" )\n",
    "\n",
    "# name the output feature class\n",
    "nvFC  = 'NV_Crash_2022'\n",
    "\n",
    "\n",
    "\n",
    "# Nevada data frame to feature class \n",
    "# input data is in NAD 1983 UTM Zone 11N coordinate system\n",
    "arcpy.management.XYTableToPoint(nvCSV, nvFC, \n",
    "                                x_coords, y_coords, \"\",\n",
    "                                # set prjoection transform to from\n",
    "                                arcpy.SpatialReference(26911))\n",
    "# output data for project tool\n",
    "output_NV_Crash_Project = \"NV_Crash_2022_Project\"\n",
    "\n",
    "# project from UTM to WGS\n",
    "arcpy.Project_management(nvFC, output_NV_Crash_Project, out_coordinate_system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started data transfer: 2025-01-29 16:36:03\n",
      "Finished data transfer: 2025-01-29 16:36:07\n",
      "Finished updating staging layer\n",
      "features deleted\n"
     ]
    }
   ],
   "source": [
    "# ## Spatial Join of Corridor IDs\n",
    "# in memory points to be used for spatial join results\n",
    "corridorPoints = memory + 'CrashPoint_Corridor'\n",
    "# Spatial Join\n",
    "tahoeCrash = \"NV_Crash_2022_Project\"\n",
    "\n",
    "arcpy.SpatialJoin_analysis(tahoeCrash, corridor, corridorPoints, \n",
    "                           \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", \"\", \"HAVE_THEIR_CENTER_IN\", \"\", \"\")\n",
    "\n",
    "# use function to transfer spatial join results to crash stagin layer\n",
    "\n",
    "\n",
    "\n",
    "def fieldJoinCalc(updateFC, updateFieldsList, sourceFC, sourceFieldsList):\n",
    "    from time import strftime  \n",
    "    print (\"Started data transfer: \" + strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "#     log.info(\"Started data transfer: \" + strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    # Use list comprehension to build a dictionary from arcpy SearchCursor  \n",
    "    valueDict = {r[0]:(r[1:]) for r in arcpy.da.SearchCursor(sourceFC, sourceFieldsList)}  \n",
    "   \n",
    "    with arcpy.da.UpdateCursor(updateFC, updateFieldsList) as updateRows:  \n",
    "        for updateRow in updateRows:  \n",
    "            # store the Join value of the row being updated in a keyValue variable  \n",
    "            keyValue = updateRow[0]  \n",
    "            # verify that the keyValue is in the Dictionary  \n",
    "            if keyValue in valueDict:  \n",
    "                # transfer the value stored under the keyValue from the dictionary to the updated field.  \n",
    "                updateRow[1] = valueDict[keyValue][0]  \n",
    "                updateRows.updateRow(updateRow)    \n",
    "    del valueDict  \n",
    "    print (\"Finished data transfer: \" + strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "fieldJoinCalc(tahoeCrash, ['OBJECTID', 'Corridor_ID'], corridorPoints, ['OBJECTID','CORRIDOR_NAME'])\n",
    "print(\"Finished updating staging layer\")\n",
    "\n",
    "\n",
    "tempLayer = \"deleteLayers\"\n",
    "\n",
    "# Run MakeFeatureLayer\n",
    "arcpy.management.MakeFeatureLayer(tahoeCrash, tempLayer)\n",
    " \n",
    "arcpy.management.SelectLayerByLocation(tempLayer, \"have_their_center_in\", \n",
    "                                       trpa,\n",
    "                                       search_distance=\"\", \n",
    "                                       selection_type=\"NEW_SELECTION\", \n",
    "                                       invert_spatial_relationship=\"INVERT\")\n",
    " \n",
    "# Run GetCount and if some features have been selected, then \n",
    "#  run DeleteFeatures to remove the selected features.\n",
    "if int(arcpy.management.GetCount(tempLayer)[0]) > 0:\n",
    "    arcpy.management.DeleteFeatures(tempLayer)\n",
    "print(\"features deleted\")\n",
    "\n",
    "# outfc = \n",
    "# Update SDE - Truncate Append\n",
    "# updateSDE(tahoeCrash, outfc, fieldnames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

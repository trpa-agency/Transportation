{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import os\n",
    "import arcpy\n",
    "from utils import *\n",
    "import numpy as np\n",
    "import pickle\n",
    "# external connection packages\n",
    "from sqlalchemy.engine import URL\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# pandas options\n",
    "pd.options.mode.copy_on_write = True\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = 999\n",
    "pd.options.display.max_rows    = 999\n",
    "\n",
    "# my workspace \n",
    "workspace = r\"C:\\Users\\mbindl\\Desktop\\Workspace.gdb\"\n",
    "# current working directory\n",
    "local_path = pathlib.Path().absolute()\n",
    "\n",
    "# get bonus_condit\n",
    "# set data path as a subfolder of the current working directory TravelDemandModel\\2022\\\n",
    "data_dir = local_path.parents[0] / 'data'\n",
    "# folder to save processed data\n",
    "out_dir  = local_path.parents[0] / 'data/processed_data'\n",
    "# workspace gdb for stuff that doesnt work in memory\n",
    "# gdb = os.path.join(local_path,'Workspace.gdb')\n",
    "gdb = workspace\n",
    "# set environement workspace to in memory \n",
    "arcpy.env.workspace = 'memory'\n",
    "# # clear memory workspace\n",
    "# arcpy.management.Delete('memory')\n",
    "\n",
    "# overwrite true\n",
    "arcpy.env.overwriteOutput = True\n",
    "# Set spatial reference to NAD 1983 UTM Zone 10N\n",
    "sr = arcpy.SpatialReference(26910)\n",
    "\n",
    "# get parcels from the database\n",
    "# network path to connection files\n",
    "filePath = \"F:/GIS/PARCELUPDATE/Workspace/\"\n",
    "# database file path \n",
    "sdeBase    = os.path.join(filePath, \"Vector.sde\")\n",
    "sdeCollect = os.path.join(filePath, \"Collection.sde\")\n",
    "sdeTabular = os.path.join(filePath, \"Tabular.sde\")\n",
    "sdeEdit    = os.path.join(filePath, \"Edit.sde\")\n",
    "\n",
    "# Pickle variables\n",
    "# part 1 - spatial joins and new categorical fields\n",
    "parcel_pickle_part1    = data_dir / 'parcel_pickle1.pkl'\n",
    "# part 2 - forecasting applied\n",
    "parcel_pickle_part2    = data_dir / 'parcel_pickle2.pkl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from arcgis import GeoAccessor, GeoSeriesAccessor\n",
    "\n",
    "# current working directory\n",
    "local_path = Path().absolute()\n",
    "# folder to save processed data\n",
    "out_dir  = local_path.parents[0] / 'data'\n",
    "# path to the parcel master feature class\n",
    "parcel_master = Path(\"F:/GIS/DB_CONNECT/Vector.sde\") / \"sde.SDE.Parcel_Master\"\n",
    "# get data frame the feature class\n",
    "sdfParcels    = pd.DataFrame.spatial.from_featureclass(parcel_master)\n",
    "\n",
    "# Filter the DataFrame to include only rows where 'YEAR_BUILT' is numeric, not '0', and not blank, space, or NaN\n",
    "sdf = sdfParcels.loc[\n",
    "    sdfParcels['YEAR_BUILT'].astype(str).str.isnumeric() & \n",
    "    (sdfParcels['YEAR_BUILT'].astype(str).str.strip() != '') & \n",
    "    (sdfParcels['YEAR_BUILT'] != '0')\n",
    "].copy()\n",
    "\n",
    "# Convert 'YEAR_BUILT' to integer\n",
    "sdf.loc[:, 'YEAR_BUILT'] = sdf['YEAR_BUILT'].astype(int)\n",
    "\n",
    "# Create 'Before1975' and 'After1975' columns\n",
    "sdf.loc[:, 'Before1975'] = (sdf['YEAR_BUILT'] <= 1975).astype(int)\n",
    "sdf.loc[:, 'After1975'] = (sdf['YEAR_BUILT'] > 1975).astype(int)\n",
    "\n",
    "# Group by jurisdiction and sum the counts\n",
    "df = sdf.groupby('JURISDICTION')[['Before1975', 'After1975']].sum().reset_index()\n",
    "\n",
    "# Calculate percentage columns\n",
    "df['Before1975_percent'] = (df['Before1975'] / (df['Before1975'] + df['After1975']) * 100).astype(int)\n",
    "df['After1975_percent'] = (df['After1975'] / (df['Before1975'] + df['After1975']) * 100).astype(int)\n",
    "\n",
    "# Rename columns to be more readable with % sign\n",
    "df.rename(columns={\n",
    "    'JURISDICTION':'Jurisdiction',\n",
    "    'Before1975': 'Built 1975 or Before',\n",
    "    'After1975': 'After 1975',\n",
    "    'Before1975_percent': 'Built 1975 or Before (%)',\n",
    "    'After1975_percent': 'Built After 1975 (%)'\n",
    "}, inplace=True)\n",
    "\n",
    "# Jurisdiction names\n",
    "jurisdictions = {\n",
    "    'CC': 'Carson City County',\n",
    "    'DG': 'Douglas County',\n",
    "    'CSLT': 'City of South Lake Tahoe',\n",
    "    'EL': 'El Dorado County',\n",
    "    'PL': 'Placer County',\n",
    "    'WA': 'Washoe County'\n",
    "}\n",
    "# change values of Jurisdiction to jurisdiction names\n",
    "df['Jurisdiction'] = df['Jurisdiction'].map(jurisdictions)\n",
    "df.to_csv(out_dir / 'TahoeParcels_Built1975byJurisdiction.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spjn_parcel_corridor = \"C:\\Users\\mbindl\\Documents\\GitHub\\Transportation\\RegionalTransportationPlan\\2023\\data\\SpJn_Parcel_Corridor.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spjn_parcel_corridor = r\"C:\\Users\\mbindl\\Documents\\GitHub\\Transportation\\RegionalTransportationPlan\\2023\\data\\SpJn_Parcel_Corridor.csv\"\n",
    "# get csv as df\n",
    "df = pd.read_csv(spjn_parcel_corridor)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get total Residential Units, Tourist Units, and Commercial Units by Corridor\n",
    "df.melt(id_vars=['CORRIDOR_NAME'], \n",
    "        value_vars=['Residential_Units', 'TouristAccommodation_Units', 'CommercialFloorArea_SqFt'], \n",
    "        var_name='Unit Type', \n",
    "        value_name='Value').groupby(['CORRIDOR_NAME', 'Unit Type']).sum()\n",
    "\n",
    "# get total Residential Units, Tourist Units, and Commercial Units by Corridor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pivot_table to aggregate and get total units by corridor and unit type\n",
    "pivot_df = df.pivot_table(index='CORRIDOR_NAME', \n",
    "                          values=['Residential_Units', 'TouristAccommodation_Units', 'CommercialFloorArea_SqFt'], \n",
    "                          aggfunc='sum', \n",
    "                          fill_value=0).reset_index()\n",
    "\n",
    "pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out parcels with 0 or NaN Residential Units\n",
    "df_filtered = df[df['Residential_Units'].notna() & (df['Residential_Units'] > 0)]\n",
    "\n",
    "# Create categories based on the number of Residential Units\n",
    "bins = [1, 2, 20, float('inf')]  # Defines the categories: (0, 1], (1, 20], (20, inf)\n",
    "labels = ['1 Residential Unit', '2-20 Residential Units', '>20 Residential Units']  # Labels for each bin\n",
    "\n",
    "# Add a new column to categorize parcels based on their Residential_Units\n",
    "df_filtered['Residential_Unit_Category'] = pd.cut(df_filtered['Residential_Units'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Group by CORRIDOR_NAME and Residential_Unit_Category, and calculate counts\n",
    "category_counts_by_corridor = df_filtered.groupby(['CORRIDOR_NAME', 'Residential_Unit_Category']).size().unstack(fill_value=0)\n",
    "\n",
    "# Calculate the percentage of parcels in each category for each corridor\n",
    "category_percentages_by_corridor = category_counts_by_corridor.div(category_counts_by_corridor.sum(axis=1), axis=0) * 100\n",
    "\n",
    "category_percentages_by_corridor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out parcels with NaN in EXISTING_LANDUSE (optional, if needed)\n",
    "df_filtered_landuse = df[df['EXISTING_LANDUSE'].notna()]\n",
    "\n",
    "# Calculate the count of parcels for each residential land use type\n",
    "landuse_counts = df_filtered_landuse['EXISTING_LANDUSE'].value_counts()\n",
    "\n",
    "# Calculate the percentage of parcels for each land use type relative to the total number of parcels\n",
    "total_parcels_landuse = len(df_filtered_landuse)\n",
    "landuse_percentages = (landuse_counts / total_parcels_landuse) * 100\n",
    "\n",
    "landuse_percentages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data for the relevant land use types\n",
    "landuse_filtered = df[df['EXISTING_LANDUSE'].isin(['Single Family Residential', 'Condominium', 'Multi-Family Residential'])]\n",
    "\n",
    "# Calculate the count of parcels for each of these land use types\n",
    "landuse_counts_filtered = landuse_filtered['EXISTING_LANDUSE'].value_counts()\n",
    "\n",
    "# Calculate the percentage of each land use type relative to the total of these three types\n",
    "total_filtered = landuse_counts_filtered.sum()\n",
    "landuse_percentages_filtered = (landuse_counts_filtered / total_filtered) * 100\n",
    "\n",
    "# Display the results\n",
    "print(landuse_percentages_filtered)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arcgispro-py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

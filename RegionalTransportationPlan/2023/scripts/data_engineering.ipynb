{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RTP Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import os\n",
    "import arcpy\n",
    "from utils import *\n",
    "import numpy as np\n",
    "import pickle\n",
    "# external connection packages\n",
    "from sqlalchemy.engine import URL\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# pandas options\n",
    "pd.options.mode.copy_on_write = True\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = 999\n",
    "pd.options.display.max_rows    = 999\n",
    "\n",
    "# my workspace \n",
    "workspace = r\"C:\\Users\\mbindl\\Desktop\\Workspace.gdb\"\n",
    "# current working directory\n",
    "local_path = pathlib.Path().absolute()\n",
    "# set data path as a subfolder of the current working directory TravelDemandModel\\2022\\\n",
    "data_dir = local_path.parents[0] / 'data'\n",
    "# folder to save processed data\n",
    "out_dir  = local_path.parents[0] / 'data/processed_data'\n",
    "# workspace gdb for stuff that doesnt work in memory\n",
    "# gdb = os.path.join(local_path,'Workspace.gdb')\n",
    "gdb = workspace\n",
    "# set environement workspace to in memory \n",
    "arcpy.env.workspace = 'memory'\n",
    "# # clear memory workspace\n",
    "# arcpy.management.Delete('memory')\n",
    "\n",
    "# overwrite true\n",
    "arcpy.env.overwriteOutput = True\n",
    "# Set spatial reference to NAD 1983 UTM Zone 10N\n",
    "sr = arcpy.SpatialReference(26910)\n",
    "\n",
    "# get parcels from the database\n",
    "# network path to connection files\n",
    "filePath = \"F:/GIS/PARCELUPDATE/Workspace/\"\n",
    "# database file path \n",
    "sdeBase    = os.path.join(filePath, \"Vector.sde\")\n",
    "sdeCollect = os.path.join(filePath, \"Collection.sde\")\n",
    "sdeTabular = os.path.join(filePath, \"Tabular.sde\")\n",
    "sdeEdit    = os.path.join(filePath, \"Edit.sde\")\n",
    "\n",
    "# Pickle variables\n",
    "# part 1 - spatial joins and new categorical fields\n",
    "parcel_pickle_part1    = data_dir / 'parcel_pickle1.pkl'\n",
    "# part 2 - forecasting applied\n",
    "parcel_pickle_part2    = data_dir / 'parcel_pickle2.pkl'\n",
    "\n",
    "# columsn to list\n",
    "initial_columns = [ 'APN',\n",
    "                    'APO_ADDRESS',\n",
    "                    'Residential_Units',\n",
    "                    'TouristAccommodation_Units',\n",
    "                    'CommercialFloorArea_SqFt',\n",
    "                    'YEAR',\n",
    "                    'JURISDICTION',\n",
    "                    'COUNTY',\n",
    "                    'OWNERSHIP_TYPE',\n",
    "                    'COUNTY_LANDUSE_DESCRIPTION',\n",
    "                    'EXISTING_LANDUSE',\n",
    "                    'REGIONAL_LANDUSE',\n",
    "                    'YEAR_BUILT',\n",
    "                    'PLAN_ID',\n",
    "                    'PLAN_NAME',\n",
    "                    'ZONING_ID',\n",
    "                    'ZONING_DESCRIPTION',\n",
    "                    'TOWN_CENTER',\n",
    "                    'LOCATION_TO_TOWNCENTER',\n",
    "                    'TAZ',\n",
    "                    'PARCEL_ACRES',\n",
    "                    'PARCEL_SQFT',\n",
    "                    'WITHIN_BONUSUNIT_BNDY',\n",
    "                    'WITHIN_TRPA_BNDY',\n",
    "                    'SHAPE']\n",
    "\n",
    "# schema for the final output\n",
    "final_schema = [ 'APN',\n",
    "                'APO_ADDRESS',\n",
    "                'Residential_Units',\n",
    "                'TouristAccommodation_Units',\n",
    "                'CommercialFloorArea_SqFt',\n",
    "                'YEAR',\n",
    "                'JURISDICTION',\n",
    "                'COUNTY',\n",
    "                'OWNERSHIP_TYPE',\n",
    "                'COUNTY_LANDUSE_DESCRIPTION',\n",
    "                'EXISTING_LANDUSE',\n",
    "                'REGIONAL_LANDUSE',\n",
    "                'YEAR_BUILT',\n",
    "                'PLAN_ID',\n",
    "                'PLAN_NAME',\n",
    "                'ZONING_ID',\n",
    "                'ZONING_DESCRIPTION',\n",
    "                'TOWN_CENTER',\n",
    "                'LOCATION_TO_TOWNCENTER',\n",
    "                'TAZ',\n",
    "                'PARCEL_ACRES',\n",
    "                'PARCEL_SQFT',\n",
    "                'WITHIN_BONUSUNIT_BNDY',\n",
    "                'WITHIN_TRPA_BNDY',\n",
    "                'IPES_SCORE',\n",
    "                'IPES_SCORE_TYPE',\n",
    "                'RETIRED',\n",
    "                'HOUSING_ZONING',\n",
    "                'MAX_RESIDENTIAL_UNITS', \n",
    "                'MAX_COMMERCIAL_FLOOR_AREA', \n",
    "                'MAX_TAU_UNITS',\n",
    "                'SHAPE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conn(db):\n",
    "    # Get database user and password from environment variables on machine running script\n",
    "    db_user             = os.environ.get('DB_USER')\n",
    "    db_password         = os.environ.get('DB_PASSWORD')\n",
    "    # driver is the ODBC driver for SQL Server\n",
    "    driver              = 'ODBC Driver 17 for SQL Server'\n",
    "    # server names are\n",
    "    sql_12              = 'sql12'\n",
    "    sql_14              = 'sql14'\n",
    "    # make it case insensitive\n",
    "    db = db.lower()\n",
    "    # make sql database connection with pyodbc\n",
    "    if db   == 'sde_tabular':\n",
    "        connection_string = f\"DRIVER={driver};SERVER={sql_12};DATABASE={db};UID={db_user};PWD={db_password}\"\n",
    "        connection_url = URL.create(\"mssql+pyodbc\", query={\"odbc_connect\": connection_string})\n",
    "        engine = create_engine(connection_url)\n",
    "    elif db == 'tahoebmpsde':\n",
    "        connection_string = f\"DRIVER={driver};SERVER={sql_14};DATABASE={db};UID={db_user};PWD={db_password}\"\n",
    "        connection_url = URL.create(\"mssql+pyodbc\", query={\"odbc_connect\": connection_string})\n",
    "        engine = create_engine(connection_url)\n",
    "    elif db == 'sde':\n",
    "        connection_string = f\"DRIVER={driver};SERVER={sql_12};DATABASE={db};UID={db_user};PWD={db_password}\"\n",
    "        connection_url = URL.create(\"mssql+pyodbc\", query={\"odbc_connect\": connection_string})\n",
    "        engine = create_engine(connection_url)\n",
    "    # else return None\n",
    "    else:\n",
    "        engine = None\n",
    "    # connection file to use in pd.read_sql\n",
    "    return engine\n",
    "\n",
    "# save to pickle\n",
    "def to_pickle(data, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f'{filename} pickled')\n",
    "\n",
    "# save to pickle and feature class\n",
    "def to_pickle_fc(data, filename):\n",
    "    data.spatial.to_featureclass(filename)\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f'{filename} pickled and saved as feature class')\n",
    "\n",
    "# get a pickled file as a dataframe\n",
    "def from_pickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    print(f'{filename} unpickled')\n",
    "    return data\n",
    "\n",
    "# function to get where Zoningin_ID Use_Type = Multi-Family and Density\n",
    "def get_mf_zones(df):\n",
    "    columns_to_keep = ['Zoning_ID', 'Use_Type', 'Density']\n",
    "    # filter Use_Type to Multiple Family Dwelling\n",
    "    df = df.loc[df['Use_Type'] == 'Multiple Family Dwelling']\n",
    "    return df[columns_to_keep]\n",
    "\n",
    "def get_sf_zones(df):\n",
    "    columns_to_keep = ['Zoning_ID', 'Use_Type', 'Density']\n",
    "    # filter Use_Type to Single Family Dwelling\n",
    "    df = df.loc[df['Use_Type'] == 'Single Family Dwelling']\n",
    "    return df[columns_to_keep]\n",
    "\n",
    "def get_sf_only_zones(df):\n",
    "    columns_to_keep = ['Zoning_ID', 'Use_Type', 'Density']\n",
    "    # filter Use_Type to Single Family Dwelling and not Multiple Family Dwelling\n",
    "    dfMF = get_mf_zones(df)\n",
    "    dfSF = get_sf_zones(df)\n",
    "    df = dfSF.loc[~dfSF['Zoning_ID'].isin(dfMF['Zoning_ID'])]\n",
    "    return df[columns_to_keep]\n",
    "\n",
    "def get_mf_only_zones(df):\n",
    "    columns_to_keep = ['Zoning_ID', 'Use_Type', 'Density']\n",
    "    # filter Use_Type to Multiple Family Dwelling and not Single Family Dwelling\n",
    "    dfSF = get_sf_zones(df)\n",
    "    dfMF = get_mf_zones(df)\n",
    "    df = dfMF.loc[~dfMF['Zoning_ID'].isin(dfSF['Zoning_ID'])]\n",
    "    return df[columns_to_keep]\n",
    "\n",
    "def get_recieving_zones(df):\n",
    "    columns_to_keep = ['Zoning_ID', 'SPECIAL_DESIGNATION']\n",
    "    # filter transfer recieving\n",
    "    df = df.loc[df['SPECIAL_DESIGNATION'] == 'Receive']\n",
    "    return df[columns_to_keep]\n",
    "\n",
    "def get_sending_zones(df):\n",
    "    columns_to_keep = ['Zoning_ID', 'SPECIAL_DESIGNATION']\n",
    "    df = df.loc[df['SPECIAL_DESIGNATION'] == 'Transfer']\n",
    "    return df[columns_to_keep]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parcel development layer polygons\n",
    "parcel_db = Path(sdeEdit) / \"SDE.Parcel\\\\SDE.Parcel_History_Attributed\"\n",
    "# query 2022 rows\n",
    "sdf_units = pd.DataFrame.spatial.from_featureclass(parcel_db)\n",
    "sdf_units = sdf_units.loc[sdf_units['YEAR'] == 2022]\n",
    "sdf_units.spatial.sr = sr\n",
    "\n",
    "# # get parcel level data from Collection SDE\n",
    "# vhr feature layer polygons \n",
    "vhr_db = Path(sdeCollect) / \"SDE.Parcel\\\\SDE.Parcel_VHR\"\n",
    "sdf_vhr = pd.DataFrame.spatial.from_featureclass(vhr_db)\n",
    "sdf_vhr.spatial.sr = sr\n",
    "# filter vhr layer to active status\n",
    "sdf_vhr = sdf_vhr.loc[sdf_vhr['Status'] == 'Active']\n",
    "\n",
    "# TAZ feature layer polygons\n",
    "taz_db = Path(sdeBase) / \"SDE.Transportation\\\\SDE.Transportation_Analysis_Zone\"\n",
    "# get as spatial dataframe\n",
    "sdf_taz = pd.DataFrame.spatial.from_featureclass(taz_db)\n",
    "# set spatial reference to NAD 1983 UTM Zone 10N\n",
    "sdf_taz.spatial.sr = sr\n",
    "\n",
    "# censuse feature class\n",
    "census_fc    = Path(sdeBase) / \"SDE.Census\\\\SDE.Tahoe_Census_Geography\"\n",
    "# bouns unit boundary feature class\n",
    "bonus_unit_fc = Path(sdeBase) / \"SDE.Planning\\SDE.Bonus_unit_boundary\"\n",
    "\n",
    "# disable Z values on block group feature layer\n",
    "with arcpy.EnvManager(outputZFlag=\"Disabled\"):    \n",
    "    arcpy.conversion.FeatureClassToGeodatabase(\n",
    "        Input_Features=\"F:\\GIS\\DB_CONNECT\\Vector.sde\\SDE.Census\\SDE.Tahoe_Census_Geography\",\n",
    "        Output_Geodatabase=r\"C:\\Users\\mbindl\\Desktop\\Workspace.gdb\"\n",
    "    )\n",
    "# disable Z values on block group feature layer\n",
    "with arcpy.EnvManager(outputZFlag=\"Disabled\"):    \n",
    "    arcpy.conversion.FeatureClassToGeodatabase(\n",
    "        Input_Features=\"F:\\GIS\\DB_CONNECT\\Vector.sde\\SDE.Planning\\SDE.Bonus_unit_boundary\",\n",
    "        Output_Geodatabase=r\"C:\\Users\\mbindl\\Desktop\\Workspace.gdb\"\n",
    "    )\n",
    "\n",
    "# block group feature layer polygons with no Z\n",
    "sdf_block = pd.DataFrame.spatial.from_featureclass(Path(gdb) / 'Tahoe_Census_Geography')\n",
    "sdf_block = sdf_block.loc[(sdf_block['YEAR'] == 2020) & (sdf_block['GEOGRAPHY'] == 'Block Group')]\n",
    "sdf_block.spatial.sr = sr\n",
    "\n",
    "# bonus unit boundary wihtout Z\n",
    "sdf_bonus = pd.DataFrame.spatial.from_featureclass(Path(gdb) / 'Bonus_unit_boundary')\n",
    "sdf_bonus.spatial.sr = sr\n",
    "\n",
    "# get parcel level data from LTinfo\n",
    "dfIPES       = pd.read_json(\"https://www.laketahoeinfo.org/WebServices/GetParcelIPESScores/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476\")\n",
    "dfLCV_LTinfo = pd.read_json('https://www.laketahoeinfo.org/WebServices/GetParcelsByLandCapability/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476')\n",
    "dfRetired    = pd.read_json(\"https://www.laketahoeinfo.org/WebServices/GetAllParcels/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476\")\n",
    "dfBankedDev  = pd.read_json('https://www.laketahoeinfo.org/WebServices/GetBankedDevelopmentRights/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476')\n",
    "dfTransacted = pd.read_json('https://www.laketahoeinfo.org/WebServices/GetTransactedAndBankedDevelopmentRights/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476')\n",
    "dfAllParcels = pd.read_json('https://www.laketahoeinfo.org/WebServices/GetAllParcels/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476')\n",
    "\n",
    "# get use tables \n",
    "# zoning data\n",
    "sde_engine = get_conn('sde')\n",
    "with sde_engine.begin() as conn:\n",
    "    df_uses    = pd.read_sql(\"SELECT * FROM sde.SDE.PermissibleUses\", conn)\n",
    "    df_special = pd.read_sql(\"SELECT * FROM sde.SDE.Special_Designation\", conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parcel Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial join to get TAZ\n",
    "arcpy.SpatialJoin_analysis(sdf_units, sdf_taz, \"Existing_Development_TAZ\", \n",
    "                           \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", \"\", \"HAVE_THEIR_CENTER_IN\")\n",
    "# spatial join to get Block Group\n",
    "arcpy.SpatialJoin_analysis(sdf_units, sdf_block, \"Existing_Development_BlockGroup\", \n",
    "                           \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", \"\", \"HAVE_THEIR_CENTER_IN\")\n",
    "# spatial join of Bonus Unit Boundary\n",
    "arcpy.SpatialJoin_analysis(sdf_units, sdf_bonus, \"Existing_Development_BonusUnitBoundary\",\n",
    "                            \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", \"\", \"INTERSECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial dataframe with only initial columns\n",
    "sdfParcels = sdf_units[initial_columns]\n",
    "\n",
    "# get results of spatial joins as spatial dataframes\n",
    "sdf_units_taz   = pd.DataFrame.spatial.from_featureclass(\"Existing_Development_TAZ\", sr=sr)  \n",
    "sdf_units_block = pd.DataFrame.spatial.from_featureclass(\"Existing_Development_BlockGroup\", sr=sr)\n",
    "sdf_units_bonus = pd.DataFrame.spatial.from_featureclass(\"Existing_Development_BonusUnitBoundary\", sr=sr)\n",
    "# cast to string\n",
    "sdf_units_bonus['WITHIN_BONUSUNIT_BNDY'] = sdf_units_bonus['WITHIN_BONUSUNIT_BNDY'].astype(str)\n",
    "sdf_units_bonus['WITHIN_BONUSUNIT_BNDY'] = 'No'\n",
    "# if Id is not NA then within bonus unit boundary = yes, else\n",
    "sdf_units_bonus.loc[sdf_units_bonus['Id'].notna(), 'WITHIN_BONUSUNIT_BNDY'] = 'Yes'\n",
    "\n",
    "# map dictionary to sdf_units dataframe to fill in TAZ and Block Group fields\n",
    "sdfParcels['TAZ']                   = sdfParcels.APN.map(dict(zip(sdf_units_taz.APN,   sdf_units_taz.TAZ_1)))\n",
    "sdfParcels['BLOCK_GROUP']           = sdfParcels.APN.map(dict(zip(sdf_units_block.APN, sdf_units_block.TRPAID)))\n",
    "# map IPES score to parcels\n",
    "sdfParcels['IPES_SCORE']            = sdfParcels['APN'].map(dict(zip(dfIPES.APN, dfIPES.IPESScore)))\n",
    "sdfParcels['IPES_SCORE_TYPE']       = sdfParcels['APN'].map(dict(zip(dfIPES.APN, dfIPES.IPESScoreType)))\n",
    "# retired parcels\n",
    "sdfParcels['RETIRED']               = sdfParcels['APN'].map(dict(zip(dfAllParcels.APN, dfAllParcels.RetiredFromDevelopment)))\n",
    "sdfParcels['WITHIN_BONUSUNIT_BNDY'] = sdfParcels['APN'].map(dict(zip(sdf_units_bonus.APN, sdf_units_bonus.WITHIN_BONUSUNIT_BNDY)))\n",
    "# define housnig zoning and density\n",
    "sdfParcels['HOUSING_ZONING']        = 'NA'\n",
    "\n",
    "# if the zoning id is in the list of single family zones then set to SF\n",
    "sdfParcels.loc[sdfParcels['ZONING_ID'].isin(get_sf_zones(df_uses)['Zoning_ID']), 'HOUSING_ZONING'] = 'SF'\n",
    "# if the zoning id is in the list of multiple family zones then set to MF\n",
    "sdfParcels.loc[sdfParcels['ZONING_ID'].isin(get_mf_zones(df_uses)['Zoning_ID']), 'HOUSING_ZONING'] = 'MF'\n",
    "# if the zoning id is in the list of single family zones and not in the multiple family zones then set to SF only\n",
    "sdfParcels.loc[sdfParcels['ZONING_ID'].isin(get_sf_only_zones(df_uses)['Zoning_ID']), 'HOUSING_ZONING'] = 'SF_only'\n",
    "# if the zoning id is in the list of multiple family zones and not in the single family zones then set to MF only\n",
    "sdfParcels.loc[sdfParcels['ZONING_ID'].isin(get_mf_only_zones(df_uses)['Zoning_ID']), 'HOUSING_ZONING'] = 'MF_only'\n",
    "\n",
    "# get density for MF and MF only zones, max residential units, and adjusted residential units\n",
    "dfMF = get_mf_zones(df_uses)\n",
    "sdfParcels['DENSITY']                    = sdfParcels['ZONING_ID'].map(dict(zip(dfMF.Zoning_ID, dfMF.Density)))\n",
    "sdfParcels['MAX_RESIDENTIAL_UNITS']      = sdfParcels['PARCEL_ACRES'] * sdfParcels['DENSITY']\n",
    "sdfParcels['MAX_BUILDABLE_UNITS'] = sdfParcels['MAX_RESIDENTIAL_UNITS']*0.6\n",
    "\n",
    "# if COUNTY is in EL or PL and SF allowed then set ADU_ALLOWED to yes\n",
    "sdfParcels['ADU_ALLOWED'] = 'No'\n",
    "sdfParcels.loc[(sdfParcels['COUNTY'].isin(['EL','PL'])) & (~sdfParcels['HOUSING_ZONING'].isin(['MF_only', 'NA'])), 'ADU_ALLOWED'] = 'Yes'\n",
    "sdfParcels.loc[(sdfParcels['COUNTY'].isin(['WA','DG','CC'])) & (sdfParcels['PARCEL_ACRES']>=1) &(~sdfParcels['HOUSING_ZONING'].isin(['MF_only', 'NA'])), 'ADU_ALLOWED'] = 'Yes'\n",
    "\n",
    "sdfParcels['POTENTIAL_BUILDABLE_UNITS'] = 0\n",
    "sdfParcels.loc[(sdfParcels['Residential_Units']>0)&(sdfParcels['HOUSING_ZONING'].isin(['MF','MF_only','SF_only'])), 'POTENTIAL_BUILDABLE_UNITS'] = sdfParcels['MAX_BUILDABLE_UNITS']-sdfParcels['Residential_Units']\n",
    "\n",
    "# export to pickle\n",
    "sdfParcels.to_pickle(parcel_pickle_part1)\n",
    "# to feature class\n",
    "sdfParcels.spatial.to_featureclass(Path(gdb)/'Parcel_Base_2022')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdfParcels.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Year Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residential Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Start with Known projects\t\n",
    "* Add Known Residential Allocations from 2023-2024\n",
    "* Add Known Residential Bonus Units from permitted projects, applications in review, and RBU reservations in LT Info\n",
    "* Add Known Accessory Dwelling permits not completed\n",
    "* Remove Known Banking projects that removed units in 2023-2024\n",
    "    \n",
    "2) Assign Remaining Residential Bonus Units within Residential Bonus Unit Boundary\t\n",
    "* Identify\tVacant buildable lots within Bonus Unit boundary - calculate allowed density\n",
    "* Assign remaining jurisdiction pool units to available parcels within jurisidiction\n",
    "* Assign remaining TRPA pool units to available parcels throughout region (use adjusted weighting from existing residential units?)\n",
    "* Calcuate Remaining local jurisdiction Residential Allocation pool units less units used for known projects\n",
    "* Calcuate Remaining banked units less units used for known projects\n",
    "* Calcuate Remaining converted units less units used for known projects\n",
    "    \n",
    "3) Evaluate Vacant Buildable Lots with Multi-family Residential Allowed Use\t\n",
    "* Identify\tVacant buildable lots with allowed Multi-family use, calculate allowed density\n",
    "* Assign 15% of remaining local jurisdiction Residential Allocation pool units to available multi-family parcels within jurisidiction\n",
    "* Assign 35% of remaining Banked units to available multi-family parcels (use adjusted weighting from existing residential units?)\n",
    "* Assign 35% of remaining Converted units to available multi-family parcels (use adjusted weighting from existing residential units?)\n",
    "    \n",
    "4) Evaluate Vacant Buildable Lots with Single-family Residential Allowed Use\t\n",
    "* Identify\tVacant buildable lots with allowed Single-family use\n",
    "* Assign 70% of local jurisdiction Residential Allocation pool units to available single-family parcels within jurisidiction (if enough vacant parcels are available, otherwise build all available parcels)\n",
    "* Assign 50% of Banked units to available single-family parcels (use adjusted weighting from existing residential units?) (if enough vacant parcels are available, otherwise build all available parcels)\n",
    "* Assign 50% of Converted units to available single-family parcels (use adjusted weighting from existing residential units?) (if enough vacant parcels are available)\n",
    "    \n",
    "5) Evaluate Underbuilt parcels with Multi-family Residential Allowed Use\t\n",
    "* Identify\tUnderbuilt Residential lots with allowed Multi-family use\n",
    "* Assign 12% of local jurisdiction Residential Allocation pool units to underbuilt Multi-family parcels within jurisidiction\n",
    "* Assign 12% of banked residential units to underbuilt Multi-family parcels (use adjusted weighting from existing residential units?)\n",
    "* Assign 12% of converted residential units to underbuilt Multi-family parcels (use adjusted weighting from existing residential units?)\n",
    "    \n",
    "6) Evaluate Underbuilt parcels with Accessory Dwelling Uses Allowed (All California Parcels and NV Parcels Greater than 1 Acre)\t\n",
    "* Identify\tAccessory Dwelling Uses Allowed (All California Parcels and NV Parcels Greater than 1 Acre)\n",
    "* Assign remainder of local jurisdiction Residential Allocation pool units to ADUs\n",
    "* Assign remainder of banked units to ADUs (weighted higher in CA due to limitations)\n",
    "* Assign remainder of converted units to ADUs (weighted higher in CA due to limitations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pickle\n",
    "sdfParcels = from_pickle(parcel_pickle_part1)\n",
    "sdfParcels['FORECASTED_RESIDENTIAL_UNITS'] = 0\n",
    "# fill na 0 and cast ADJUSTED_RESIDENTIAL_UNITS to int \n",
    "sdfParcels['MAX_BUILDABLE_UNITS'] = sdfParcels['MAX_BUILDABLE_UNITS'].fillna(0).astype(int)\n",
    "sdfParcels.loc[sdfParcels['HOUSING_ZONING'] == 'SF_only', 'MAX_BUILDABLE_UNITS'] = 1\n",
    "# set FORECAST_REASON to a null string\n",
    "sdfParcels['FORECAST_REASON'] = ''\n",
    "# cast as type str\n",
    "sdfParcels['FORECAST_REASON'] = sdfParcels['FORECAST_REASON'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sort parcels so that development can be assigned randomly\n",
    "sdfParcels = sdfParcels.sample(frac=1).reset_index(drop=True)\n",
    "# Export index and apn to a csv file for future reference with a name that includes the date and time\n",
    "# This will allow us to recreate the same random order in the future\n",
    "sdfParcels[['APN']].to_csv(out_dir / f\"Parcel_Sort_Order_{pd.Timestamp.now().strftime('%Y%m%d%H%M%S')}.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_residential_units(df, condition, target_sum, reason):\n",
    "    # filter to parcels available for development\n",
    "    sdfAvailable = df.loc[eval(condition)]\n",
    "    running_sum = 0\n",
    "    rows_to_fill = []\n",
    "    # Loop through the rows and fill the 'new_column'\n",
    "    for idx, row in sdfAvailable.iterrows():\n",
    "        # Calculate the remaining amount that can be filled\n",
    "        remaining_amount = target_sum - running_sum\n",
    "        if row['MAX_BUILDABLE_UNITS'] <= remaining_amount:\n",
    "            # If the current row's value fits, add it to the column\n",
    "            df.loc[idx, 'FORECASTED_RESIDENTIAL_UNITS'] = row['MAX_BUILDABLE_UNITS']\n",
    "            running_sum += row['MAX_BUILDABLE_UNITS']\n",
    "            rows_to_fill.append(idx)\n",
    "        elif remaining_amount > 0:\n",
    "            # If it exceeds the remaining amount, fill with the remaining value\n",
    "            df.loc[idx, 'FORECASTED_RESIDENTIAL_UNITS'] = remaining_amount\n",
    "            running_sum += remaining_amount\n",
    "            rows_to_fill.append(idx)\n",
    "            break\n",
    "        else:\n",
    "            break\n",
    "    # reason for development\n",
    "    df.loc[rows_to_fill, 'FORECAST_REASON'] = reason\n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Conditional Statements for Forecasting Residential Unit Development ##\n",
    "# Within TRPA Boundary as condition for all\n",
    "# Parcel criteria for development of CSLT Bonus Units\n",
    "Base_Criteria = \"(df['FORECAST_REASON'] == '') &\"\n",
    "CSLT_Bonus_condition   = \"(df['FORECAST_REASON'] == '') & (df['JURISDICTION'] == 'CSLT') & (df['WITHIN_BONUSUNIT_BNDY'] == 'Yes') & (df['EXISTING_LANDUSE'] == 'Vacant') & (df['OWNERSHIP_TYPE'] == 'Private') & (df['IPES_SCORE'] > 0) & (df['RETIRED'] == 'No')\"\n",
    "# Parcel criteria for development of CLST General Unit Pool\n",
    "CSLT_General_Condition = \"(df['FORECAST_REASON'] == '') & (df['JURISDICTION'] == 'CSLT') & (df['EXISTING_LANDUSE'] == 'Vacant') & (df['OWNERSHIP_TYPE'] == 'Private') & (df['IPES_SCORE'] > 0) & (df['RETIRED'] == 'No')\"\n",
    "# Parcel criteria for development of EL General Unit Pool\n",
    "EL_General_Condition   = \"(df['FORECAST_REASON'] == '') & (df['JURISDICTION'] == 'EL') & (df['EXISTING_LANDUSE'] == 'Vacant') & (df['OWNERSHIP_TYPE'] == 'Private') & (df['IPES_SCORE'] > 0) & (df['RETIRED'] == 'No')\"\n",
    "# Parcel criteria for development of PL General Unit Pool\n",
    "PL_General_Condition   = \"(df['FORECAST_REASON'] == '') & (df['JURISDICTION'] == 'PL') & (df['EXISTING_LANDUSE'] == 'Vacant') & (df['OWNERSHIP_TYPE'] == 'Private') & (df['IPES_SCORE'] > 726) & (df['RETIRED'] == 'No')\"\n",
    "# Parcel criteria for development of PL Bonus Units\n",
    "PL_Bonus_Condition     = \"(df['FORECAST_REASON'] == '') & (df['JURISDICTION'] == 'PL') & (df['WITHIN_BONUSUNIT_BNDY'] == 'Yes') & (df['EXISTING_LANDUSE'] == 'Vacant') & (df['OWNERSHIP_TYPE'] == 'Private') & (df['IPES_SCORE'] > 726) & (df['RETIRED'] == 'No')\"\n",
    "# Parcel criteria for development of WA General Unit Pool\n",
    "WA_General_Condition   = \"(df['FORECAST_REASON'] == '') & (df['JURISDICTION'] == 'WA') & (df['EXISTING_LANDUSE'] == 'Vacant') & (df['OWNERSHIP_TYPE'] == 'Private') & (df['IPES_SCORE'] > 0) & (df['RETIRED'] == 'No')\"\n",
    "# Parcel criteria for development of WA Bonus Units\n",
    "WA_Bonus_Condition     = \"(df['FORECAST_REASON'] == '') & (df['JURISDICTION'] == 'WA') & (df['WITHIN_BONUSUNIT_BNDY'] == 'Yes') & (df['EXISTING_LANDUSE'] == 'Vacant') & (df['OWNERSHIP_TYPE'] == 'Private') & (df['IPES_SCORE'] > 0) & (df['RETIRED'] == 'No')\"\n",
    "# Parcel criteria for development of DG General Unit Pool\n",
    "DG_General_Condition   = \"(df['FORECAST_REASON'] == '') & (df['JURISDICTION'] == 'DG') & (df['EXISTING_LANDUSE'] == 'Vacant') & (df['OWNERSHIP_TYPE'] == 'Private') & (df['IPES_SCORE'] > 0) & (df['RETIRED'] == 'No')\"\n",
    "# Parcel criteria for development of DG Bonus Units\n",
    "DG_Bonus_Condition     = \"(df['FORECAST_REASON'] == '') & (df['JURISDICTION'] == 'DG') & (df['WITHIN_BONUSUNIT_BNDY'] == 'Yes') & (df['EXISTING_LANDUSE'] == 'Vacant') & (df['OWNERSHIP_TYPE'] == 'Private') & (df['IPES_SCORE'] > 0) & (df['RETIRED'] == 'No')\"\n",
    "# Parcel criteria for development of TRPA General Unit Pool\n",
    "TRPA_General_Condition = \"(df['FORECAST_REASON'] == '') & (df['EXISTING_LANDUSE'] == 'Vacant') & (df['OWNERSHIP_TYPE'] == 'Private') & (df['IPES_SCORE'] > 0) & (df['RETIRED'] == 'No')\"\n",
    "# Parcel criteria for development of TRPA Bonus Units\n",
    "TRPA_Bonus_Condition   = \"(df['FORECAST_REASON'] == '') & (df['WITHIN_BONUSUNIT_BNDY'] == 'Yes') & (df['EXISTING_LANDUSE'] == 'Vacant') & (df['OWNERSHIP_TYPE'] == 'Private') & (df['IPES_SCORE'] > 0) & (df['RETIRED'] == 'No')\"\n",
    "# Parcel criteria for ADU development\n",
    "ADU_Condition          = \"(df['FORECAST_REASON'] == '') & (df['ADU_ALLOWED'] == 'Yes') & (df['Residential_Units'] == 1) & (df['OWNERSHIP_TYPE'] == 'Private')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get lookup lists\n",
    "res_assigned_lookup = \"Lookup_Lists/forecast_residential_assigned_units.csv\"\n",
    "res_zoned_lookup    = \"Lookup_Lists/forecast_residential_zoned_units.csv\"\n",
    "# get zoned units lookups as data frames\n",
    "dfResZoned    = pd.read_csv(res_zoned_lookup)\n",
    "dfResAssigned = pd.read_csv(res_assigned_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join assigned lookup to parcels\n",
    "sdfAssignedParcels = dfResAssigned.merge(sdfParcels, how='left', on='APN')\n",
    "sdfAssignedParcels['UnitPoolType'] = 'General'\n",
    "sdfAssignedParcels.loc[sdfAssignedParcels['Source'] == 'Residential Bonus Unit', 'UnitPoolType'] = 'Bonus Unit'\n",
    "# If the first 3 characters of the Source are 'ADU', set the UnitPoolType to 'ADU'\n",
    "sdfAssignedParcels\n",
    "sdfAssignedParcels.loc[sdfAssignedParcels['Source'].str.startswith('ADU'), 'UnitPoolType'] = 'ADU'\n",
    "dfResAssignedParcelTotal = sdfAssignedParcels.groupby(['UnitPoolType', 'JURISDICTION'])['Unit Change'].sum().reset_index()\n",
    "# These are more than we have in dfResZoned. Should they come out of TRPA?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yearly development rate = future units / 28 years\n",
    "dfResZoned['Yearly_Development_Rate'] = dfResZoned['Future_Units'] / 28\n",
    "# Add Known Residential Allocations from 2023-2024\n",
    "# set residential units to assigned total \n",
    "sdfParcels['FORECASTED_RESIDENTIAL_UNITS']        = sdfParcels.APN.map(dict(zip(dfResAssigned.APN, dfResAssigned['Unit Change'])))\n",
    "# set forecast reason to assigned for all parcels in the assigned list\n",
    "sdfParcels.loc[sdfParcels['APN'].isin(dfResAssigned.APN), 'FORECAST_REASON'] = 'Assigned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Assigned Units from Zoned Units\n",
    "# Specific project are in chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with years anything assigned is 2035\n",
    "# Does not matter about jurisdiction\n",
    "# bring back random sort - should maybe be static? Date time?\n",
    "# Assign CTC asset lands based on lookup list. Just a isin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast CSLT Bonus Units\n",
    "target_sum = dfResZoned.loc[(dfResZoned.Jurisdiction == 'CSLT')&(dfResZoned.Unit_Pool == 'Bonus Unit'), 'Future_Units'].values[0]\n",
    "sdfParcels = forecast_residential_units(sdfParcels, CSLT_Bonus_condition, target_sum, 'CSLT Bonus Units Built')\n",
    "# # Forecast CSLT General Units\n",
    "target_sum = dfResZoned.loc[(dfResZoned.Jurisdiction == 'CSLT')&(dfResZoned.Unit_Pool == 'General'), 'Future_Units'].values[0]\n",
    "sdfParcels = forecast_residential_units(sdfParcels, CSLT_General_Condition, target_sum, 'CSLT General Units Built')\n",
    "# # Forecast EL General Units\n",
    "target_sum = dfResZoned.loc[(dfResZoned.Jurisdiction == 'EL')&(dfResZoned.Unit_Pool == 'General'), 'Future_Units'].values[0]\n",
    "sdfParcels = forecast_residential_units(sdfParcels, EL_General_Condition, target_sum, 'EL General Units Built')\n",
    "# # Forecast PL Bonus Units\n",
    "target_sum = dfResZoned.loc[(dfResZoned.Jurisdiction == 'PL')&(dfResZoned.Unit_Pool == 'Bonus Unit'), 'Future_Units'].values[0]\n",
    "sdfParcels = forecast_residential_units(sdfParcels, PL_Bonus_Condition, target_sum, 'PL Bonus Units Built')\n",
    "# # Forecast PL General Units\n",
    "target_sum = dfResZoned.loc[(dfResZoned.Jurisdiction == 'PL')&(dfResZoned.Unit_Pool == 'General'), 'Future_Units'].values[0]\n",
    "sdfParcels = forecast_residential_units(sdfParcels, PL_General_Condition, target_sum, 'PL General Units Built')\n",
    "# # Forecast WA Bonus Units\n",
    "target_sum = dfResZoned.loc[(dfResZoned.Jurisdiction == 'WA')&(dfResZoned.Unit_Pool == 'Bonus Unit'), 'Future_Units'].values[0]\n",
    "sdfParcels = forecast_residential_units(sdfParcels, WA_Bonus_Condition, target_sum, 'WA Bonus Units Built')\n",
    "# # Forecast WA General Units\n",
    "target_sum = dfResZoned.loc[(dfResZoned.Jurisdiction == 'WA')&(dfResZoned.Unit_Pool == 'General'), 'Future_Units'].values[0]\n",
    "sdfParcels = forecast_residential_units(sdfParcels, WA_General_Condition, target_sum, 'WA General Units Built')\n",
    "# # Forecast DG Bonus Units\n",
    "target_sum = dfResZoned.loc[(dfResZoned.Jurisdiction == 'DG')&(dfResZoned.Unit_Pool == 'Bonus Unit'), 'Future_Units'].values[0]\n",
    "sdfParcels = forecast_residential_units(sdfParcels, DG_Bonus_Condition, target_sum, 'DG Bonus Units Built')\n",
    "# # Forecast DG General Units\n",
    "target_sum = dfResZoned.loc[(dfResZoned.Jurisdiction == 'DG')&(dfResZoned.Unit_Pool == 'General'), 'Future_Units'].values[0]\n",
    "sdfParcels = forecast_residential_units(sdfParcels, DG_General_Condition, target_sum, 'DG General Units Built')\n",
    "# # Forecast TRPA Bonus Units\n",
    "target_sum = dfResZoned.loc[(dfResZoned.Jurisdiction == 'TRPA')&(dfResZoned.Unit_Pool == 'Bonus Unit'), 'Future_Units'].values[0]\n",
    "sdfParcels = forecast_residential_units(sdfParcels, TRPA_Bonus_Condition, target_sum, 'TRPA Bonus Units Built')\n",
    "# # Forecast TRPA General Units\n",
    "target_sum = dfResZoned.loc[(dfResZoned.Jurisdiction == 'TRPA')&(dfResZoned.Unit_Pool == 'General'), 'Future_Units'].values[0]\n",
    "sdfParcels = forecast_residential_units(sdfParcels, TRPA_General_Condition, target_sum, 'TRPA General Units Built')\n",
    "# forecast ADU Units\n",
    "target_sum = dfResZoned.loc[(dfResZoned.Jurisdiction == 'TRPA')&(dfResZoned.Unit_Pool == 'ADU'), 'Future_Units'].values[0]\n",
    "sdfParcels = forecast_residential_units(sdfParcels, ADU_Condition, target_sum, 'ADU Units Built')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TotalDevelopment = dfResZoned.Future_Units.sum()\n",
    "Development_2035 = (TotalDevelopment*.46).astype(int)\n",
    "sdfParcels['Development_Year']=None\n",
    "#Assining all development that's currently in the works as being cone by 2035\n",
    "sdfParcels.loc[sdfParcels['FORECAST_REASON'] == 'Assigned', 'Development_Year'] = 2035\n",
    "RemainingDevelopment_2035 = Development_2035 - sdfParcels.loc[sdfParcels['FORECAST_REASON'] == 'Assigned', 'FORECASTED_RESIDENTIAL_UNITS'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Development_2035_Condition = sdfParcels['FORECASTED_RESIDENTIAL_UNITS'].where(sdfParcels['FORECAST_REASON'] != 'Assigned').cumsum()\n",
    "sdfParcels.loc[Development_2035_Condition < RemainingDevelopment_2035, 'Development_Year'] = 2035\n",
    "sdfParcels.loc[(sdfParcels['FORECAST_REASON']!= '') & (sdfParcels['Development_Year'].isnull()), 'Development_Year'] = 2050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = sdfParcels.groupby('FORECAST_REASON').agg({'FORECASTED_RESIDENTIAL_UNITS':'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to pickle\n",
    "sdfParcels.to_pickle(parcel_pickle_part2)\n",
    "# to feature class\n",
    "sdfParcels.spatial.to_featureclass(Path(gdb)/'Parcel_Forecast')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tourist Accommodation Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lookup lists\n",
    "tau_assigned_lookup = \"Lookup_Lists/forecast_tourist_assigned_units.csv\"\n",
    "tau_zoned_lookup    = \"Lookup_Lists/forecast_tourist_zoned_units.csv\"\n",
    "# get zoned units lookups as data frames\n",
    "dfTouristZoned    = pd.read_csv(tau_zoned_lookup)\n",
    "dfTouristAssigned = pd.read_csv(tau_assigned_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdfParcels['FORECASTED_TAU_UNITS'] = 0\n",
    "# set tourist units to assigned total\n",
    "sdfParcels['FORECASTED_TAU_UNITS'] = sdfParcels.APN.map(dict(zip(dfTouristAssigned.APN, dfTouristAssigned['Unit Change'])))\n",
    "# set forecast reason to assigned for all parcels in the assigned list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commercial Floor Area Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lookup lists\n",
    "cfa_assigned_lookup = \"Lookup_Lists/forecast_tourist_assigned_units.csv\"\n",
    "cfa_zoned_lookup    = \"Lookup_Lists/forecast_tourist_zoned_units.csv\"\n",
    "# get zoned units lookups as data frames\n",
    "dfCommercialZoned    = pd.read_csv(cfa_zoned_lookup)\n",
    "dfCommercialAssigned = pd.read_csv(cfa_assigned_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# condition = commercial allowed use, in town center, existing land use is vacant or commercial, existing coverage is less than 30% of parcel sqft, and not retired, and private ownership\n",
    "comercial_condition = \"(df['FORECAST_REASON'] == '') & (df['COUNTY_LANDUSE_DESCRIPTION'] == 'Commercial') & (df['EXISTING_LANDUSE'].isin(['Vacant','Commercial'])) & (df['CommercialFloorArea_SqFt'] < 0.3*df['PARCEL_SQFT']) & (df['OWNERSHIP_TYPE'] == 'Private') & (df['RETIRED'] == 'No')\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

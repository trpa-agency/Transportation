{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RTP Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import os\n",
    "import arcpy\n",
    "from utils import *\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# pandas options\n",
    "pd.options.mode.copy_on_write = True\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = 999\n",
    "pd.options.display.max_rows    = 999\n",
    "\n",
    "# my workspace \n",
    "workspace = r\"C:\\Users\\mbindl\\Desktop\\Workspace.gdb\"\n",
    "# current working directory\n",
    "local_path = pathlib.Path().absolute()\n",
    "# set data path as a subfolder of the current working directory TravelDemandModel\\2022\\\n",
    "data_dir = local_path.parents[0] / 'data'\n",
    "# folder to save processed data\n",
    "out_dir  = local_path.parents[0] / 'data/processed_data'\n",
    "# workspace gdb for stuff that doesnt work in memory\n",
    "# gdb = os.path.join(local_path,'Workspace.gdb')\n",
    "gdb = workspace\n",
    "# set environement workspace to in memory \n",
    "arcpy.env.workspace = 'memory'\n",
    "# # clear memory workspace\n",
    "# arcpy.management.Delete('memory')\n",
    "\n",
    "# overwrite true\n",
    "arcpy.env.overwriteOutput = True\n",
    "# Set spatial reference to NAD 1983 UTM Zone 10N\n",
    "sr = arcpy.SpatialReference(26910)\n",
    "\n",
    "# get parcels from the database\n",
    "# network path to connection files\n",
    "filePath = \"F:/GIS/PARCELUPDATE/Workspace/\"\n",
    "# database file path \n",
    "sdeBase    = os.path.join(filePath, \"Vector.sde\")\n",
    "sdeCollect = os.path.join(filePath, \"Collection.sde\")\n",
    "sdeTabular = os.path.join(filePath, \"Tabular.sde\")\n",
    "sdeEdit    = os.path.join(filePath, \"Edit.sde\")\n",
    "\n",
    "# schema for the final output\n",
    "final_schema = ['APN', 'Residential_Units', 'TouristAccommodation_Units', 'CommercialFloorArea_SqFt',\n",
    "                'ZONING_ID', 'EXISTING_LANDUSE', 'COUNTY', 'JURISDICTION', 'OWNERSHIP_TYPE',\n",
    "                'IPES_SCORE', 'VHR', 'BLOCK_GROUP', 'TAZ', 'RETIRED', \n",
    "                'JURISDICTION', 'COUNTY', 'OWNERSHIP_TYPE','EXISTING_LANDUSE',  \n",
    "                'WITHIN_BONUS_UNIT_BNDRY', 'WITHIN_TRPA_BNDY',\n",
    "                'MAX_RESIDENTIAL_UNITS', 'MAX_COMMERCIAL_FLOOR_AREA', 'MAX_TAU_UNITS',\n",
    "                'PARCEL_ACRES', 'PARCEL_SQFT', 'SHAPE']\n",
    "\n",
    "# Pickle variables\n",
    "# part 1 - spatial join categories, occupancy rates, and parcels\n",
    "parcel_pickle_part1    = data_dir / 'parcel_pickle1.pkl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parcel development layer polygons\n",
    "parcel_db = Path(sdeEdit) / \"SDE.Parcel\\\\SDE.Parcel_History_Attributed\"\n",
    "# query 2022 rows\n",
    "sdf_units = pd.DataFrame.spatial.from_featureclass(parcel_db)\n",
    "sdf_units = sdf_units.loc[sdf_units['YEAR'] == 2022]\n",
    "sdf_units.spatial.sr = sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_units.CommercialFloorArea_SqFt.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfIPES     = pd.read_json(\"https://laketahoeinfo.org/WebServices/GetParcelIPESScores/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476\")\n",
    "dfRetired  = pd.read_json(\"https://www.laketahoeinfo.org/WebServices/GetAllParcels/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conn(db):\n",
    "    # Get database user and password from environment variables on machine running script\n",
    "    db_user             = os.environ.get('DB_USER')\n",
    "    db_password         = os.environ.get('DB_PASSWORD')\n",
    "\n",
    "    # driver is the ODBC driver for SQL Server\n",
    "    driver              = 'ODBC Driver 17 for SQL Server'\n",
    "    # server names are\n",
    "    sql_12              = 'sql12'\n",
    "    sql_14              = 'sql14'\n",
    "    # make it case insensitive\n",
    "    db = db.lower()\n",
    "    # make sql database connection with pyodbc\n",
    "    if db   == 'sde_tabular':\n",
    "        connection_string = f\"DRIVER={driver};SERVER={sql_12};DATABASE={db};UID={db_user};PWD={db_password}\"\n",
    "        connection_url = URL.create(\"mssql+pyodbc\", query={\"odbc_connect\": connection_string})\n",
    "        engine = create_engine(connection_url)\n",
    "    elif db == 'tahoebmpsde':\n",
    "        connection_string = f\"DRIVER={driver};SERVER={sql_14};DATABASE={db};UID={db_user};PWD={db_password}\"\n",
    "        connection_url = URL.create(\"mssql+pyodbc\", query={\"odbc_connect\": connection_string})\n",
    "        engine = create_engine(connection_url)\n",
    "    elif db == 'sde':\n",
    "        connection_string = f\"DRIVER={driver};SERVER={sql_12};DATABASE={db};UID={db_user};PWD={db_password}\"\n",
    "        connection_url = URL.create(\"mssql+pyodbc\", query={\"odbc_connect\": connection_string})\n",
    "        engine = create_engine(connection_url)\n",
    "    # else return None\n",
    "    else:\n",
    "        engine = None\n",
    "    # connection file to use in pd.read_sql\n",
    "    return engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zoning data\n",
    "engine = get_conn('sde')\n",
    "with engine.begin() as conn:\n",
    "    df_uses = pd.read_sql(\"SELECT * FROM sde.SDE.PermissibleUses\", conn)\n",
    "\n",
    "# function to get where Zoningin_ID Use_Type = Multi-Family and Density\n",
    "def get_mf_zones(df):\n",
    "    columns_to_keep = ['Zoning_ID', 'Use_Type', 'Density']\n",
    "    # filter Use_Type to Multiple Family Dwelling\n",
    "    df = df.loc[df['Use_Type'] == 'Multiple Family Dwelling']\n",
    "    return df[columns_to_keep]\n",
    "\n",
    "def get_sf_zones(df):\n",
    "    columns_to_keep = ['Zoning_ID', 'Use_Type', 'Density']\n",
    "    # filter Use_Type to Single Family Dwelling\n",
    "    df = df.loc[df['Use_Type'] == 'Single Family Dwelling']\n",
    "    return df[columns_to_keep]\n",
    "\n",
    "def get_sf_only_zones(df):\n",
    "    columns_to_keep = ['Zoning_ID', 'Use_Type', 'Density']\n",
    "    # filter Use_Type to Single Family Dwelling and not Multiple Family Dwelling\n",
    "    dfMF = get_mf_zones(df)\n",
    "    dfSF = get_sf_zones(df)\n",
    "    df = dfSF.loc[~dfSF['Zoning_ID'].isin(dfMF['Zoning_ID'])]\n",
    "    return df[columns_to_keep]\n",
    "\n",
    "\n",
    "dfMF = get_mf_zones(df_uses)\n",
    "dfMF.Use_Type.value_counts()\n",
    "\n",
    "dfSF = get_sf_zones(df_uses)\n",
    "dfSF.Use_Type.value_counts()\n",
    "\n",
    "\n",
    "dfSF_only = get_sf_only_zones(df_uses)\n",
    "dfSF_only.Use_Type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parcel development layer polygons\n",
    "parcel_db = Path(sdeEdit) / \"SDE.Parcel\\\\SDE.Parcel_History_Attributed\"\n",
    "# query 2022 rows\n",
    "sdf_units = pd.DataFrame.spatial.from_featureclass(parcel_db)\n",
    "sdf_units = sdf_units.loc[sdf_units['YEAR'] == 2022]\n",
    "sdf_units.spatial.sr = sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecast Formetted Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathCSV = data_dir / \"RegionalTransportationPlan/2023/data/Forecasts_Table1.csv\"\n",
    "print(pathCSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from great_tables import *\n",
    "import pandas as pd\n",
    "\n",
    "forecast = pd.read_csv(data_dir / \"Forecasts_Table1.csv\")\n",
    "# drop notes column \n",
    "forecast.drop(columns=['Notes'], inplace=True)\n",
    "# change column names\n",
    "forecast.rename(columns={'Change by 2050': 'Change(#)', 'Percent Change': 'Change(%)'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version of great tables\n",
    "gt.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from great_tables import GT, style, loc\n",
    "from great_tables.data import gtcars\n",
    "\n",
    "(\n",
    "    GT(\n",
    "        gtcars[[\"mfr\", \"model\", \"hp\", \"trq\", \"msrp\"]].head(5),\n",
    "        rowname_col=\"model\",\n",
    "        groupname_col=\"mfr\"\n",
    "    )\n",
    "    .tab_stubhead(label=\"car\")\n",
    "    .tab_style(\n",
    "        style=[\n",
    "            style.text(color=\"crimson\", weight=\"bold\"),\n",
    "            style.fill(color=\"lightgray\")\n",
    "        ],\n",
    "        locations=loc.row_groups()\n",
    "    )\n",
    "    .fmt_integer(columns=[\"hp\", \"trq\"])\n",
    "    .fmt_currency(columns=\"msrp\", decimals=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "GT(forecast).tab_header(title=\"Table 1. Forecast Data Summary\").tab_spanner(\n",
    "    label=\"\", columns=['Category', 'Variable','Base Year 2022',  'Forecast 2050', 'Change(#)', 'Change(%)']).tab_stub(\n",
    "        rowname_col='Variable', groupname_col='Category').tab_style(\n",
    "            style=style.fill(color=\"aliceblue\"), locations=loc.body()).save(\"forecast.jpeg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transit Stacked Bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data for transit ridership\n",
    "def get_data_transit():\n",
    "    url = \"https://www.laketahoeinfo.org/WebServices/GetTransitMonitoringData/CSV/e17aeb86-85e3-4260-83fd-a2b32501c476\"\n",
    "\n",
    "    dfTransit = pd.read_csv(url)\n",
    "    dfTransit['Month'] = pd.to_datetime(dfTransit['Month'])\n",
    "    dfTransit['Month'] = dfTransit['Month'].dt.strftime('%Y-%m')\n",
    "    # filter out rows where RouteType is not Paratransit, Commuter, or Seasonal Fixed\n",
    "    df = dfTransit.loc[~dfTransit['RouteType'].isin(['Paratransit', 'Commuter', 'Seasonal Fixed Route'])]\n",
    "    # df = dfTransit.loc[dfTransit['RouteType'] != 'Paratransit']\n",
    "\n",
    "    # replace transit operator values with abreviations\n",
    "    df['TransitOperator'] = df['TransitOperator'].replace(\n",
    "        ['Tahoe Transportation District',\n",
    "       'Tahoe Truckee Area Regional Transit',\n",
    "       'South Shore Transportation Management Association'],\n",
    "       [\"TTD\", \"TART\", \"SSTMA\"])\n",
    "    # route name = route type + transit operator\n",
    "    df['RouteName'] = df['RouteType'] + ' - ' + df['TransitOperator']\n",
    "    # group by RouteType, TransitOperator, and Month with sum of MonthlyRidership\n",
    "    df = df.groupby(['RouteName', 'Month'])['MonthlyRidership'].sum().reset_index()\n",
    "    # rename columns to Date, Name, Ridership\n",
    "    df.rename(columns={'Month':'Date', 'RouteName':'Name', 'MonthlyRidership':'Ridership'}, inplace=True)\n",
    "    # sort by Date\n",
    "    df = df.sort_values('Date')\n",
    "    return df\n",
    "\n",
    "# html/3.3.a_Transit_Ridership.html\n",
    "def plot_transit(df):\n",
    "    trendline(\n",
    "        df,\n",
    "        path_html=\"html/3.3.a_Transit_Ridership.html\",\n",
    "        div_id=\"3.3.a_Transit_Ridership\",\n",
    "        x=\"Date\",\n",
    "        y=\"Ridership\",\n",
    "        color=\"Name\",\n",
    "        color_sequence=[\"#023f64\", \"#7ebfb5\", \"#a48352\", \"#FC9A62\"],\n",
    "        sort=\"Date\",\n",
    "        orders=None,\n",
    "        x_title=\"Date\",\n",
    "        y_title=\"Ridership\",\n",
    "        markers=True,\n",
    "        hover_data=None,\n",
    "        tickvals=None,\n",
    "        ticktext=None,\n",
    "        tickangle=None,\n",
    "        hovermode=\"x unified\",\n",
    "        format=\",.0f\",\n",
    "        custom_data=[\"Name\"],\n",
    "        hovertemplate=\"<br>\".join([\n",
    "            \"<b>%{y:,.0f}</b> riders on\",\n",
    "            \"<i>%{customdata[0]}</i> lines\"\n",
    "                ])+\"<extra></extra>\",\n",
    "        additional_formatting = dict(\n",
    "                                    title = \"Transit Ridership\",\n",
    "                                    margin=dict(t=20),\n",
    "                                    legend=dict(\n",
    "                                        # title=\"Transit Ridership\",\n",
    "                                        orientation=\"h\",\n",
    "                                        entrywidth=120,\n",
    "                                        yanchor=\"bottom\",\n",
    "                                        y=1.05,\n",
    "                                        xanchor=\"right\",\n",
    "                                        x=0.95,\n",
    "                                    ))\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arcgispro-py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

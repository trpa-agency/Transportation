{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tahoe Regional Transportation Plan Forecasting\n",
    "> Data Engineering Tasks\n",
    "* Residential development forecasting for 2035 and 2050"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import os\n",
    "import arcpy\n",
    "from utils import *\n",
    "import numpy as np\n",
    "import pickle\n",
    "# external connection packages\n",
    "from sqlalchemy.engine import URL\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# pandas options\n",
    "pd.options.mode.copy_on_write = True\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = 999\n",
    "pd.options.display.max_rows    = 999\n",
    "\n",
    "# my workspace \n",
    "workspace = r\"C:\\Users\\mbindl\\Desktop\\Workspace.gdb\"\n",
    "# current working directory\n",
    "local_path = pathlib.Path().absolute()\n",
    "# set data path as a subfolder of the current working directory TravelDemandModel\\2022\\\n",
    "data_dir = local_path.parents[0] / 'data'\n",
    "# folder to save processed data\n",
    "out_dir  = local_path.parents[0] / 'data/processed_data'\n",
    "# workspace gdb for stuff that doesnt work in memory\n",
    "# gdb = os.path.join(local_path,'Workspace.gdb')\n",
    "gdb = workspace\n",
    "# set environement workspace to in memory \n",
    "arcpy.env.workspace = 'memory'\n",
    "# # clear memory workspace\n",
    "# arcpy.management.Delete('memory')\n",
    "\n",
    "# overwrite true\n",
    "arcpy.env.overwriteOutput = True\n",
    "# Set spatial reference to NAD 1983 UTM Zone 10N\n",
    "sr = arcpy.SpatialReference(26910)\n",
    "\n",
    "# get parcels from the database\n",
    "# network path to connection files\n",
    "filePath = \"F:/GIS/PARCELUPDATE/Workspace/\"\n",
    "# database file path \n",
    "sdeBase    = os.path.join(filePath, \"Vector.sde\")\n",
    "sdeCollect = os.path.join(filePath, \"Collection.sde\")\n",
    "sdeTabular = os.path.join(filePath, \"Tabular.sde\")\n",
    "sdeEdit    = os.path.join(filePath, \"Edit.sde\")\n",
    "\n",
    "# Pickle variables\n",
    "# part 1 - spatial joins and new categorical fields\n",
    "parcel_pickle_part1    = data_dir / 'parcel_pickle1.pkl'\n",
    "# part 2 - forecasting applied\n",
    "parcel_pickle_part2    = data_dir / 'parcel_pickle2.pkl'\n",
    "\n",
    "# columsn to list\n",
    "initial_columns = [ 'APN',\n",
    "                    'APO_ADDRESS',\n",
    "                    'Residential_Units',\n",
    "                    'TouristAccommodation_Units',\n",
    "                    'CommercialFloorArea_SqFt',\n",
    "                    'YEAR',\n",
    "                    'JURISDICTION',\n",
    "                    'COUNTY',\n",
    "                    'OWNERSHIP_TYPE',\n",
    "                    'COUNTY_LANDUSE_DESCRIPTION',\n",
    "                    'EXISTING_LANDUSE',\n",
    "                    'REGIONAL_LANDUSE',\n",
    "                    'YEAR_BUILT',\n",
    "                    'PLAN_ID',\n",
    "                    'PLAN_NAME',\n",
    "                    'ZONING_ID',\n",
    "                    'ZONING_DESCRIPTION',\n",
    "                    'TOWN_CENTER',\n",
    "                    'LOCATION_TO_TOWNCENTER',\n",
    "                    'TAZ',\n",
    "                    'PARCEL_ACRES',\n",
    "                    'PARCEL_SQFT',\n",
    "                    'WITHIN_BONUSUNIT_BNDY',\n",
    "                    'WITHIN_TRPA_BNDY',\n",
    "                    'SHAPE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conn(db):\n",
    "    # Get database user and password from environment variables on machine running script\n",
    "    db_user             = os.environ.get('DB_USER')\n",
    "    db_password         = os.environ.get('DB_PASSWORD')\n",
    "    # driver is the ODBC driver for SQL Server\n",
    "    driver              = 'ODBC Driver 17 for SQL Server'\n",
    "    # server names are\n",
    "    sql_12              = 'sql12'\n",
    "    sql_14              = 'sql14'\n",
    "    # make it case insensitive\n",
    "    db = db.lower()\n",
    "    # make sql database connection with pyodbc\n",
    "    if db   == 'sde_tabular':\n",
    "        connection_string = f\"DRIVER={driver};SERVER={sql_12};DATABASE={db};UID={db_user};PWD={db_password}\"\n",
    "        connection_url = URL.create(\"mssql+pyodbc\", query={\"odbc_connect\": connection_string})\n",
    "        engine = create_engine(connection_url)\n",
    "    elif db == 'tahoebmpsde':\n",
    "        connection_string = f\"DRIVER={driver};SERVER={sql_14};DATABASE={db};UID={db_user};PWD={db_password}\"\n",
    "        connection_url = URL.create(\"mssql+pyodbc\", query={\"odbc_connect\": connection_string})\n",
    "        engine = create_engine(connection_url)\n",
    "    elif db == 'sde':\n",
    "        connection_string = f\"DRIVER={driver};SERVER={sql_12};DATABASE={db};UID={db_user};PWD={db_password}\"\n",
    "        connection_url = URL.create(\"mssql+pyodbc\", query={\"odbc_connect\": connection_string})\n",
    "        engine = create_engine(connection_url)\n",
    "    # else return None\n",
    "    else:\n",
    "        engine = None\n",
    "    # connection file to use in pd.read_sql\n",
    "    return engine\n",
    "\n",
    "# save to pickle\n",
    "def to_pickle(data, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f'{filename} pickled')\n",
    "\n",
    "# save to pickle and feature class\n",
    "def to_pickle_fc(data, filename):\n",
    "    data.spatial.to_featureclass(filename)\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f'{filename} pickled and saved as feature class')\n",
    "\n",
    "# get a pickled file as a dataframe\n",
    "def from_pickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    print(f'{filename} unpickled')\n",
    "    return data\n",
    "def get_commercial_zones(df):\n",
    "    columns_to_keep = ['Zoning_ID', 'Category', 'Density']\n",
    "    # filter Use_Type to Multiple Family Dwelling\n",
    "    df = df.loc[df['Category'] == 'Commercial']\n",
    "    return df[columns_to_keep]\n",
    "\n",
    "def get_tourist_zones(df):\n",
    "    columns_to_keep = ['Zoning_ID', 'Category', 'Density']\n",
    "    # filter Use_Type to Multiple Family Dwelling\n",
    "    df = df.loc[df['Category'] == 'Tourist Accommodation']\n",
    "    return df[columns_to_keep]\n",
    "\n",
    "# function to get where Zoningin_ID Use_Type = Multi-Family and Density\n",
    "def get_mf_zones(df):\n",
    "    columns_to_keep = ['Zoning_ID', 'Use_Type', 'Density']\n",
    "    # filter Use_Type to Multiple Family Dwelling\n",
    "    df = df.loc[df['Use_Type'] == 'Multiple Family Dwelling']\n",
    "    return df[columns_to_keep]\n",
    "\n",
    "# function to get where Zoningin_ID Use_Type = Multi-Family and Density\n",
    "def get_sf_zones(df):\n",
    "    columns_to_keep = ['Zoning_ID', 'Use_Type', 'Density']\n",
    "    # filter Use_Type to Multiple Family Dwelling\n",
    "    df = df.loc[df['Use_Type'] == 'Single Family Dwelling']\n",
    "    return df[columns_to_keep]\n",
    "\n",
    "def get_mf_only_zones(df):\n",
    "    columns_to_keep = ['Zoning_ID', 'Use_Type', 'Density']\n",
    "    # filter Use_Type to Single Family Dwelling and not Multiple Family Dwelling\n",
    "    dfMF = get_mf_zones(df)\n",
    "    dfSF = get_sf_zones(df)\n",
    "    # get Zoning_ID that are in both dataframes\n",
    "    df = dfMF.loc[~dfMF['Zoning_ID'].isin(dfSF['Zoning_ID'])]\n",
    "    return df[columns_to_keep]\n",
    "\n",
    "def get_sf_only_zones(df):\n",
    "    columns_to_keep = ['Zoning_ID', 'Use_Type', 'Density']\n",
    "    # filter Use_Type to Single Family Dwelling and not Multiple Family Dwelling\n",
    "    dfMF = get_mf_zones(df)\n",
    "    dfSF = get_sf_zones(df)\n",
    "    df = dfSF.loc[~dfSF['Zoning_ID'].isin(dfMF['Zoning_ID'])]\n",
    "    return df[columns_to_keep]\n",
    "\n",
    "def get_sf_mf_zones(df):\n",
    "    columns_to_keep = ['Zoning_ID', 'Use_Type', 'Density']\n",
    "    # get SF and MF zones\n",
    "    dfSF = get_sf_zones(df)\n",
    "    dfMF = get_mf_zones(df)\n",
    "    # add the two dataframes together\n",
    "    df = pd.concat([dfSF, dfMF])\n",
    "    # only keep duplicate Zoning_ID\n",
    "    df = df[df.duplicated(subset=['Zoning_ID'], keep=False)]\n",
    "    return df[columns_to_keep]\n",
    "\n",
    "def get_recieving_zones(df):\n",
    "    columns_to_keep = ['Zoning_ID', 'SPECIAL_DESIGNATION']\n",
    "    # filter transfer recieving\n",
    "    df = df.loc[df['SPECIAL_DESIGNATION'] == 'Receive']\n",
    "    return df[columns_to_keep]\n",
    "\n",
    "def get_sending_zones(df):\n",
    "    columns_to_keep = ['Zoning_ID', 'SPECIAL_DESIGNATION']\n",
    "    df = df.loc[df['SPECIAL_DESIGNATION'] == 'Transfer']\n",
    "    return df[columns_to_keep]\n",
    "\n",
    "def forecast_residential_units(df, condition, target_sum, reason):\n",
    "    # filter to parcels available for development\n",
    "    sdfAvailable = df.loc[eval(condition)]\n",
    "    running_sum = 0\n",
    "    rows_to_fill = []\n",
    "    # Loop through the rows and fill the 'new_column'\n",
    "    for idx, row in sdfAvailable.iterrows():\n",
    "        # Calculate the remaining amount that can be filled\n",
    "        remaining_amount = target_sum - running_sum\n",
    "        if row['MAX_UNITS'] <= remaining_amount:\n",
    "            # If the current row's value fits, add it to the column\n",
    "            df.loc[idx, 'FORECASTED_RESIDENTIAL_UNITS'] = row['MAX_UNITS']\n",
    "            running_sum += row['MAX_UNITS']\n",
    "            if row['MAX_UNITS'] > 0:\n",
    "                rows_to_fill.append(idx)\n",
    "        elif remaining_amount > 0:\n",
    "            # If it exceeds the remaining amount, fill with the remaining value\n",
    "            df.loc[idx, 'FORECASTED_RESIDENTIAL_UNITS'] = remaining_amount\n",
    "            running_sum += remaining_amount\n",
    "            if row['MAX_UNITS'] > 0:\n",
    "                rows_to_fill.append(idx)\n",
    "            break\n",
    "        else:\n",
    "            break\n",
    "    # reason for development\n",
    "    df.loc[rows_to_fill, 'FORECAST_REASON'] = reason\n",
    "    df_summary = pd.DataFrame({'Reason': [reason], 'Parcels_Available':[len(sdfAvailable)], 'Parcels_Used':[len(rows_to_fill)],\n",
    "                                'Total_Forecasted_Units': [running_sum], 'Total_Remaining_Units': [target_sum - running_sum]})   \n",
    "    return df, df_summary  \n",
    "\n",
    "def forecast_residential_units_infill(df, condition, target_sum, reason):\n",
    "    # filter to parcels available for development\n",
    "    sdfAvailable = df.loc[eval(condition)]\n",
    "    running_sum = 0\n",
    "    rows_to_fill = []\n",
    "    # Loop through the rows and fill the 'new_column'\n",
    "    for idx, row in sdfAvailable.iterrows():\n",
    "        # Calculate the remaining amount that can be filled\n",
    "        remaining_amount = target_sum - running_sum\n",
    "        if row['POTENTIAL_UNITS'] <= remaining_amount:\n",
    "            # If the current row's value fits, add it to the column\n",
    "            df.loc[idx, 'FORECASTED_RESIDENTIAL_UNITS'] = row['POTENTIAL_UNITS']\n",
    "            running_sum += row['POTENTIAL_UNITS']\n",
    "            if row['POTENTIAL_UNITS'] > 0:\n",
    "                rows_to_fill.append(idx)\n",
    "        elif remaining_amount > 0:\n",
    "            # If it exceeds the remaining amount, fill with the remaining value\n",
    "            df.loc[idx, 'FORECASTED_RESIDENTIAL_UNITS'] = remaining_amount\n",
    "            running_sum += remaining_amount\n",
    "            if row['POTENTIAL_UNITS'] > 0:\n",
    "                rows_to_fill.append(idx)\n",
    "            break\n",
    "        else:\n",
    "            break\n",
    "    # reason for development\n",
    "    df.loc[rows_to_fill, 'FORECAST_REASON'] = reason\n",
    "    df_summary = pd.DataFrame({'Reason': [reason], 'Parcels_Available':[len(sdfAvailable)], 'Parcels_Used':[len(rows_to_fill)],\n",
    "                                'Total_Forecasted_Units': [running_sum], 'Total_Remaining_Units': [target_sum - running_sum]})   \n",
    "    return df, df_summary\n",
    "\n",
    "def get_target_sum(df, Jurisdiction, Unit_Pool, zoning_type):\n",
    "    if zoning_type == 'MF':\n",
    "        return df.loc[(df['Jurisdiction'] == Jurisdiction) & (df['Unit_Pool'] == Unit_Pool), 'Future_Units_Adjusted_MF'].values[0]\n",
    "    elif zoning_type == 'SF':\n",
    "        return df.loc[(df['Jurisdiction'] == Jurisdiction) & (df['Unit_Pool'] == Unit_Pool), 'Future_Units_Adjusted_SF'].values[0]\n",
    "    elif zoning_type == 'Infill':\n",
    "        return df.loc[(df['Jurisdiction'] == Jurisdiction) & (df['Unit_Pool'] == Unit_Pool), 'Future_Units_Adjusted_Infill'].values[0]\n",
    "    return df.loc[(df['Jurisdiction'] == Jurisdiction) & (df['Unit_Pool'] == Unit_Pool), 'Future_Units_Adjusted'].values[0]\n",
    "\n",
    "# funtion to adust the forecasted residential units vs the target sum in the unit pool\n",
    "def adjust_pools(df, dfPool):\n",
    "    dfPool_Deductions = df.groupby(['JURISDICTION','FORECAST_REASON'])['FORECASTED_RESIDENTIAL_UNITS'].sum()\n",
    "    dfPool_Deductions = dfPool_Deductions.reset_index()\n",
    "    # unit pool field\n",
    "    dfPool_Deductions['Unit_Pool'] = 'NA'\n",
    "    # set unit pool to Juirsidiction + Forecast Reason\n",
    "    dfPool_Deductions['Unit_Pool'] = dfPool_Deductions['JURISDICTION'] + ' ' + dfPool_Deductions['FORECAST_REASON']\n",
    "    # merge the two dataframes\n",
    "    dfPool = pd.merge(dfPool, dfPool_Deductions, on='Unit_Pool', how='left')\n",
    "    # fill NaN with 0\n",
    "    dfPool['FORECASTED_RESIDENTIAL_UNITS'] = dfPool['FORECASTED_RESIDENTIAL_UNITS'].fillna(0)\n",
    "    # adjust the forecasted units\n",
    "    dfPool['Future_Units_Adjusted']    = dfPool['Future_Units_Adjusted'] - dfPool['FORECASTED_RESIDENTIAL_UNITS']\n",
    "    # adjust the forecasted units\n",
    "    dfPool['Future_Units_Adjusted_MF'] = dfPool['Future_Units_Adjusted_MF'] - dfPool['FORECASTED_RESIDENTIAL_UNITS']\n",
    "    # adjust the forecasted units\n",
    "    dfPool['Future_Units_Adjusted_SF'] = dfPool['Future_Units_Adjusted_SF'] - dfPool['FORECASTED_RESIDENTIAL_UNITS']\n",
    "    # adjust the forecasted units\n",
    "    dfPool['Future_Units_Adjusted_Infill'] = dfPool['Future_Units_Adjusted_Infill'] - dfPool['FORECASTED_RESIDENTIAL_UNITS']\n",
    "    # return the dataframe\n",
    "    return dfPool_Deductions, dfPool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parcel development layer polygons\n",
    "parcel_db = Path(sdeEdit) / \"SDE.Parcel\\\\SDE.Parcel_History_Attributed\"\n",
    "# query 2022 rows\n",
    "sdf_units = pd.DataFrame.spatial.from_featureclass(parcel_db)\n",
    "sdf_units = sdf_units.loc[sdf_units['YEAR'] == 2022]\n",
    "sdf_units.spatial.sr = sr\n",
    "\n",
    "# # get parcel level data from Collection SDE\n",
    "# vhr feature layer polygons \n",
    "vhr_db = Path(sdeCollect) / \"SDE.Parcel\\\\SDE.Parcel_VHR\"\n",
    "sdf_vhr = pd.DataFrame.spatial.from_featureclass(vhr_db)\n",
    "sdf_vhr.spatial.sr = sr\n",
    "# filter vhr layer to active status\n",
    "sdf_vhr = sdf_vhr.loc[sdf_vhr['Status'] == 'Active']\n",
    "\n",
    "# TAZ feature layer polygons\n",
    "taz_db = Path(sdeBase) / \"SDE.Transportation\\\\SDE.Transportation_Analysis_Zone\"\n",
    "# get as spatial dataframe\n",
    "sdf_taz = pd.DataFrame.spatial.from_featureclass(taz_db)\n",
    "# set spatial reference to NAD 1983 UTM Zone 10N\n",
    "sdf_taz.spatial.sr = sr\n",
    "\n",
    "# censuse feature class\n",
    "census_fc    = Path(sdeBase) / \"SDE.Census\\\\SDE.Tahoe_Census_Geography\"\n",
    "# bouns unit boundary feature class\n",
    "bonus_unit_fc = Path(sdeBase) / \"SDE.Planning\\SDE.Bonus_unit_boundary\"\n",
    "\n",
    "# disable Z values on block group feature layer\n",
    "with arcpy.EnvManager(outputZFlag=\"Disabled\"):    \n",
    "    arcpy.conversion.FeatureClassToGeodatabase(\n",
    "        Input_Features=\"F:\\GIS\\DB_CONNECT\\Vector.sde\\SDE.Census\\SDE.Tahoe_Census_Geography\",\n",
    "        Output_Geodatabase=r\"C:\\Users\\mbindl\\Desktop\\Workspace.gdb\"\n",
    "    )\n",
    "# disable Z values on block group feature layer\n",
    "with arcpy.EnvManager(outputZFlag=\"Disabled\"):    \n",
    "    arcpy.conversion.FeatureClassToGeodatabase(\n",
    "        Input_Features=\"F:\\GIS\\DB_CONNECT\\Vector.sde\\SDE.Planning\\SDE.Bonus_unit_boundary\",\n",
    "        Output_Geodatabase=r\"C:\\Users\\mbindl\\Desktop\\Workspace.gdb\"\n",
    "    )\n",
    "\n",
    "# block group feature layer polygons with no Z\n",
    "sdf_block = pd.DataFrame.spatial.from_featureclass(Path(gdb) / 'Tahoe_Census_Geography')\n",
    "sdf_block = sdf_block.loc[(sdf_block['YEAR'] == 2020) & (sdf_block['GEOGRAPHY'] == 'Block Group')]\n",
    "sdf_block.spatial.sr = sr\n",
    "\n",
    "# bonus unit boundary wihtout Z\n",
    "sdf_bonus = pd.DataFrame.spatial.from_featureclass(Path(gdb) / 'Bonus_unit_boundary')\n",
    "sdf_bonus.spatial.sr = sr\n",
    "\n",
    "# get parcel level data from LTinfo\n",
    "dfIPES       = pd.read_json(\"https://www.laketahoeinfo.org/WebServices/GetParcelIPESScores/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476\")\n",
    "dfLCV_LTinfo = pd.read_json('https://www.laketahoeinfo.org/WebServices/GetParcelsByLandCapability/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476')\n",
    "dfRetired    = pd.read_json(\"https://www.laketahoeinfo.org/WebServices/GetAllParcels/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476\")\n",
    "dfBankedDev  = pd.read_json('https://www.laketahoeinfo.org/WebServices/GetBankedDevelopmentRights/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476')\n",
    "dfTransacted = pd.read_json('https://www.laketahoeinfo.org/WebServices/GetTransactedAndBankedDevelopmentRights/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476')\n",
    "dfAllParcels = pd.read_json('https://www.laketahoeinfo.org/WebServices/GetAllParcels/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476')\n",
    "\n",
    "# get use tables \n",
    "# zoning data\n",
    "sde_engine = get_conn('sde')\n",
    "with sde_engine.begin() as conn:\n",
    "    df_uses    = pd.read_sql(\"SELECT * FROM sde.SDE.PermissibleUses\", conn)\n",
    "    df_special = pd.read_sql(\"SELECT * FROM sde.SDE.Special_Designation\", conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parcel Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial join to get TAZ\n",
    "arcpy.SpatialJoin_analysis(sdf_units, sdf_taz, \"Existing_Development_TAZ\", \n",
    "                           \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", \"\", \"HAVE_THEIR_CENTER_IN\")\n",
    "# spatial join to get Block Group\n",
    "arcpy.SpatialJoin_analysis(sdf_units, sdf_block, \"Existing_Development_BlockGroup\", \n",
    "                           \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", \"\", \"HAVE_THEIR_CENTER_IN\")\n",
    "# spatial join of Bonus Unit Boundary\n",
    "arcpy.SpatialJoin_analysis(sdf_units, sdf_bonus, \"Existing_Development_BonusUnitBoundary\",\n",
    "                            \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", \"\", \"INTERSECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial dataframe with only initial columns\n",
    "sdfParcels = sdf_units[initial_columns]\n",
    "\n",
    "# get results of spatial joins as spatial dataframes\n",
    "sdf_units_taz   = pd.DataFrame.spatial.from_featureclass(\"Existing_Development_TAZ\", sr=sr)  \n",
    "sdf_units_block = pd.DataFrame.spatial.from_featureclass(\"Existing_Development_BlockGroup\", sr=sr)\n",
    "sdf_units_bonus = pd.DataFrame.spatial.from_featureclass(\"Existing_Development_BonusUnitBoundary\", sr=sr)\n",
    "# cast to string\n",
    "sdf_units_bonus['WITHIN_BONUSUNIT_BNDY'] = sdf_units_bonus['WITHIN_BONUSUNIT_BNDY'].astype(str)\n",
    "sdf_units_bonus['WITHIN_BONUSUNIT_BNDY'] = 'No'\n",
    "# if Id is not NA then within bonus unit boundary = yes, else\n",
    "sdf_units_bonus.loc[sdf_units_bonus['Id'].notna(), 'WITHIN_BONUSUNIT_BNDY'] = 'Yes'\n",
    "\n",
    "# map dictionary to sdf_units dataframe to fill in TAZ and Block Group fields\n",
    "sdfParcels['TAZ']                   = sdfParcels.APN.map(dict(zip(sdf_units_taz.APN,   sdf_units_taz.TAZ_1)))\n",
    "sdfParcels['BLOCK_GROUP']           = sdfParcels.APN.map(dict(zip(sdf_units_block.APN, sdf_units_block.TRPAID)))\n",
    "# map IPES score to parcels\n",
    "sdfParcels['IPES_SCORE']            = sdfParcels['APN'].map(dict(zip(dfIPES.APN, dfIPES.IPESScore)))\n",
    "sdfParcels['IPES_SCORE_TYPE']       = sdfParcels['APN'].map(dict(zip(dfIPES.APN, dfIPES.IPESScoreType)))\n",
    "# retired parcels\n",
    "sdfParcels['RETIRED']               = sdfParcels['APN'].map(dict(zip(dfAllParcels.APN, dfAllParcels.RetiredFromDevelopment)))\n",
    "sdfParcels['WITHIN_BONUSUNIT_BNDY'] = sdfParcels['APN'].map(dict(zip(sdf_units_bonus.APN, sdf_units_bonus.WITHIN_BONUSUNIT_BNDY)))\n",
    "# define housnig zoning and density\n",
    "sdfParcels['HOUSING_ZONING']          = 'NA'\n",
    "sdfParcels['COMMERCIAL_ALLOWED']      = 'No'\n",
    "sdfParcels['TOURIST_ALLOWED']         = 'No'\n",
    "\n",
    "# if the zoning id is in the list of multiple family zones then set to MF\n",
    "sdfParcels.loc[sdfParcels['ZONING_ID'].isin(get_sf_mf_zones(df_uses)['Zoning_ID']), 'HOUSING_ZONING'] = 'SF/MF'\n",
    "# if the zoning id is in the list of single family zones and not in the multiple family zones then set to SF only\n",
    "sdfParcels.loc[sdfParcels['ZONING_ID'].isin(get_sf_only_zones(df_uses)['Zoning_ID']), 'HOUSING_ZONING'] = 'SF_only'\n",
    "# if the zoning id is in the list of multiple family zones and not in the single family zones then set to MF only\n",
    "sdfParcels.loc[sdfParcels['ZONING_ID'].isin(get_mf_only_zones(df_uses)['Zoning_ID']), 'HOUSING_ZONING'] = 'MF_only'\n",
    "# if the zoning id is in the list of commercial zones then set to Commercial\n",
    "sdfParcels.loc[sdfParcels['ZONING_ID'].isin(get_commercial_zones(df_uses)['Zoning_ID']), 'COMMERCIAL_ALLOWED'] = 'Yes'\n",
    "# if the zoning id is in the list of tourist zones then set to Tourist Accommodation\n",
    "sdfParcels.loc[sdfParcels['ZONING_ID'].isin(get_tourist_zones(df_uses)['Zoning_ID']), 'TOURIST_ALLOWED'] = 'Yes'\n",
    "\n",
    "# if COUNTY is in EL or PL and SF allowed then set ADU_ALLOWED to yes or if COUNTY is in WA, DG, or CC and parcel acres is greater than 1 and SF allowed then set ADU_ALLOWED to yes\n",
    "sdfParcels['ADU_ALLOWED'] = 'No'\n",
    "sdfParcels.loc[(sdfParcels['COUNTY'].isin(['EL','PL'])) & (~sdfParcels['HOUSING_ZONING'].isin(['MF_only', 'NA'])), 'ADU_ALLOWED'] = 'Yes'\n",
    "sdfParcels.loc[(sdfParcels['COUNTY'].isin(['WA','DG','CC'])) & (sdfParcels['PARCEL_ACRES']>=1) &(~sdfParcels['HOUSING_ZONING'].isin(['MF_only', 'NA'])), 'ADU_ALLOWED'] = 'Yes'\n",
    "\n",
    "# get density for MF and MF only zones, max residential units, and adjusted residential units\n",
    "dfMF = get_mf_zones(df_uses)\n",
    "sdfParcels['DENSITY']                    = sdfParcels['ZONING_ID'].map(dict(zip(dfMF.Zoning_ID, dfMF.Density)))\n",
    "sdfParcels['MAX_RESIDENTIAL_UNITS']      = sdfParcels['PARCEL_ACRES'] * sdfParcels['DENSITY']\n",
    "sdfParcels['MAX_UNITS']                  = sdfParcels['MAX_RESIDENTIAL_UNITS']*0.6\n",
    "sdfParcels['MAX_UNITS']                  = sdfParcels['MAX_UNITS'].fillna(0).astype(int)\n",
    "\n",
    "# set SF only zones to 1 max unit\n",
    "sdfParcels.loc[sdfParcels['HOUSING_ZONING'] == 'SF_only', 'MAX_UNITS'] = 1\n",
    "\n",
    "# set field for underbuilt evaluation\n",
    "sdfParcels['POTENTIAL_UNITS'] = 0\n",
    "sdfParcels['POTENTIAL_UNITS'] = sdfParcels['MAX_UNITS'] - sdfParcels['Residential_Units']\n",
    "# calculate parcels with the greatest buildable potential  filter to the top 10% of parcels\n",
    "# what value is in the top 10% of the potential buildable units\n",
    "top_10_threshold = sdfParcels.POTENTIAL_UNITS.quantile(0.9)\n",
    "# filter out rows where POTENTIAL_BUILDABLE_UNITS is NaN\n",
    "sdfParcels['TOP_TEN_POTENTIAL_UNITS'] = sdfParcels.apply(lambda x: 'Yes' if x['POTENTIAL_UNITS'] >= top_10_threshold else 'No', axis=1)\n",
    "\n",
    "# set FORECASTED_RESIDENTIAL_UNITS to 0\n",
    "sdfParcels['FORECASTED_RESIDENTIAL_UNITS']     = 0\n",
    "# set FORECAST_COMMERCIAN_UNITS to 0\n",
    "sdfParcels['FORECASTED_COMMERCIAL_SQFT']       = 0\n",
    "# set FORECAST_TOURIST_UNITS to 0\n",
    "sdfParcels['FORECASTED_TOURIST_UNITS']         = 0\n",
    "# set FORECAST_REASON to na\n",
    "sdfParcels['FORECAST_REASON']                  = None\n",
    "# FORECASTED_OCCUPANCY_RATE as a float field\n",
    "sdfParcels['FORECASTED_RES_OCCUPANCY_RATE']    = 0.0\n",
    "\n",
    "# export to pickle\n",
    "sdfParcels.to_pickle(parcel_pickle_part1)\n",
    "# to feature class\n",
    "sdfParcels.spatial.to_featureclass(Path(gdb)/'Parcel_Base_2022')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Year Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residential Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Assign Known Projects\n",
    "* Assign using Lookup_Lists\\forecast_residential_assigned_units.csv: \n",
    "    * Known Residential Allocations from 2023-2024, \n",
    "    * Known Residential Bonus Units from permitted projects, \n",
    "    * applications in review, and \n",
    "    * RBU reservations in LT Info\n",
    "    * Known Accessory Dwelling permits not completed\n",
    "    * Remove Known Banking projects that removed units in 2023-2024\n",
    "* Calcuate Remaining local jurisdiction pool units less units used for known projects\n",
    "    \n",
    "2) Assign Residential Bonus Units within Bonus Unit Boundary\n",
    "* Full build out of CTC Asset Lands using jurisdiction bonus unit pools\n",
    "* Identify vacant buildable lots within Bonus Unit boundary\n",
    "* Assign remaining jurisdiction pool units to available parcels within jurisidiction\n",
    "\n",
    "3) Assigne remaining Jurisdiction Residential Bonus and General Units\n",
    "* Identify\tVacant buildable lots with allowed Multi-family use, calculate allowed density\n",
    "* Assign 15% of remaining local jurisdiction Residential Allocation pool units to available multi-family parcels within jurisidiction\n",
    "* Assign 35% of remaining Banked units to available multi-family parcels (use adjusted weighting from existing residential units?)\n",
    "* Assign 35% of remaining Converted units to available multi-family parcels (use adjusted weighting from existing residential units?)\n",
    "    \n",
    "4) Assign remaining TRPA pool units to available parcels throughout region (use adjusted weighting from existing residential units?)\n",
    "* Evaluate Vacant Buildable Lots with Single-family Residential Allowed Use\t\n",
    "* Identify\tVacant buildable lots with allowed Single-family use\n",
    "* Identify\tAccessory Dwelling Uses Allowed (All California Parcels and NV Parcels Greater than 1 Acre)\n",
    "* Evaluate Underbuilt parcels with Multi-family Residential Allowed Use\t\n",
    "* Identify\tUnderbuilt Residential lots with allowed Multi-family use\n",
    "* Evaluate Underbuilt parcels with Accessory Dwelling Uses Allowed (All California Parcels and NV Parcels Greater than 1 Acre)\t\n",
    "* Identify parcels that are in Town Centers or within a quarter mile and are top x% of underbuilt parcels. \n",
    "* Underbuilt = exclude existing condo and common area land uses. sf mf or mixed use\n",
    "* ADU potential = exclude existing condo or common area land uses, and wait to use any NV parcels>acre. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Get the Parcel Base Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pickle 1 as a spatially enabled dataframe - spatial joins, foreign keys, and new categorical fields added already\n",
    "sdfParcels = from_pickle(parcel_pickle_part1)\n",
    "# Randomly sort parcels so that development can be assigned randomly\n",
    "sdfParcels = sdfParcels.sample(frac=1).reset_index(drop=True)\n",
    "# Export index and apn to a csv file for future reference with a name that includes the date and time\n",
    "# This will allow us to recreate the same random order in the future\n",
    "sdfParcels[['APN']].to_csv(f\"Parcel_Sort_Order_{pd.Timestamp.now().strftime('%Y%m%d%H%M%S')}.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Get Lookup Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to lookup lists\n",
    "res_assigned_lookup = \"Lookup_Lists/forecast_residential_assigned_units.csv\"\n",
    "res_zoned_lookup    = \"Lookup_Lists/forecast_residential_zoned_units.csv\"\n",
    "ctc_assetlands_lookup = \"Lookup_Lists/CTC_AssetLands_Lookup.csv\"\n",
    "\n",
    "# get zoned and assigned lookup lists as data frames\n",
    "dfResZoned    = pd.read_csv(res_zoned_lookup)\n",
    "dfResAssigned = pd.read_csv(res_assigned_lookup)\n",
    "# get lookup list for CTC asset lands (17 parcels)\n",
    "ctc_parcels = pd.read_csv(ctc_assetlands_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Assign Known Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast Known Residential Projects from 2023-2024\n",
    "# group dfResAssigned by APN and sum Unit Change to aggregate to one total for duplciate \n",
    "dfResAssignedGrouped_APN = dfResAssigned.groupby('APN').sum('Unit Change').reset_index()\n",
    "# assign forecast residential units for assigned projects\n",
    "sdfParcels['FORECASTED_RESIDENTIAL_UNITS'] = sdfParcels.APN.map(dict(zip(dfResAssignedGrouped_APN.APN, dfResAssignedGrouped_APN['Unit Change'])))\n",
    "# forecast reason = Assigned for APNs in dfResAssignedGrouped\n",
    "sdfParcels.loc[sdfParcels['APN'].isin(dfResAssigned['APN']), 'FORECAST_REASON'] = 'Assigned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdfParcels.groupby(['JURISDICTION','FORECAST_REASON'])['FORECASTED_RESIDENTIAL_UNITS'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total forecasted units\n",
    "total_forecasted_units = sdfParcels['FORECASTED_RESIDENTIAL_UNITS'].sum()\n",
    "print(f'Total Forecasted Units: {total_forecasted_units}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Forecast full buildout of CTC Asset Lands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CTC Asset Lands ##\n",
    "# set the forecast reason to CTC Asset Lands for the 17 parcels n\n",
    "sdfParcels.loc[(sdfParcels['APN'].isin(ctc_parcels['APN'])) & (sdfParcels['FORECAST_REASON']!='Assigned'), 'FORECAST_REASON'] = 'CTC Asset Lands'\n",
    "# CTC asset lands that are truly buildable\n",
    "CTC_condition          = \"(sdfParcels['FORECAST_REASON'] == 'CTC Asset Lands') & (sdfParcels['POTENTIAL_UNITS'] > 0) & (sdfParcels['TOP_TEN_POTENTIAL_UNITS'] == 'Yes')\"\n",
    "# assign POTEINTIAL_UNITS to FORECASTED_RESIDENTIAL_UNITS for CTC Asset Lands that are buildable and not built by 2024\n",
    "sdfParcels.loc[eval(CTC_condition), 'FORECASTED_RESIDENTIAL_UNITS'] = sdfParcels['POTENTIAL_UNITS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdfParcels.groupby(['JURISDICTION','FORECAST_REASON'])['FORECASTED_RESIDENTIAL_UNITS'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Subtract Assigned Units from the appropriate pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract Assigned units from the appropriate pool\n",
    "# group dfResAssigned by Jurisdiction and Unit_Pool\n",
    "dfGroup_Assigned = dfResAssigned.groupby(['Jurisdiction', 'Unit_Pool']).sum('Unit Change').reset_index()\n",
    "# drop Occupancy_Rate and Year columns\n",
    "dfGroup_Assigned = dfGroup_Assigned.drop(columns=['Occupancy_Rate'])\n",
    "# rename Unit Change to Unit_Change\n",
    "dfGroup_Assigned = dfGroup_Assigned.rename(columns={'Unit Change':'Unit_Change'})\n",
    "\n",
    "# merge dfResAssignedGrouped with dfResZoned on Jurisdiction and Pool\n",
    "dfPool_Assigned = pd.merge(dfResZoned, dfGroup_Assigned, on=['Jurisdiction', 'Unit_Pool'], how='left')\n",
    "\n",
    "# fill NaN with 0\n",
    "dfPool_Assigned['Unit_Change'] = dfPool_Assigned['Unit_Change'].fillna(0)\n",
    "# subtract known project aggregations from the zoned unit pools\n",
    "dfPool_Assigned['Future_Units_Adjusted'] = dfPool_Assigned['Future_Units'] - dfPool_Assigned['Unit_Change']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of forecasted units, forecasted units adjusted, and unit change\n",
    "for i in ['Future_Units', 'Future_Units_Adjusted', 'Unit_Change']:\n",
    "    print(f\"{i}: {dfPool_Assigned[i].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Subtract the CTC asset lands buildout from the appropriate pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to CTC Asset Lands parcels \n",
    "sdfCTC = sdfParcels.loc[sdfParcels['FORECAST_REASON'] == 'CTC Asset Lands']\n",
    "# sum of forecasted residential units where forecast reason = ctc asset lands\n",
    "sdfCTC_ForecastedByJurisdiction = sdfCTC.groupby('JURISDICTION')['FORECASTED_RESIDENTIAL_UNITS'].sum().reset_index()\n",
    "# set unit pool to Bonus Unit or General based on Jurisdiction\n",
    "sdfCTC_ForecastedByJurisdiction.loc[(sdfCTC_ForecastedByJurisdiction['JURISDICTION']=='CSLT'), 'Unit_Pool'] = 'General'\n",
    "sdfCTC_ForecastedByJurisdiction.loc[(sdfCTC_ForecastedByJurisdiction['JURISDICTION']=='PL'), 'Unit_Pool']   = 'Bonus Unit'\n",
    "sdfCTC_ForecastedByJurisdiction.loc[(sdfCTC_ForecastedByJurisdiction['JURISDICTION']=='EL'), 'Unit_Pool']   = 'General'\n",
    "# rename columns\n",
    "sdfCTC_ForecastedByJurisdiction.rename(columns={'JURISDICTION': 'Jurisdiction', 'FORECASTED_RESIDENTIAL_UNITS': 'Forecasted_CTC'}, inplace=True)\n",
    "\n",
    "# Subtract CTC asset lands from the appropriate pool\n",
    "dfPool_CTC = sdfCTC_ForecastedByJurisdiction.copy()\n",
    "# merge with dfResMerge\n",
    "dfPool_CTC = pd.merge(dfPool_Assigned, dfPool_CTC, on=['Jurisdiction', 'Unit_Pool'], how='left')\n",
    "\n",
    "# fill NaN with 0\n",
    "dfPool_CTC['Forecasted_CTC'] = dfPool_CTC['Forecasted_CTC'].fillna(0)\n",
    "# subtract CTC asset lands from the appropriate pool\n",
    "dfPool_CTC['Future_Units_Adjusted'] = dfPool_CTC['Future_Units_Adjusted'] - dfPool_CTC['Forecasted_CTC']\n",
    "# add CTC Max Build to the unit change\n",
    "dfPool_CTC['Unit_Change'] = dfPool_CTC['Unit_Change'] + dfPool_CTC['Forecasted_CTC']\n",
    "# drop CTC Built column\n",
    "dfPool_CTC.drop(columns=['Forecasted_CTC'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of forecasted units, forecasted units adjusted, and unit change\n",
    "for i in ['Future_Units', 'Future_Units_Adjusted', 'Unit_Change']:\n",
    "    print(f\"{i}: {dfPool_CTC[i].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPool_CTC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Setup Unit Pools Proportion to be Used in Each Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPool = dfPool_CTC.copy()\n",
    "# proportion target of forecast development by type\n",
    "portion_multifamily = .35\n",
    "portion_singlefamily = .5\n",
    "portion_infill = .15\n",
    "# Set units to use for each zoning type\n",
    "dfPool['Future_Units_Adjusted'] = dfPool['Future_Units_Adjusted'].fillna(0)\n",
    "dfPool['Future_Units_Adjusted_MF'] = (dfPool['Future_Units_Adjusted'] * portion_multifamily).round().astype(int)\n",
    "dfPool['Future_Units_Adjusted_SF'] = (dfPool['Future_Units_Adjusted'] * portion_singlefamily).round().astype(int)\n",
    "dfPool['Future_Units_Adjusted_Infill'] = (dfPool['Future_Units_Adjusted'] * portion_infill).round().astype(int)\n",
    "# Assign any rounding error to the single family pool\n",
    "dfPool['Adjustment'] = dfPool['Future_Units_Adjusted'] - dfPool['Future_Units_Adjusted_MF'] - dfPool['Future_Units_Adjusted_SF'] - dfPool['Future_Units_Adjusted_Infill']\n",
    "dfPool['Future_Units_Adjusted_SF'] = dfPool['Future_Units_Adjusted_SF'] + dfPool['Adjustment']\n",
    "dfPool.drop(columns=['Adjustment'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Define Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##------------------------------------ Conditional Statements for Forecasting Residential Unit Development------------------------------ ##\n",
    "# vacant buildable criteria\n",
    "vacant_buildable_criteria        = \"(df['FORECAST_REASON'].isna()) & (df['EXISTING_LANDUSE'] == 'Vacant') & (df['OWNERSHIP_TYPE'] == 'Private') & (df['RETIRED'] == 'No') & (df['IPES_SCORE'] > 0)\"\n",
    "placer_vacant_buildable_criteria = \"(df['FORECAST_REASON'].isna()) & (df['EXISTING_LANDUSE'] == 'Vacant') & (df['OWNERSHIP_TYPE'] == 'Private') & (df['RETIRED'] == 'No') & (df['IPES_SCORE'] > 726)\" \n",
    "# Within TRPA Boundary as condition for all\n",
    "trpa_boundary_criteria = \"(df['WITHIN_TRPA_BNDY'] == 1)\"\n",
    "no_zoning_criteria     = \"(df['HOUSING_ZONING'] != 'NA')\"\n",
    "sf_only_criteria       = \"(df['HOUSING_ZONING'] == 'SF_only')\"\n",
    "mf_only_criteria       = \"(df['HOUSING_ZONING'] == 'MF_only')\"\n",
    "sf_mf_criteria         = \"(df['HOUSING_ZONING'].isin(['SF/MF', 'MF_only']))\"\n",
    "bonus_criteria         = \"(df['WITHIN_BONUSUNIT_BNDY'] == 'Yes')\"\n",
    "adu_criteria           = \"(df['ADU_ALLOWED'] == 'Yes')\"\n",
    "towncenter_condition   = \"(~df['TOWN_CENTER'].isna())\"\n",
    "tc_quarter_condition   = \"(df['LOCATION_TO_TOWNCENTER'] == 'Within 1/4 Mile')\"\n",
    "top_10_condition       = \"(df['TOP_TEN_POTENTIAL_UNITS'] == 'Yes')\"\n",
    "parcel_size_condition  = \"(df['PARCEL_ACRES'] >= 0.15)\"\n",
    "condo_condition        = \"(~df['EXISTING_LANDUSE'].isin('Condominium', 'Condomunium Common Area'))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Forecast Jurisdiction Pools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPool = dfPool.copy()\n",
    "dfPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of forecasted units, forecasted units adjusted, and unit change\n",
    "for i in ['Future_Units', 'Future_Units_Adjusted', 'Unit_Change']:\n",
    "    print(f\"{i}: {dfPool[i].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Assign Units using the Bonus Unit Pools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ------------------------------------------------------UNIT ASSIGNMENTS BY POOL------------------------------------------------------ ####\n",
    "\n",
    "##-----------------------------------------------------------Bonus Unit Assignments-----------------------------------------------------------##\n",
    "## CSLT Bonus Unit Assignments ##\n",
    "# Assign Bonus Units to Multifamily zoned parcels within the Bonus Unit Boundary\n",
    "target_sum             = get_target_sum(dfPool, 'CSLT', 'Bonus Unit', 'MF')\n",
    "CSLT_Bonus_condition   = \"(df['JURISDICTION'] == 'CSLT') & (df['WITHIN_BONUSUNIT_BNDY'] == 'Yes')\" + \" & \" + trpa_boundary_criteria + \" & \" + vacant_buildable_criteria + \" & \" + sf_mf_criteria\n",
    "sdfParcels, df_summary = forecast_residential_units(sdfParcels, CSLT_Bonus_condition, target_sum, 'CSLT Bonus Units MF')\n",
    "df_built_parcels = df_summary\n",
    "# Assign Bonus Units to Single Family only zoned parcels within the Bonus Unit Boundary\n",
    "target_sum             = get_target_sum(dfPool, 'CSLT', 'Bonus Unit', 'SF')\n",
    "CSLT_Bonus_condition   = \"(df['JURISDICTION'] == 'CSLT') & (df['WITHIN_BONUSUNIT_BNDY'] == 'Yes')\" + \" & \" + trpa_boundary_criteria + \" & \" + vacant_buildable_criteria + \" & \" + sf_only_criteria\n",
    "sdfParcels, df_summary = forecast_residential_units(sdfParcels, CSLT_Bonus_condition, target_sum, 'CSLT Bonus Units SF')\n",
    "df_built_parcels = pd.concat([df_built_parcels, df_summary], ignore_index=True)\n",
    "# Assign Bonus Units to Developable Infill parcels within the Bonus Unit Boundary\n",
    "target_sum             = get_target_sum(dfPool, 'CSLT', 'Bonus Unit', 'Infill')\n",
    "CSLT_Bonus_condition   = \"(df['JURISDICTION'] == 'CSLT') & (df['WITHIN_BONUSUNIT_BNDY'] == 'Yes')\" + \" & \" + trpa_boundary_criteria + \" & \" + sf_mf_criteria\n",
    "sdfParcels, df_summary = forecast_residential_units_infill(sdfParcels, CSLT_Bonus_condition, target_sum, 'CSLT Bonus Units Infill')\n",
    "df_built_parcels = pd.concat([df_built_parcels, df_summary], ignore_index=True)\n",
    "## Douglas Bonus Unit Assignments ##\n",
    "# Assign Bonus Units to Multifamily zoned parcels within the Bonus Unit Boundary\n",
    "target_sum             = get_target_sum(dfPool, 'DG', 'Bonus Unit', 'MF')\n",
    "DG_Bonus_condition     = \"(df['JURISDICTION'] == 'DG') & (df['WITHIN_BONUSUNIT_BNDY'] == 'Yes')\" + \" & \" + trpa_boundary_criteria + \" & \" + vacant_buildable_criteria + \" & \" + sf_mf_criteria\n",
    "sdfParcels, df_summary = forecast_residential_units(sdfParcels, DG_Bonus_condition, target_sum, 'DG Bonus Units MF')\n",
    "df_built_parcels = pd.concat([df_built_parcels, df_summary], ignore_index=True)\n",
    "# Assign Bonus Units to Single Family only zoned parcels within the Bonus Unit Boundary\n",
    "target_sum             = get_target_sum(dfPool, 'DG', 'Bonus Unit', 'SF')\n",
    "DG_Bonus_condition     = \"(df['JURISDICTION'] == 'DG') & (df['WITHIN_BONUSUNIT_BNDY'] == 'Yes')\" + \" & \" + trpa_boundary_criteria + \" & \" + vacant_buildable_criteria + \" & \" + sf_only_criteria\n",
    "sdfParcels, df_summary = forecast_residential_units(sdfParcels, DG_Bonus_condition, target_sum, 'DG Bonus Units SF')\n",
    "# Assign Bonus Units to Developable Infill parcels within the Bonus Unit Boundary\n",
    "target_sum             = get_target_sum(dfPool, 'DG', 'Bonus Unit', 'Infill')\n",
    "DG_Bonus_condition     = \"(df['JURISDICTION'] == 'DG') & (df['WITHIN_BONUSUNIT_BNDY'] == 'Yes')\" + \" & \" + trpa_boundary_criteria +  \" & \" + sf_mf_criteria\n",
    "sdfParcels, df_summary = forecast_residential_units_infill(sdfParcels, DG_Bonus_condition, target_sum, 'DG Bonus Units Infill')\n",
    "df_built_parcels = pd.concat([df_built_parcels, df_summary], ignore_index=True)\n",
    "\n",
    "## Placer Bonus Unit Assignments ##\n",
    "# Assign Bonus Units to Multifamily zoned parcels within the Bonus Unit Boundary\n",
    "target_sum             = get_target_sum(dfPool, 'PL', 'Bonus Unit', 'MF')\n",
    "PL_Bonus_condition     = \"(df['JURISDICTION'] == 'PL') & (df['WITHIN_BONUSUNIT_BNDY'] == 'Yes')\" + \" & \" + trpa_boundary_criteria + \" & \" + placer_vacant_buildable_criteria + \" & \" + sf_mf_criteria\n",
    "sdfParcels, df_summary = forecast_residential_units(sdfParcels, PL_Bonus_condition, target_sum, 'PL Bonus Units MF')\n",
    "df_built_parcels = pd.concat([df_built_parcels, df_summary], ignore_index=True)\n",
    "# Assign Bonus Units to Single Family only zoned parcels within the Bonus Unit Boundary\n",
    "target_sum             = get_target_sum(dfPool, 'PL', 'Bonus Unit', 'SF')\n",
    "PL_Bonus_condition     = \"(df['JURISDICTION'] == 'PL') & (df['WITHIN_BONUSUNIT_BNDY'] == 'Yes')\" + \" & \" + trpa_boundary_criteria + \" & \" + placer_vacant_buildable_criteria + \" & \" + sf_only_criteria\n",
    "sdfParcels, df_summary = forecast_residential_units(sdfParcels, PL_Bonus_condition, target_sum, 'PL Bonus Units SF')\n",
    "df_built_parcels = pd.concat([df_built_parcels, df_summary], ignore_index=True)\n",
    "# Assign Bonus Units to Developable Infill parcels within the Bonus Unit Boundary\n",
    "target_sum             = get_target_sum(dfPool, 'PL', 'Bonus Unit', 'Infill')\n",
    "PL_Bonus_condition     = \"(df['JURISDICTION'] == 'PL') & (df['WITHIN_BONUSUNIT_BNDY'] == 'Yes')\" + \" & \" + trpa_boundary_criteria +  \" & \" + sf_mf_criteria\n",
    "sdfParcels, df_summary = forecast_residential_units(sdfParcels, PL_Bonus_condition, target_sum, 'PL Bonus Units Infill')\n",
    "df_built_parcels = pd.concat([df_built_parcels, df_summary], ignore_index=True)\n",
    "\n",
    "## Washoe Bonus Unit Assignments ##\n",
    "# Assign Bonus Units to Multifamily zoned parcels within the Bonus Unit Boundary\n",
    "target_sum             = get_target_sum(dfPool, 'WA', 'Bonus Unit', 'MF')\n",
    "WA_Bonus_condition     = \"(df['JURISDICTION'] == 'WA') & (df['WITHIN_BONUSUNIT_BNDY'] == 'Yes')\" + \" & \" + trpa_boundary_criteria + \" & \" + vacant_buildable_criteria + \" & \" + sf_mf_criteria\n",
    "sdfParcels, df_summary = forecast_residential_units(sdfParcels, WA_Bonus_condition, target_sum, 'WA Bonus Units MF')\n",
    "df_built_parcels = pd.concat([df_built_parcels, df_summary], ignore_index=True)\n",
    "# Assign Bonus Units to Single Family only zoned parcels within the Bonus Unit Boundary\n",
    "target_sum             = get_target_sum(dfPool, 'WA', 'Bonus Unit', 'SF')\n",
    "WA_Bonus_condition     = \"(df['JURISDICTION'] == 'WA') & (df['WITHIN_BONUSUNIT_BNDY'] == 'Yes')\" + \" & \" + trpa_boundary_criteria + \" & \" + vacant_buildable_criteria + \" & \" + sf_only_criteria\n",
    "sdfParcels, df_summary = forecast_residential_units(sdfParcels, WA_Bonus_condition, target_sum, 'WA Bonus Units SF')\n",
    "df_built_parcels = pd.concat([df_built_parcels, df_summary], ignore_index=True)\n",
    "# Assign Bonus Units to Developable Infill parcels within the Bonus Unit Boundary\n",
    "target_sum             = get_target_sum(dfPool, 'WA', 'Bonus Unit', 'Infill')\n",
    "WA_Bonus_condition     = \"(df['JURISDICTION'] == 'WA') & (df['WITHIN_BONUSUNIT_BNDY'] == 'Yes')\" + \" & \" + trpa_boundary_criteria + \" & \" + sf_mf_criteria\n",
    "sdfParcels, df_summary = forecast_residential_units_infill(sdfParcels, WA_Bonus_condition, target_sum, 'WA Bonus Units Infill')\n",
    "df_built_parcels = pd.concat([df_built_parcels, df_summary], ignore_index=True)\n",
    "\n",
    "df_built_parcels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of forecasted units, forecasted units adjusted, and unit change\n",
    "for i in ['Future_Units', 'Future_Units_Adjusted', 'Unit_Change']:\n",
    "    print(f\"{i}: {dfPool[i].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Assign Units using General Unit Pools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------General Unit Assignments---------------------------------------------------- ##\n",
    "## CSTL General Unit Assignments ##\n",
    "# Assign General Jurisdiction Units to Multifamily zoned parcels\n",
    "target_sum             = get_target_sum(dfPool, 'CSLT', 'General', 'MF')\n",
    "CSLT_MF_condition     = \"(df['JURISDICTION'] == 'CSLT') & \" + trpa_boundary_criteria + \" & \" + vacant_buildable_criteria + \" & \" + sf_mf_criteria\n",
    "sdfParcels, df_summary = forecast_residential_units(sdfParcels, CSLT_MF_condition, target_sum, 'CSLT General Units MF')\n",
    "df_built_parcels = pd.concat([df_built_parcels, df_summary], ignore_index=True)\n",
    "# Assign General Jurisdiction Units to Single Family only zoned parcels\n",
    "target_sum             = get_target_sum(dfPool, 'CSLT', 'General', 'SF')\n",
    "CSLT_SF_condition     = \"(df['JURISDICTION'] == 'CSLT') & \" + trpa_boundary_criteria + \" & \" + vacant_buildable_criteria + \" & \" + sf_only_criteria\n",
    "sdfParcels, df_summary = forecast_residential_units(sdfParcels, CSLT_SF_condition, target_sum, 'CSLT General Units SF')\n",
    "df_built_parcels = pd.concat([df_built_parcels, df_summary], ignore_index=True)\n",
    "# Assign General Jurisdiction Units to Developable Infill parcels\n",
    "target_sum             = get_target_sum(dfPool, 'CSLT', 'General', 'Infill')\n",
    "CSLT_Infill_condition = \"(df['JURISDICTION'] == 'CSLT') & \" + trpa_boundary_criteria + \" & \" + vacant_buildable_criteria + \" & \" + sf_mf_criteria\n",
    "sdfParcels, df_summary = forecast_residential_units_infill(sdfParcels, CSLT_Infill_condition, target_sum, 'CSLT General Units Infill')\n",
    "df_built_parcels = pd.concat([df_built_parcels, df_summary], ignore_index=True)\n",
    "\n",
    "## El Dorado General Unit Assignments ##\n",
    "# Assign General Jurisdiction Units to Multifamily zoned parcels\n",
    "target_sum             = get_target_sum(dfPool, 'EL', 'General', 'MF')\n",
    "EL_MF_condition        = \"(df['JURISDICTION'] == 'EL') & \" + trpa_boundary_criteria + \" & \" + vacant_buildable_criteria + \" & \" + sf_mf_criteria\n",
    "sdfParcels, df_summary = forecast_residential_units(sdfParcels, EL_MF_condition, target_sum, 'EL General Units MF')\n",
    "df_built_parcels = pd.concat([df_built_parcels, df_summary], ignore_index=True)\n",
    "# Assign General Jurisdiction Units to Single Family only zoned parcels\n",
    "target_sum             = get_target_sum(dfPool, 'EL', 'General', 'SF')\n",
    "EL_SF_condition        = \"(df['JURISDICTION'] == 'EL') & \" + trpa_boundary_criteria + \" & \" + vacant_buildable_criteria + \" & \" + sf_only_criteria\n",
    "sdfParcels, df_summary = forecast_residential_units(sdfParcels, EL_SF_condition, target_sum, 'EL General Units SF')\n",
    "df_built_parcels = pd.concat([df_built_parcels, df_summary], ignore_index=True)\n",
    "# Assign General Jurisdiction Units to Developable Infill parcels\n",
    "target_sum             = get_target_sum(dfPool, 'EL', 'General', 'Infill')\n",
    "EL_Infill_condition    = \"(df['JURISDICTION'] == 'EL') & \" + trpa_boundary_criteria + \" & \" + vacant_buildable_criteria + \" & \" + sf_mf_criteria\n",
    "sdfParcels, df_summary = forecast_residential_units_infill(sdfParcels, EL_Infill_condition, target_sum, 'EL General Units Infill')\n",
    "df_built_parcels = pd.concat([df_built_parcels, df_summary], ignore_index=True)\n",
    "\n",
    "# Placer General Unit Assignments\n",
    "# Assign General Jurisdiction Units to Multifamily zoned parcels\n",
    "target_sum             = get_target_sum(dfPool, 'PL', 'General', 'MF')\n",
    "PL_MF_condition        = \"(df['JURISDICTION'] == 'PL') & \" + trpa_boundary_criteria + \" & \" + vacant_buildable_criteria + \" & \" + sf_mf_criteria\n",
    "sdfParcels, df_summary = forecast_residential_units(sdfParcels, PL_MF_condition, target_sum, 'PL General Units MF')\n",
    "df_built_parcels = pd.concat([df_built_parcels, df_summary], ignore_index=True)\n",
    "# Assign General Jurisdiction Units to Single Family only zoned parcels\n",
    "target_sum             = get_target_sum(dfPool, 'PL', 'General', 'SF')\n",
    "PL_SF_condition        = \"(df['JURISDICTION'] == 'PL') & \" + trpa_boundary_criteria + \" & \" + placer_vacant_buildable_criteria + \" & \" + sf_only_criteria\n",
    "sdfParcels, df_summary = forecast_residential_units(sdfParcels, PL_SF_condition, target_sum, 'PL General Units SF')\n",
    "df_built_parcels = pd.concat([df_built_parcels, df_summary], ignore_index=True)\n",
    "# Assign General Jurisdiction Units to Developable Infill parcels\n",
    "target_sum             = get_target_sum(dfPool, 'PL', 'General', 'Infill')\n",
    "PL_Infill_condition    = \"(df['JURISDICTION'] == 'PL') & \" + trpa_boundary_criteria + \" & \" + placer_vacant_buildable_criteria + \" & \" + sf_mf_criteria\n",
    "sdfParcels, df_summary = forecast_residential_units_infill(sdfParcels, PL_Infill_condition, target_sum, 'PL General Units Infill')\n",
    "df_built_parcels = pd.concat([df_built_parcels, df_summary], ignore_index=True)\n",
    "\n",
    "# Douglas General Unit Assignments\n",
    "# Assign General Jurisdiction Units to Multifamily zoned parcels\n",
    "target_sum             = get_target_sum(dfPool, 'DG', 'General', 'MF')\n",
    "DG_MF_condition        = \"(df['JURISDICTION'] == 'DG') & \" + trpa_boundary_criteria + \" & \" + vacant_buildable_criteria + \" & \" + sf_mf_criteria\n",
    "sdfParcels, df_summary = forecast_residential_units(sdfParcels, DG_MF_condition, target_sum, 'DG General Units MF')\n",
    "df_built_parcels = pd.concat([df_built_parcels, df_summary], ignore_index=True)\n",
    "# Assign General Jurisdiction Units to Single Family only zoned parcels\n",
    "target_sum             = get_target_sum(dfPool, 'DG', 'General', 'SF')\n",
    "DG_SF_condition        = \"(df['JURISDICTION'] == 'DG') & \" + trpa_boundary_criteria + \" & \" + vacant_buildable_criteria + \" & \" + sf_only_criteria\n",
    "sdfParcels, df_summary = forecast_residential_units(sdfParcels, DG_SF_condition, target_sum, 'DG General Units SF')\n",
    "df_built_parcels = pd.concat([df_built_parcels, df_summary], ignore_index=True)\n",
    "# Assign General Jurisdiction Units to Developable Infill parcels\n",
    "target_sum             = get_target_sum(dfPool, 'DG', 'General', 'Infill')\n",
    "DG_Infill_condition    = \"(df['JURISDICTION'] == 'DG') & \" + trpa_boundary_criteria + \" & \" + vacant_buildable_criteria + \" & \" + sf_mf_criteria\n",
    "sdfParcels, df_summary = forecast_residential_units_infill(sdfParcels, DG_Infill_condition, target_sum, 'DG General Units Infill')\n",
    "df_built_parcels = pd.concat([df_built_parcels, df_summary], ignore_index=True)\n",
    "\n",
    "# Washoe General Unit Assignments\n",
    "# Assign General Jurisdiction Units to Multifamily zoned parcels \n",
    "target_sum             = get_target_sum(dfPool, 'WA', 'General', 'MF')\n",
    "WA_MF_condition        = \"(df['JURISDICTION'] == 'WA') & \" + trpa_boundary_criteria + \" & \" + vacant_buildable_criteria + \" & \" + sf_mf_criteria\n",
    "sdfParcels, df_summary = forecast_residential_units(sdfParcels, WA_MF_condition, target_sum, 'WA General Units MF')\n",
    "df_built_parcels = pd.concat([df_built_parcels, df_summary], ignore_index=True)\n",
    "# Assign General Jurisdiction Units to Single Family only zoned parcels\n",
    "target_sum             = get_target_sum(dfPool, 'WA', 'General', 'SF')\n",
    "WA_SF_condition        = \"(df['JURISDICTION'] == 'WA') & \" + trpa_boundary_criteria + \" & \" + vacant_buildable_criteria + \" & \" + sf_only_criteria\n",
    "sdfParcels, df_summary = forecast_residential_units(sdfParcels, WA_SF_condition, target_sum, 'WA General Units SF')\n",
    "df_built_parcels = pd.concat([df_built_parcels, df_summary], ignore_index=True)\n",
    "# Assign General Jurisdiction Units to Developable Infill parcels\n",
    "target_sum             = get_target_sum(dfPool, 'WA', 'General', 'Infill')\n",
    "WA_Infill_condition    = \"(df['JURISDICTION'] == 'WA') & \" + trpa_boundary_criteria + \" & \" + sf_mf_criteria\n",
    "sdfParcels, df_summary = forecast_residential_units_infill(sdfParcels, WA_Infill_condition, target_sum, 'WA General Units Infill')\n",
    "df_built_parcels = pd.concat([df_built_parcels, df_summary], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_built_parcels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Forecast TRPA Pools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ------------------------------------------------------UNIT ASSIGNMENTS BY POOL------------------------------------------------------ ####\n",
    "dfPool = dfPool.copy()\n",
    "##-----------------------------------------------------------Bonus Unit Assignments-----------------------------------------------------------##\n",
    "## TRPA Bonus Unit Assignments ##\n",
    "# Assign Bonus Units to Multifamily zoned parcels within the Bonus Unit Boundary\n",
    "target_sum             = get_target_sum(dfPool, 'TRPA', 'Bonus Unit', 'MF')\n",
    "TRPA_Bonus_condition   = \"(df['WITHIN_BONUSUNIT_BNDY'] == 'Yes')\" + \" & \" + trpa_boundary_criteria + \" & \" + vacant_buildable_criteria + \" & \" + sf_mf_criteria\n",
    "sdfParcels, df_summary = forecast_residential_units(sdfParcels, TRPA_Bonus_condition, target_sum, 'TRPA Bonus Units MF')\n",
    "df_built_parcels = pd.concat([df_built_parcels, df_summary], ignore_index=True)\n",
    "# Assign Bonus Units to Single Family only zoned parcels within the Bonus Unit Boundary\n",
    "target_sum             = get_target_sum(dfPool, 'TRPA', 'Bonus Unit', 'SF')\n",
    "TRPA_Bonus_condition   = \"(df['WITHIN_BONUSUNIT_BNDY'] == 'Yes')\" + \" & \" + trpa_boundary_criteria + \" & \" + vacant_buildable_criteria + \" & \" + sf_only_criteria\n",
    "sdfParcels, df_summary = forecast_residential_units(sdfParcels, TRPA_Bonus_condition, target_sum, 'TRPA Bonus Units SF')\n",
    "df_built_parcels = pd.concat([df_built_parcels, df_summary], ignore_index=True)\n",
    "# Assign Bonus Units to Developable Infill parcels within the Bonus Unit Boundary\n",
    "target_sum             = get_target_sum(dfPool, 'TRPA', 'Bonus Unit', 'Infill')\n",
    "TRPA_Bonus_condition   = \"(df['WITHIN_BONUSUNIT_BNDY'] == 'Yes')\" + \" & \" + trpa_boundary_criteria + \" & \" + sf_mf_criteria\n",
    "sdfParcels, df_summary = forecast_residential_units_infill(sdfParcels, TRPA_Bonus_condition, target_sum, 'TRPA Bonus Units Infill')\n",
    "df_built_parcels = pd.concat([df_built_parcels, df_summary], ignore_index=True)\n",
    "\n",
    "## TRPA General Unit Assignments ##\n",
    "# Assign General Jurisdiction Units to Multifamily zoned parcels\n",
    "target_sum             = get_target_sum(dfPool, 'TRPA', 'General', 'MF')\n",
    "TRPA_MF_condition      = trpa_boundary_criteria + \" & \" + vacant_buildable_criteria + \" & \" + sf_mf_criteria\n",
    "sdfParcels, df_summary = forecast_residential_units(sdfParcels, TRPA_MF_condition, target_sum, 'TRPA General Units MF')\n",
    "df_built_parcels = pd.concat([df_built_parcels, df_summary], ignore_index=True)\n",
    "# Assign General Jurisdiction Units to Single Family only zoned parcels\n",
    "target_sum             = get_target_sum(dfPool, 'TRPA', 'General', 'SF')\n",
    "TRPA_SF_condition     = trpa_boundary_criteria + \" & \" + vacant_buildable_criteria + \" & \" + sf_only_criteria\n",
    "sdfParcels, df_summary = forecast_residential_units(sdfParcels, TRPA_SF_condition, target_sum, 'TRPA General Units SF')\n",
    "df_built_parcels = pd.concat([df_built_parcels, df_summary], ignore_index=True)\n",
    "# Assign General Jurisdiction Units to Developable Infill parcels\n",
    "target_sum             = get_target_sum(dfPool, 'TRPA', 'General', 'Infill')\n",
    "TRPA_Infill_condition = trpa_boundary_criteria + \" & \" + vacant_buildable_criteria + \" & \" + sf_mf_criteria\n",
    "sdfParcels, df_summary = forecast_residential_units_infill(sdfParcels, TRPA_Infill_condition, target_sum, 'TRPA General Units Infill')\n",
    "df_built_parcels = pd.concat([df_built_parcels, df_summary], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = df_built_parcels.Total_Forecasted_Units.sum() + 1162 +54\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_built_parcels.Total_Remaining_Units.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPool.Future_Units_Adjusted.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_built_parcels.Total_Remaining_Units.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPool.Future_Units.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPool.Future_Units_Adjusted.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPool.Unit_Change.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_built_parcels.Total_Remaining_Units.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4385-3555"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPool.Future_Units.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPool.Future_Units_Adjusted.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdfParcels.FORECASTED_RESIDENTIAL_UNITS.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPool.info()\n",
    "dfPoolMelt = dfPool.melt(id_vars=['Jurisdiction', 'Unit_Pool'], value_vars=['Future_Units_Adjusted_MF', 'Future_Units_Adjusted_SF', 'Future_Units_Adjusted_Infill'])\n",
    "# drop Future_Units_Adjusted_ from variable\n",
    "dfPoolMelt['variable'] = dfPoolMelt['variable'].str.replace('Future_Units_Adjusted_', '')\n",
    "\n",
    "dfPoolMelt['Unit_Pool'] = dfPoolMelt['Jurisdiction'] + ' ' + dfPoolMelt['Unit_Pool']\n",
    "dfPoolMelt.rename(columns={'variable': 'Reason', 'value': 'Units'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfForecastGroup = sdfParcels.groupby(['FORECAST_REASON'])['FORECASTED_RESIDENTIAL_UNITS'].sum().reset_index()\n",
    "# split last FORECAST Reason into two columns based on last space in string\n",
    "dfForecastGroup['Reason'] = dfForecastGroup['FORECAST_REASON'].str.split(' ').str[-1]\n",
    "dfForecastGroup['Jurisdiction'] = dfForecastGroup['FORECAST_REASON'].str.split(' ').str[0]\n",
    "# if FORECAST_REASON valie contains 'Bonus' then set Unit Pool to Bonus Units\n",
    "dfForecastGroup.loc[dfForecastGroup['FORECAST_REASON'].str.contains('Bonus'), 'Unit_Pool'] = dfForecastGroup.Jurisdiction + ' ' + 'Bonus Unit'\n",
    "# if FORECAST_REASON valie contains 'General' then set Unit Pool to General Units\n",
    "dfForecastGroup.loc[dfForecastGroup['FORECAST_REASON'].str.contains('General'), 'Unit_Pool'] = dfForecastGroup.Jurisdiction + ' ' + 'General'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMerge = dfForecastGroup.merge(dfPoolMelt, on=['Unit_Pool', 'Reason'], how='left')\n",
    "dfMerge['Unit_Diff'] = dfMerge['FORECASTED_RESIDENTIAL_UNITS'] - dfMerge['Units']\n",
    "dfMerge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sdfParcels where forecast reason is CLT Bonus Units MF\n",
    "sdfParcels.loc[sdfParcels['FORECAST_REASON'] == 'CSLT Bonus Units MF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMerge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Assign the Remainders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign Units to SF/MF and MF only parcels, 35% of all pools to MF only parcels\n",
    "# Check total units available\n",
    "# Assign Units to SF only parcels, 50% of all pools to SF only parcels\n",
    "# Assign Units to underbuilt MF parcels within the Bonus Unit Boundary\n",
    "# Assign Units to parcels within a Town Center\n",
    "# Assign Units to parcels within 1/4 mile of a Town Center\n",
    "## TRPA ADU Unit Assignments ##\n",
    "# Assign remaining Units to parcels eligible for ADU constuction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign ADUs to existing residential parcels residential units=1 and ADU_ALLOWED = Yes\n",
    "target_sum = get_target_sum(dfPool, 'TRPA', 'General', 'ADU')\n",
    "TRPA_ADU_condition = trpa_boundary_criteria + \" & \" + adu_criteria\n",
    "sdfParcels, df_summary = forecast_residential_units(sdfParcels, TRPA_ADU_condition, target_sum, 'TRPA ADU Units')\n",
    "df_built_parcels = pd.concat([df_built_parcels, df_summary], ignore_index=True)\n",
    "\n",
    "# assign the rest of units to Town Center infill\n",
    "target_sum = get_target_sum(dfPool, 'TRPA', 'General', 'Infill')\n",
    "TRPA_Infill_condition = trpa_boundary_criteria + \" & \" + towncenter_condition + \" & \"+ vacant_buildable_criteria + \" & \" + sf_mf_criteria\n",
    "sdfParcels, df_summary = forecast_residential_units_infill(sdfParcels, TRPA_Infill_condition, target_sum, 'TRPA Town Center Infill Units')\n",
    "df_built_parcels = pd.concat([df_built_parcels, df_summary], ignore_index=True)\n",
    "\n",
    "# assign the remainder to within a quarter mile of town center\n",
    "target_sum = get_target_sum(dfPool, 'TRPA', 'General', 'Infill')\n",
    "TRPA_Infill_condition = trpa_boundary_criteria + \" & \" + tc_quarter_condition + \" & \"+ vacant_buildable_criteria + \" & \" + sf_mf_criteria\n",
    "sdfParcels, df_summary = forecast_residential_units_infill(sdfParcels, TRPA_Infill_condition, target_sum, 'TRPA Quarter Mile Infill Units')\n",
    "df_built_parcels = pd.concat([df_built_parcels, df_summary], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Assign Forecast Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assigning Development Year to Parcels ##\n",
    "# Get total development by 2035\n",
    "TotalDevelopment = dfResZoned.Future_Units.sum()\n",
    "Development_2035 = (TotalDevelopment*.46).astype(int)\n",
    "sdfParcels['Development_Year']=None\n",
    "#Assining all development that's currently in the works as being cone by 2035\n",
    "sdfParcels.loc[sdfParcels['FORECAST_REASON'] == 'Assigned', 'Development_Year'] = 2035\n",
    "RemainingDevelopment_2035 = Development_2035 - sdfParcels.loc[sdfParcels['FORECAST_REASON'] == 'Assigned', 'FORECASTED_RESIDENTIAL_UNITS'].sum()\n",
    "\n",
    "Development_2035_Condition = sdfParcels['FORECASTED_RESIDENTIAL_UNITS'].where(sdfParcels['FORECAST_REASON'] != 'Assigned').cumsum()\n",
    "sdfParcels.loc[Development_2035_Condition < RemainingDevelopment_2035, 'Development_Year'] = 2035\n",
    "sdfParcels.loc[(sdfParcels['FORECAST_REASON']!= '') & (sdfParcels['Development_Year'].isnull()), 'Development_Year'] = 2050\n",
    "development_year = sdfParcels.groupby('Development_Year')['FORECASTED_RESIDENTIAL_UNITS'].sum()\n",
    "development_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign Occupancy Rate to all forecasted residential parcels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdfParcels['FORECASTED_OCCUPANCY_RATE'] = 0\n",
    "# map lookup known project occupancy rates to parcels\n",
    "sdfParcels['FORECASETED_OCCUPANCY_RATE'] = sdfParcels['APN'].map(dict(zip(dfResAssigned.APN, dfResAssigned['Occupancy_Rate'])))\n",
    "sdfParcels.FORECAST_REASON.value_counts()\n",
    "\n",
    "# if FORECAST_REASON has the word 'Bonus' in it set occupancy rate to 100%\n",
    "sdfParcels.loc[sdfParcels['FORECAST_REASON'].fillna('').str.contains('Bonus'), 'FORECASTED_OCCUPANCY_RATE'] = 1\n",
    "# if FORECAST_REASON has the word 'CTC' in it set occupancy rate to 100%\n",
    "sdfParcels.loc[sdfParcels['FORECAST_REASON'].fillna('').str.contains('CTC'), 'FORECASTED_OCCUPANCY_RATE'] = 1\n",
    "# if FORECAST_REASON has the word 'General' and housing zoning is SF_only in it set occupancy rate to 35\n",
    "sdfParcels.loc[(sdfParcels['FORECAST_REASON'].fillna('').str.contains('General')) & (sdfParcels['HOUSING_ZONING'] == 'SF_only'), 'FORECASTED_OCCUPANCY_RATE'] = 0.35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "built_units = sdfParcels.groupby('FORECAST_REASON').agg({'FORECASTED_RESIDENTIAL_UNITS':'sum'})\n",
    "dfResZoned = dfPool.copy()\n",
    "# Create a dictionary to map the forecast reason to the Jurisdiction and Unit Pool\n",
    "Forecast_Reason_lookup = {'CSLT Bonus Units Built':{'Jurisdiction':'CSLT', 'Unit_Pool':'Bonus Unit'},\n",
    "                          'CSLT General Units Built':{'Jurisdiction':'CSLT', 'Unit_Pool':'General'},\n",
    "                          'EL General Units Built':{'Jurisdiction':'EL', 'Unit_Pool':'General'},\n",
    "                          'PL Bonus Units Built':{'Jurisdiction':'PL', 'Unit_Pool':'Bonus Unit'},\n",
    "                          'PL General Units Built':{'Jurisdiction':'PL', 'Unit_Pool':'General'},\n",
    "                          'WA Bonus Units Built':{'Jurisdiction':'WA', 'Unit_Pool':'Bonus Unit'},\n",
    "                          'WA General Units Built':{'Jurisdiction':'WA', 'Unit_Pool':'General'},\n",
    "                          'DG Bonus Units Built':{'Jurisdiction':'DG', 'Unit_Pool':'Bonus Unit'},\n",
    "                          'DG General Units Built':{'Jurisdiction':'DG', 'Unit_Pool':'General'},\n",
    "                          'TRPA Bonus Units Built':{'Jurisdiction':'TRPA', 'Unit_Pool':'Bonus Unit'},\n",
    "                          'TRPA General Units Built':{'Jurisdiction':'TRPA', 'Unit_Pool':'General'},\n",
    "                          'ADU Units Built':{'Jurisdiction':'TRPA', 'Unit_Pool':'ADU'}}\n",
    "# Map 'Jurisdiction' and 'Unit_Pool' separately from the dictionary\n",
    "built_units['Jurisdiction'] = built_units.index.map(lambda x: Forecast_Reason_lookup.get(x, {}).get('Jurisdiction'))\n",
    "built_units['Unit_Pool'] = built_units.index.map(lambda x: Forecast_Reason_lookup.get(x, {}).get('Unit_Pool'))\n",
    "unit_comparison = built_units.merge(dfResZoned, how='left', on=['Jurisdiction', 'Unit_Pool'])\n",
    "unit_comparison['Difference'] = unit_comparison['Future_Units_Adjusted'] - unit_comparison['FORECASTED_RESIDENTIAL_UNITS']\n",
    "unit_comparison.to_csv('unit_comparison.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecasted residential uints by location to twon center\n",
    "sdfParcels.groupby(['LOCATION_TO_TOWNCENTER','FORECAST_REASON'])['FORECASTED_RESIDENTIAL_UNITS'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize Existing and Forecasted Units by Jurisdiction and Unit Pool by TAZ\n",
    "dfTAZ = sdfParcels.groupby('TAZ')[['FORECASTED_RESIDENTIAL_UNITS', 'Residential_Units']].sum().reset_index()\n",
    "dfTAZ.to_csv(data_dir/'TAZ_Units.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gropu by TAZ and sum forecasted residential units and residential units\n",
    "# Forecast year total residential units by TAZ\n",
    "sdfParcels.groupby('TAZ')[['FORECASTED_RESIDENTIAL_UNITS', 'Residential_Units']].sum().reset_index()\n",
    "# total units with forecast year 2035 and 2050\n",
    "sdfParcels.groupby('Development_Year')[['FORECASTED_RESIDENTIAL_UNITS', 'Residential_Units']].sum().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tourist Accommodation Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lookup lists\n",
    "tau_assigned_lookup = \"Lookup_Lists/forecast_tourist_assigned_units.csv\"\n",
    "# get assigned units lookup as data frames\n",
    "dfTouristAssigned = pd.read_csv(tau_assigned_lookup)\n",
    "# assign tourist units to parcels\n",
    "sdfParcels['FORECASTED_TAU_UNITS'] = 0\n",
    "# set tourist units to assigned total\n",
    "sdfParcels['FORECASTED_TAU_UNITS'] = sdfParcels.APN.map(dict(zip(dfTouristAssigned.APN, dfTouristAssigned['Unit_Pool'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commercial Floor Area Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lookup lists\n",
    "cfa_assigned_lookup = \"Lookup_Lists/forecast_commercial_assigned_units.csv\"\n",
    "# get zoned units lookups as data frames\n",
    "dfCommercialAssigned = pd.read_csv(cfa_assigned_lookup)\n",
    "# set commercial floor area to assigned total\n",
    "sdfParcels['FORECASTED_COMMERCIAL_FLOOR_AREA'] = 0\n",
    "sdfParcels['FORECASTED_COMMERCIAL_FLOOR_AREA'] = sdfParcels.APN.map(dict(zip(dfCommercialAssigned.APN, dfCommercialAssigned['NEW_CFA'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to pickle\n",
    "sdfParcels.to_pickle(parcel_pickle_part2)\n",
    "# to csv\n",
    "sdfParcels.to_csv(data_dir/'Parcels_Forecast.csv', index=False)\n",
    "# to feature class\n",
    "sdfParcels.spatial.to_featureclass(Path(gdb)/'Parcel_Forecast')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arcgispro-py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RTP Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import os\n",
    "import arcpy\n",
    "from utils import *\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# pandas options\n",
    "pd.options.mode.copy_on_write = True\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = 999\n",
    "pd.options.display.max_rows    = 999\n",
    "\n",
    "# my workspace \n",
    "workspace = r\"C:\\Users\\mbindl\\Desktop\\Workspace.gdb\"\n",
    "# current working directory\n",
    "local_path = pathlib.Path().absolute()\n",
    "# set data path as a subfolder of the current working directory TravelDemandModel\\2022\\\n",
    "data_dir = local_path.parents[0] / 'data'\n",
    "# folder to save processed data\n",
    "out_dir  = local_path.parents[0] / 'data/processed_data'\n",
    "# workspace gdb for stuff that doesnt work in memory\n",
    "# gdb = os.path.join(local_path,'Workspace.gdb')\n",
    "gdb = workspace\n",
    "# set environement workspace to in memory \n",
    "arcpy.env.workspace = 'memory'\n",
    "# # clear memory workspace\n",
    "# arcpy.management.Delete('memory')\n",
    "\n",
    "# overwrite true\n",
    "arcpy.env.overwriteOutput = True\n",
    "# Set spatial reference to NAD 1983 UTM Zone 10N\n",
    "sr = arcpy.SpatialReference(26910)\n",
    "\n",
    "# get parcels from the database\n",
    "# network path to connection files\n",
    "filePath = \"F:/GIS/PARCELUPDATE/Workspace/\"\n",
    "# database file path \n",
    "sdeBase    = os.path.join(filePath, \"Vector.sde\")\n",
    "sdeCollect = os.path.join(filePath, \"Collection.sde\")\n",
    "sdeTabular = os.path.join(filePath, \"Tabular.sde\")\n",
    "sdeEdit    = os.path.join(filePath, \"Edit.sde\")\n",
    "\n",
    "# schema for the final output\n",
    "final_schema = ['APN', 'Residential_Units', 'TouristAccommodation_Units', 'CommercialFloorArea_SqFt',\n",
    "                'ZONING_ID', 'EXISTING_LANDUSE', 'COUNTY', 'JURISDICTION', 'OWNERSHIP_TYPE',\n",
    "                'IPES_SCORE', 'VHR', 'BLOCK_GROUP', 'TAZ', 'RETIRED', \n",
    "                'JURISDICTION', 'COUNTY', 'OWNERSHIP_TYPE','EXISTING_LANDUSE',  \n",
    "                'WITHIN_BONUS_UNIT_BNDRY', 'WITHIN_TRPA_BNDY', \n",
    "                'MAX_RESIDENTIAL_UNITS', 'MAX_COMMERCIAL_FLOOR_AREA', 'MAX_TAU_UNITS',\n",
    "                'PARCEL_ACRES', 'PARCEL_SQFT', 'SHAPE']\n",
    "\n",
    "# Pickle variables\n",
    "# part 1 - spatial join categories, occupancy rates, and parcels\n",
    "parcel_pickle_part1    = data_dir / 'parcel_pickle1.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get parcel data from Edit SDE\n",
    "parcel_db = Path(sdeEdit) / \"SDE.Parcel\\\\SDE.Parcel_History_Attributed\"\n",
    "# query 2022 rows\n",
    "sdf_units = pd.DataFrame.spatial.from_featureclass(parcel_db)\n",
    "sdf_units = sdf_units.loc[sdf_units['YEAR'] == 2022]\n",
    "sdf_units.spatial.sr = sr\n",
    "\n",
    "# # get parcel level data from Collection SDE\n",
    "# vhr feature layer polygons \n",
    "vhr_db = Path(sdeCollect) / \"SDE.Parcel\\\\SDE.Parcel_VHR\"\n",
    "sdf_vhr = pd.DataFrame.spatial.from_featureclass(vhr_db)\n",
    "sdf_vhr.spatial.sr = sr\n",
    "# filter vhr layer to active status\n",
    "sdf_vhr = sdf_vhr.loc[sdf_vhr['Status'] == 'Active']\n",
    "\n",
    "\n",
    "# get parcel level data from LTinfo\n",
    "dfIPES       = pd.read_json(\"https://www.laketahoeinfo.org/WebServices/GetParcelIPESScores/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476\")\n",
    "dfLCV_LTinfo = pd.read_json('https://www.laketahoeinfo.org/WebServices/GetParcelsByLandCapability/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476')\n",
    "dfRetired    = pd.read_json(\"https://www.laketahoeinfo.org/WebServices/GetAllParcels/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476\")\n",
    "dfBankedDev  = pd.read_json('https://www.laketahoeinfo.org/WebServices/GetBankedDevelopmentRights/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476')\n",
    "dfTransacted = pd.read_json('https://www.laketahoeinfo.org/WebServices/GetTransactedAndBankedDevelopmentRights/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476')\n",
    "dfAllParcels = pd.read_json('https://www.laketahoeinfo.org/WebServices/GetAllParcels/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476')\n",
    "\n",
    "# get use tables \n",
    "# zoning data\n",
    "sde_engine = get_conn('sde')\n",
    "with sde_engine.begin() as conn:\n",
    "    df_uses    = pd.read_sql(\"SELECT * FROM sde.SDE.PermissibleUses\", conn)\n",
    "    df_special = pd.read_sql(\"SELECT * FROM sde.SDE.Special_Designation\", conn)\n",
    "\n",
    "tab_engine = get_conn('sde_tabular')\n",
    "with tab_engine.begin() as conn:\n",
    "    df_permit = pd.read_sql(\"SELECT * FROM sde.SDE.Accela_Record_Details\", conn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conn(db):\n",
    "    # Get database user and password from environment variables on machine running script\n",
    "    db_user             = os.environ.get('DB_USER')\n",
    "    db_password         = os.environ.get('DB_PASSWORD')\n",
    "\n",
    "    # driver is the ODBC driver for SQL Server\n",
    "    driver              = 'ODBC Driver 17 for SQL Server'\n",
    "    # server names are\n",
    "    sql_12              = 'sql12'\n",
    "    sql_14              = 'sql14'\n",
    "    # make it case insensitive\n",
    "    db = db.lower()\n",
    "    # make sql database connection with pyodbc\n",
    "    if db   == 'sde_tabular':\n",
    "        connection_string = f\"DRIVER={driver};SERVER={sql_12};DATABASE={db};UID={db_user};PWD={db_password}\"\n",
    "        connection_url = URL.create(\"mssql+pyodbc\", query={\"odbc_connect\": connection_string})\n",
    "        engine = create_engine(connection_url)\n",
    "    elif db == 'tahoebmpsde':\n",
    "        connection_string = f\"DRIVER={driver};SERVER={sql_14};DATABASE={db};UID={db_user};PWD={db_password}\"\n",
    "        connection_url = URL.create(\"mssql+pyodbc\", query={\"odbc_connect\": connection_string})\n",
    "        engine = create_engine(connection_url)\n",
    "    elif db == 'sde':\n",
    "        connection_string = f\"DRIVER={driver};SERVER={sql_12};DATABASE={db};UID={db_user};PWD={db_password}\"\n",
    "        connection_url = URL.create(\"mssql+pyodbc\", query={\"odbc_connect\": connection_string})\n",
    "        engine = create_engine(connection_url)\n",
    "    # else return None\n",
    "    else:\n",
    "        engine = None\n",
    "    # connection file to use in pd.read_sql\n",
    "    return engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# function to get where Zoningin_ID Use_Type = Multi-Family and Density\n",
    "def get_mf_zones(df):\n",
    "    columns_to_keep = ['Zoning_ID', 'Use_Type', 'Density']\n",
    "    # filter Use_Type to Multiple Family Dwelling\n",
    "    df = df.loc[df['Use_Type'] == 'Multiple Family Dwelling']\n",
    "    return df[columns_to_keep]\n",
    "\n",
    "def get_sf_zones(df):\n",
    "    columns_to_keep = ['Zoning_ID', 'Use_Type', 'Density']\n",
    "    # filter Use_Type to Single Family Dwelling\n",
    "    df = df.loc[df['Use_Type'] == 'Single Family Dwelling']\n",
    "    return df[columns_to_keep]\n",
    "\n",
    "def get_sf_only_zones(df):\n",
    "    columns_to_keep = ['Zoning_ID', 'Use_Type', 'Density']\n",
    "    # filter Use_Type to Single Family Dwelling and not Multiple Family Dwelling\n",
    "    dfMF = get_mf_zones(df)\n",
    "    dfSF = get_sf_zones(df)\n",
    "    df = dfSF.loc[~dfSF['Zoning_ID'].isin(dfMF['Zoning_ID'])]\n",
    "    return df[columns_to_keep]\n",
    "\n",
    "def get_recieving_zones(df):\n",
    "    columns_to_keep = ['Zoning_ID', 'Use_Type', 'Density']\n",
    "    # filter Use_Type to Single Family Dwelling and not Multiple Family Dwelling\n",
    "    dfMF = get_mf_zones(df)\n",
    "    dfSF = get_sf_zones(df)\n",
    "    df = dfSF.loc[~dfSF['Zoning_ID'].isin(dfMF['Zoning_ID'])]\n",
    "    return df[columns_to_keep]\n",
    "\n",
    "def get_sending_zones(df):\n",
    "    columns_to_keep = ['Zoning_ID', 'Use_Type', 'Density']\n",
    "    # filter Use_Type to Single Family Dwelling and not Multiple Family Dwelling\n",
    "    dfMF = get_mf_zones(df)\n",
    "    dfSF = get_sf_zones(df)\n",
    "    df = dfMF.loc[~dfMF['Zoning_ID'].isin(dfSF['Zoning_ID'])]\n",
    "    return df[columns_to_keep]\n",
    "\n",
    "\n",
    "dfMF = get_mf_zones(df_uses)\n",
    "dfMF.Use_Type.value_counts()\n",
    "\n",
    "dfSF = get_sf_zones(df_uses)\n",
    "dfSF.Use_Type.value_counts()\n",
    "\n",
    "dfSF_only = get_sf_only_zones(df_uses)\n",
    "dfSF_only.Use_Type.value_counts()\n",
    "\n",
    "dfSend    = get_send_only()\n",
    "dfRecieve = get_recieve_only()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parcel development layer polygons\n",
    "parcel_db = Path(sdeEdit) / \"SDE.Parcel\\\\SDE.Parcel_History_Attributed\"\n",
    "# query 2022 rows\n",
    "sdf_units = pd.DataFrame.spatial.from_featureclass(parcel_db)\n",
    "sdf_units = sdf_units.loc[sdf_units['YEAR'] == 2022]\n",
    "sdf_units.spatial.sr = sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ParcelTables_to_ParcelFeatures.py\n",
    "Created: March 13th, 2020\n",
    "Last Updated: June 14th, 2023\n",
    "Mason Bindl, Tahoe Regional Planning Agency\n",
    "Amy Fish, Tahoe Regional Planning Agency\n",
    "\n",
    "This python script was developed to move data from \n",
    "Accela, LTinfo, and BMP databases to TRPA's dynamic Enterprise Geodatabase.\n",
    "This ETL process updates parcel based feature classes for Development Rights, BMPs, LCVs, LCCs, \n",
    "Historic Parcels, Securities, Grading Exceptions, Deed Restrictions, and Soils Hydro Projects\n",
    "\n",
    "This script uses Python 3.x and was designed to be used with \n",
    "the default ArcGIS Pro python enivorment \"\"C:/Program Files/ArcGIS/Pro/bin/Python/envs/arcgispro-py3/python.exe\"\", with\n",
    "no need for installing new libraries.\n",
    "\n",
    "This script runs nightly at 10pm on Arc10 from scheduled task \"ParcelETL\"\n",
    "\"\"\"\n",
    "#--------------------------------------------------------------------------------------------------------#\n",
    "# import packages and modules\n",
    "# base packages\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import sqlalchemy as sa\n",
    "from sqlalchemy.engine import URL\n",
    "from sqlalchemy import create_engine\n",
    "# ESRI packages\n",
    "import arcpy\n",
    "from arcgis.features import FeatureSet, GeoAccessor, GeoSeriesAccessor\n",
    "# email packages\n",
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "# set overwrite to true\n",
    "arcpy.env.overwriteOutput = True\n",
    "arcpy.env.workspace = \"C:\\GIS\\Scratch.gdb\"\n",
    "\n",
    "# in memory output file path\n",
    "wk_memory = \"memory\" + \"\\\\\"\n",
    "# set workspace and sde connections \n",
    "working_folder = \"C:\\GIS\"\n",
    "workspace      = \"C:\\GIS\\Scratch.gdb\"\n",
    "\n",
    "# network path to connection files\n",
    "filePath = \"C:\\\\GIS\\\\DB_CONNECT\"\n",
    "# database file path \n",
    "sdeBase = os.path.join(filePath, \"Vector.sde\")\n",
    "sdeCollect = os.path.join(filePath, \"Collection.sde\")\n",
    "# Feature dataset to unversion and register as version\n",
    "fdata = sdeCollect + \"\\\\sde_collection.SDE.Parcel\"\n",
    "# string to use in updaetSDE function\n",
    "sdeString  = fdata + \"\\\\sde_collection.SDE.\"\n",
    "\n",
    "# connect to bmp SQL dataabase\n",
    "connection_string = \"DRIVER={ODBC Driver 17 for SQL Server};SERVER=sql14;DATABASE=tahoebmpsde;UID=sde;PWD=staff\"\n",
    "connection_url = URL.create(\"mssql+pyodbc\", query={\"odbc_connect\": connection_string})\n",
    "engine = create_engine(connection_url)\n",
    "\n",
    "##--------------------------------------------------------------------------------------#\n",
    "## EMAIL and LOG FILE SETTINGS ##\n",
    "##--------------------------------------------------------------------------------------#\n",
    "## LOGGING SETUP\n",
    "# Configure the logging\n",
    "log_file_path = os.path.join(working_folder, \"Parcel_Development_ETL_Log.log\")  \n",
    "# setup basic logging configuration\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "                    filename=log_file_path,  # Set the log file path\n",
    "                    filemode='w')\n",
    "# Create a logger\n",
    "logger = logging.getLogger(__name__)\n",
    "# Log start message\n",
    "logger.info(\"Script Started: \" + str(datetime.datetime.now()) + \"\\n\")\n",
    "\n",
    "## EMAIL SETUP\n",
    "# path to text file\n",
    "fileToSend = log_file_path\n",
    "# email parameters\n",
    "subject = \"Parcel Development ETL\"\n",
    "sender_email = \"infosys@trpa.org\"\n",
    "# password = ''\n",
    "receiver_email = \"mbindl@trpa.gov\"\n",
    "\n",
    "#---------------------------------------------------------------------------------------#\n",
    "## FUNCTIONS ##\n",
    "#---------------------------------------------------------------------------------------#\n",
    "\n",
    "# send email with attachments\n",
    "def send_mail(body):\n",
    "    msg = MIMEMultipart()\n",
    "    msg['Subject'] = subject\n",
    "    msg['From'] = sender_email\n",
    "    msg['To'] = receiver_email\n",
    "\n",
    "    msgText = MIMEText('%s<br><br>Cheers,<br>GIS Team' % (body), 'html')\n",
    "    msg.attach(msgText)\n",
    "\n",
    "    attachment = MIMEText(open(fileToSend).read())\n",
    "    attachment.add_header(\"Content-Disposition\", \"attachment\", filename = os.path.basename(fileToSend))\n",
    "    msg.attach(attachment)\n",
    "\n",
    "    try:\n",
    "        with smtplib.SMTP(\"mail.smtp2go.com\", 25) as smtpObj:\n",
    "            smtpObj.ehlo()\n",
    "            smtpObj.starttls()\n",
    "#             smtpObj.login(sender_email, password)\n",
    "            smtpObj.sendmail(sender_email, receiver_email, msg.as_string())\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "\n",
    "# # update staging layers\n",
    "def updateStagingLayer(name, df, fields):\n",
    "    # copy fields to keep\n",
    "    dfOut = df[fields].copy()\n",
    "    # specify output feature class\n",
    "    outFC = os.path.join(workspace, name)\n",
    "    # spaital dataframe to feature class\n",
    "    dfOut.spatial.to_featureclass(outFC, sanitize_columns=False)\n",
    "    # confirm feature class was created\n",
    "    print(f\"\\nUpdated staging layer:{outFC}\")\n",
    "    logger.info(f\"\\nUpdated staging layer:{outFC}\")\n",
    "\n",
    "# replaces features in outfc with exact same schema\n",
    "def updateSDECollectFC(fcList):\n",
    "    for fc in fcList:\n",
    "        inputFC = os.path.join(workspace, fc)\n",
    "        dsc = arcpy.Describe(inputFC)\n",
    "        fields = dsc.fields\n",
    "        out_fields = [dsc.OIDFieldName, dsc.lengthFieldName, dsc.areaFieldName]\n",
    "        fieldnames = [field.name if field.name != 'Shape' else 'SHAPE@' for field in fields if field.name not in out_fields]\n",
    "        outfc = sdeString + fc\n",
    "        # deletes all rows from the SDE feature class\n",
    "        arcpy.TruncateTable_management(outfc)\n",
    "        logger.info(\"\\nDeleted all records in: {}\\n\".format(outfc))\n",
    "        from time import strftime  \n",
    "        logger.info(\"Started data transfer: \" + strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "        # insert rows from Temporary feature class to SDE feature class\n",
    "        with arcpy.da.InsertCursor(outfc, fieldnames) as oCursor:\n",
    "            count = 0\n",
    "            with arcpy.da.SearchCursor(inputFC, fieldnames) as iCursor:\n",
    "                for row in iCursor:\n",
    "                    oCursor.insertRow(row)\n",
    "                    count += 1\n",
    "                    if count % 100 == 0:\n",
    "                        logger.info(\"Inserting record %d into %s SDE feature class\" % (count, outfc))\n",
    "                logger.info(f\"\\nDone updating: {outfc}\")\n",
    "            \n",
    "\n",
    "#---------------------------------------------------------------------------------------#\n",
    "## GET DATA\n",
    "#---------------------------------------------------------------------------------------#\n",
    "\n",
    "# start timer for the get data requests\n",
    "startTimer = datetime.datetime.now()\n",
    "\n",
    "# get feature classes from enterprise geodatabase\n",
    "bonusBoundary= os.path.join(sdeBase, \"sde.SDE.Planning\\sde.SDE.Bonus_unit_boundary\")\n",
    "mfAllowed    = os.path.join(sdeBase, \"sde.SDE.Planning\\sde.SDE.Multifamily_Allowed_Zone\")\n",
    "parcelMaster = os.path.join(sdeBase, \"sde.SDE.Parcels\\\\sde.SDE.Parcel_Master\")\n",
    "parcelIPES   = os.path.join(sdeCollect, fdata, \"sde_collection.SDE.Parcel_LTinfo_IPES\")\n",
    "parcelDeed   = os.path.join(sdeCollect, fdata, \"sde_collection.SDE.Parcel_LTinfo_DeedRestriction\")\n",
    "\n",
    "# create spatial dataframe from parcel master SDE\n",
    "sdfParcels = pd.DataFrame.spatial.from_featureclass(parcelMaster)\n",
    "sdfIPES    = pd.DataFrame.spatial.from_featureclass(parcelIPES)\n",
    "sdfDeed    = pd.DataFrame.spatial.from_featureclass(parcelDeed)\n",
    "\n",
    "# report how long it took to get the data\n",
    "endTimer = datetime.datetime.now() - startTimer\n",
    "print(\"\\nTime it took to get the data: {}\".format(endTimer))   \n",
    "logger.info(\"\\nTime it took to get the data: {}\".format(endTimer)) \n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------#\n",
    "## TRANSFORM TO STAGING LAYERS\n",
    "#---------------------------------------------------------------------------------------#\n",
    "# join IPES, join Deed, MF Allowed Spatial Join, field calc of % allowed\n",
    "\n",
    "try:\n",
    "    #---------------------------------------------------------------------------------------#\n",
    "    # CREATE STAGING LAYERS ##\n",
    "    #---------------------------------------------------------------------------------------#\n",
    "    # start timer for the get data requests\n",
    "    startTimer = datetime.datetime.now()\n",
    "    #---------------------------------------------------------------------------------------#\n",
    "\n",
    "    # name of feature class\n",
    "    name = \"Parcel_Development\"\n",
    "\n",
    "    # spatial join\n",
    "\n",
    "    # List of DataFrames\n",
    "    dfs = [sdfParcels, sdfIPES]\n",
    "\n",
    "    # # Merge DataFrames horizontally\n",
    "    # combined_df = pd.merge(dfs[0], dfs[1], on='APN')\n",
    "    # for df in dfs[2:]:\n",
    "    #     combined_df = pd.merge(combined_df, df, on='APN')\n",
    "    df = pd.merge(sdfParcels, sdfIPES, on='APN', how='left')\n",
    "       # rename some of the fields\n",
    "    df.rename(columns={\"JURISDICTION_x\": \"JURISDICTION\",\n",
    "                        \"OWNERSHIP_TYPE_x\":'OWNERSHIP_TYPE',\n",
    "                        \"EXISTING_LANDUSE_x\":\"EXISTING_LANDUSE\",\n",
    "                        \"SHAPE_x\":\"SHAPE\"\n",
    "                            }, inplace=True)\n",
    "    df['MF_ALLOWED'] = \"No\"\n",
    "    df['PERCENT_COVERAGE_ALLOWED'] = (df.ESTIMATED_COVERAGE_ALLOWED/df.PARCEL_SQFT)*100\n",
    "    # Print the combined DataFrame\n",
    "    # specify fields to keep\n",
    "    fields = ['APN',\n",
    "                'APO_ADDRESS',\n",
    "                'PSTL_TOWN',\n",
    "                'PSTL_STATE',\n",
    "                'PSTL_ZIP5',\n",
    "                'JURISDICTION',\n",
    "                'COUNTY',\n",
    "                'OWNERSHIP_TYPE',\n",
    "                'COUNTY_LANDUSE_DESCRIPTION',\n",
    "                'EXISTING_LANDUSE',\n",
    "                'REGIONAL_LANDUSE',\n",
    "                'AS_SUM',\n",
    "                'TAX_SUM',\n",
    "                'TAX_YEAR',\n",
    "                'YEAR_BUILT',\n",
    "                'UNITS',\n",
    "                'BEDROOMS',\n",
    "                'BATHROOMS',\n",
    "                'BUILDING_SQFT',\n",
    "                'ESTIMATED_COVERAGE_ALLOWED',\n",
    "                'IMPERVIOUS_SURFACE_SQFT',\n",
    "                'CATCHMENT',\n",
    "                'PLAN_ID',\n",
    "                'PLAN_NAME',\n",
    "                'ZONING_ID',\n",
    "                'ZONING_DESCRIPTION',\n",
    "                'TOWN_CENTER',\n",
    "                'LOCATION_TO_TOWNCENTER',\n",
    "                'WITHIN_TRPA_BNDY',\n",
    "                'LOCAL_PLAN_HYPERLINK',\n",
    "                'DESIGN_GUIDELINES_HYPERLINK',\n",
    "                'LTINFO_HYPERLINK',\n",
    "                'PARCEL_ACRES',\n",
    "                'PARCEL_SQFT',\n",
    "                'WITHIN_BONUSUNIT_BNDY',\n",
    "                'PERCENT_COVERAGE_ALLOWED',\n",
    "                'MF_ALLOWED',\n",
    "                'SF_ALLOWED',\n",
    "                'SENDING_ZONE',\n",
    "                'RECIEVING_ZONE',\n",
    "                'DEED_RESTRICTION',\n",
    "                'IPES_SCORE',\n",
    "                'DENSITY_ALLOWED',\n",
    "                'MAX_RESIDENTIAL_UNITS',\n",
    "                'MAX_COMMERCIAL_FLOOR_AREA',\n",
    "                'MAX_TAU_UNITS',\n",
    "                ''\n",
    "                #IPES Fields\n",
    "                'IPESScore',\n",
    "                'IPESScoreType',\n",
    "                'BaseAllowableCoveragePercent',\n",
    "                'IPESTotalAllowableCoverageSqFt',\n",
    "                'ParcelHasDOAC',\n",
    "                'HistoricOrImportedIpesScore',\n",
    "                'CalculationDate',\n",
    "                'FieldEvaluationDate',\n",
    "                'RelativeErosionHazardScore',\n",
    "                'RunoffPotentialScore',\n",
    "                'AccessScore',\n",
    "                'UtilityInSEZScore',\n",
    "                'ConditionOfWatershedScore',\n",
    "                'AbilityToRevegetateScore',\n",
    "                'WaterQualityImprovementsScore',\n",
    "                'ProximityToLakeScore',\n",
    "                'LimitedIncentivePoints',\n",
    "                'TotalParcelArea',\n",
    "                'IPESBuildingSiteArea',\n",
    "                'SEZLandArea',\n",
    "                'SEZSetbackArea',\n",
    "                'InternalNotes',\n",
    "                'PublicNotes',\n",
    "            # Deed fields\n",
    "            # 'RecordingNumber',\n",
    "            # 'RecordingDate',\n",
    "            # 'Description',\n",
    "            # 'DeedRestrictionStatus',\n",
    "            # 'DeedRestrictionType',\n",
    "            # 'ProjectAreaFileNumber',\n",
    "            # 'ScoreSheetUrl',\n",
    "            # 'Status',\n",
    "            # 'ParcelNickname',\n",
    "            'SHAPE']\n",
    "            \n",
    "    # update staging feature class from dataframe\n",
    "    updateStagingLayer(name, df, fields)\n",
    "    \n",
    "    #---------------------------------------------------------------------------------------#\n",
    "    # report how long it took to get the data\n",
    "    endTimer = datetime.datetime.now() - startTimer\n",
    "    print(\"\\nTime it took to create staging layers: {}\".format(endTimer))       \n",
    "    #---------------------------------------------------------------------------------------#\n",
    "\n",
    "    ##--------------------------------------------------------------------------------------------------------#\n",
    "    ## BEGIN SDE UPDATES ##\n",
    "    ##--------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "#     # disconnect all users\n",
    "#     print(\"\\nDisconnecting all users...\")\n",
    "#     arcpy.DisconnectUser(sdeCollect, \"ALL\")\n",
    "\n",
    "#     # unregister the sde feature class as versioned\n",
    "#     print (\"\\nUnregistering feature dataset as versioned...\")\n",
    "#     arcpy.UnregisterAsVersioned_management(fdata,\"NO_KEEP_EDIT\",\"COMPRESS_DEFAULT\")\n",
    "#     print (\"\\nFinished unregistering feature dataset as versioned.\")\n",
    "\n",
    "#     # #---------------------------------------------------------------------------------------#\n",
    "\n",
    "#     # feature class list\n",
    "#     fcs =[\"Parcel_Development\"]\n",
    "\n",
    "#     # function to update all collection SDE feature classes in list\n",
    "#     updateSDECollectFC(fcs)\n",
    "\n",
    "#     #---------------------------------------------------------------------------------------#\n",
    "#     # report how long it took to get the data\n",
    "#     endTimer = datetime.datetime.now() - startTimer \n",
    "#     logger.info(f\"\\nTime it took to update Collection SDE feature classes: {endTimer}\") \n",
    "#     #---------------------------------------------------------------------------------------#\n",
    "\n",
    "#     ##--------------------------------------------------------------------------------------------------------#\n",
    "#     ## END OF UPDATES ##\n",
    "#     ##--------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "#     # disconnect all users\n",
    "#     print(\"\\nDisconnecting all users...\")\n",
    "#     logger.info(\"\\nDisconnecting all users...\")\n",
    "#     arcpy.DisconnectUser(sdeCollect, \"ALL\")\n",
    "\n",
    "#     print(\"\\nRegistering feature dataset as versioned...\")\n",
    "#     logger.info(\"\\nRegistering feature dataset as versioned...\")\n",
    "#     # register SDE feature class as versioned\n",
    "#     arcpy.RegisterAsVersioned_management(fdata, \"NO_EDITS_TO_BASE\")\n",
    "#     print(\"\\nFinished registering feature dataset as versioned.\")\n",
    "#     logger.info(\"\\nFinished registering feature dataset as versioned.\")\n",
    "    \n",
    "    # report how long it took to run the script\n",
    "    runTime = datetime.datetime.now() - startTimer\n",
    "    logger.info(f\"\\nTime it took to run this script: {runTime}\")\n",
    "\n",
    "    # send email with header based on try/except result\n",
    "    header = \"SUCCESS - Parcel feature classes were updated.\"\n",
    "    send_mail(header)\n",
    "    print('Sending email...')\n",
    "\n",
    "# catch any arcpy errors\n",
    "except arcpy.ExecuteError:\n",
    "    print(arcpy.GetMessages())\n",
    "    logger.debug(arcpy.GetMessages())\n",
    "    # send email with header based on try/except result\n",
    "    header = \"ERROR - Arcpy Exception - Check Log\"\n",
    "    send_mail(header)\n",
    "    print('Sending email...')\n",
    "\n",
    "# catch system errors\n",
    "except Exception:\n",
    "    e = sys.exc_info()[1]\n",
    "    print(e.args[0])\n",
    "    logger.debug(e)\n",
    "    # send email with header based on try/except result\n",
    "    header = \"ERROR - System Error - Check Log\"\n",
    "    send_mail(header)\n",
    "    print('Sending email...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecast Formetted Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathCSV = data_dir / \"RegionalTransportationPlan/2023/data/Forecasts_Table1.csv\"\n",
    "print(pathCSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from great_tables import *\n",
    "import pandas as pd\n",
    "\n",
    "forecast = pd.read_csv(data_dir / \"Forecasts_Table1.csv\")\n",
    "# drop notes column \n",
    "forecast.drop(columns=['Notes'], inplace=True)\n",
    "# change column names\n",
    "forecast.rename(columns={'Change by 2050': 'Change(#)', 'Percent Change': 'Change(%)'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "GT(forecast).tab_header(title=\"Table 1. Forecast Data Summary\").tab_spanner(\n",
    "    label=\"\", columns=['Category', 'Variable','Base Year 2022',  'Forecast 2050', 'Change(#)', 'Change(%)']).tab_stub(\n",
    "        rowname_col='Variable', groupname_col='Category').tab_style(\n",
    "            style=style.fill(color=\"aliceblue\"), locations=loc.body()).save(\"forecast.jpeg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transit Stacked Bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data for transit ridership\n",
    "def get_data_transit():\n",
    "    url = \"https://www.laketahoeinfo.org/WebServices/GetTransitMonitoringData/CSV/e17aeb86-85e3-4260-83fd-a2b32501c476\"\n",
    "\n",
    "    dfTransit = pd.read_csv(url)\n",
    "    dfTransit['Month'] = pd.to_datetime(dfTransit['Month'])\n",
    "    dfTransit['Month'] = dfTransit['Month'].dt.strftime('%Y-%m')\n",
    "    # filter out rows where RouteType is not Paratransit, Commuter, or Seasonal Fixed\n",
    "    df = dfTransit.loc[~dfTransit['RouteType'].isin(['Paratransit', 'Commuter', 'Seasonal Fixed Route'])]\n",
    "    # df = dfTransit.loc[dfTransit['RouteType'] != 'Paratransit']\n",
    "\n",
    "    # replace transit operator values with abreviations\n",
    "    df['TransitOperator'] = df['TransitOperator'].replace(\n",
    "        ['Tahoe Transportation District',\n",
    "       'Tahoe Truckee Area Regional Transit',\n",
    "       'South Shore Transportation Management Association'],\n",
    "       [\"TTD\", \"TART\", \"SSTMA\"])\n",
    "    # route name = route type + transit operator\n",
    "    df['RouteName'] = df['RouteType'] + ' - ' + df['TransitOperator']\n",
    "    # group by RouteType, TransitOperator, and Month with sum of MonthlyRidership\n",
    "    df = df.groupby(['RouteName', 'Month'])['MonthlyRidership'].sum().reset_index()\n",
    "    # rename columns to Date, Name, Ridership\n",
    "    df.rename(columns={'Month':'Date', 'RouteName':'Name', 'MonthlyRidership':'Ridership'}, inplace=True)\n",
    "    # sort by Date\n",
    "    df = df.sort_values('Date')\n",
    "    return df\n",
    "\n",
    "# html/3.3.a_Transit_Ridership.html\n",
    "def plot_transit(df):\n",
    "    trendline(\n",
    "        df,\n",
    "        path_html=\"html/3.3.a_Transit_Ridership.html\",\n",
    "        div_id=\"3.3.a_Transit_Ridership\",\n",
    "        x=\"Date\",\n",
    "        y=\"Ridership\",\n",
    "        color=\"Name\",\n",
    "        color_sequence=[\"#023f64\", \"#7ebfb5\", \"#a48352\", \"#FC9A62\"],\n",
    "        sort=\"Date\",\n",
    "        orders=None,\n",
    "        x_title=\"Date\",\n",
    "        y_title=\"Ridership\",\n",
    "        markers=True,\n",
    "        hover_data=None,\n",
    "        tickvals=None,\n",
    "        ticktext=None,\n",
    "        tickangle=None,\n",
    "        hovermode=\"x unified\",\n",
    "        format=\",.0f\",\n",
    "        custom_data=[\"Name\"],\n",
    "        hovertemplate=\"<br>\".join([\n",
    "            \"<b>%{y:,.0f}</b> riders on\",\n",
    "            \"<i>%{customdata[0]}</i> lines\"\n",
    "                ])+\"<extra></extra>\",\n",
    "        additional_formatting = dict(\n",
    "                                    title = \"Transit Ridership\",\n",
    "                                    margin=dict(t=20),\n",
    "                                    legend=dict(\n",
    "                                        # title=\"Transit Ridership\",\n",
    "                                        orientation=\"h\",\n",
    "                                        entrywidth=120,\n",
    "                                        yanchor=\"bottom\",\n",
    "                                        y=1.05,\n",
    "                                        xanchor=\"right\",\n",
    "                                        x=0.95,\n",
    "                                    ))\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arcgispro-py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
